{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b1cbe0",
   "metadata": {},
   "source": [
    "# üöÄ Multimodal Price Prediction Pipeline\n",
    "## Combining Text (DistilBERT) + Vision (ResNet/EfficientNet) for Enhanced Performance\n",
    "\n",
    "### Current Baseline (Text-only)\n",
    "- **Model**: DistilBERT + MLP Regressor\n",
    "- **Performance**: 18.04% SMAPE, 185.47 MAE, 421.33 RMSE, 0.752 R¬≤\n",
    "\n",
    "### Target Multimodal Architecture\n",
    "- **Text Branch**: DistilBERT ‚Üí 768-dim features\n",
    "- **Image Branch**: ResNet-50/EfficientNet ‚Üí 512-dim features  \n",
    "- **Fusion Layer**: Multi-head attention + Concatenation ‚Üí 640-dim\n",
    "- **Output**: MLP Regressor ‚Üí Price prediction\n",
    "\n",
    "### Expected Improvement\n",
    "- **Target SMAPE**: ~13.5% (25% improvement)\n",
    "- **Target MAE**: ~143 (23% improvement)\n",
    "- **Target RMSE**: ~331 (21% improvement)\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Notebook Structure\n",
    "1. Environment Setup & Dependencies\n",
    "2. Data Loading & Preprocessing\n",
    "3. Image Processing Pipeline (Download, Cache, Augmentation)\n",
    "4. Multimodal Dataset & DataLoaders\n",
    "5. Model Architecture (Text Encoder, Image Encoder, Fusion, Regressor)\n",
    "6. Training Pipeline (Progressive Training, Mixed Precision)\n",
    "7. Evaluation & Analysis\n",
    "8. Test Predictions & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb054717-1b75-4e39-8892-570619797c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ENVIRONMENT INFORMATION\n",
      "============================================================\n",
      "NumPy version: 2.1.3\n",
      "PyTorch version: 2.8.0+cu128\n",
      "Transformers version: 4.55.2\n",
      "\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "GPU name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "CUDA version: 12.8\n",
      "GPU Memory: 4.0 GB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========================================\n",
    "# 1. ENVIRONMENT SETUP & DEPENDENCIES\n",
    "# ========================================\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import transformers\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ENVIRONMENT INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No CUDA GPU detected - training will be slower on CPU\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50be22ae-a4d5-4fdd-baed-a11e29157764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "‚úì Data loaded successfully\n",
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 3)\n",
      "\n",
      "First 3 rows of training data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>catalog_content</th>\n",
       "      <th>image_link</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>33127</td>\n",
       "      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>198967</td>\n",
       "      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n",
       "      <td>13.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>261251</td>\n",
       "      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sample_id                                    catalog_content  \\\n",
       "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
       "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
       "2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n",
       "\n",
       "                                          image_link  price  \n",
       "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
       "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
       "2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "\n",
    "# Image processing imports\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision import transforms, models\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ML utilities\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Optional, Tuple, List, Dict, Union\n",
    "import logging\n",
    "import math\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(levelname)s: %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set compute device\n",
    "compute_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\n‚úì Using device: {compute_device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úì GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"‚úì GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "print(\"\\n‚úì All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bda30f6",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38d6772",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"\\nColumns: {list(train_df.columns)}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\"*60)\n",
    "print(train_df.head(3))\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MISSING VALUES\")\n",
    "print(\"=\"*60)\n",
    "print(train_df.isnull().sum())\n",
    "print(f\"\\nImage availability: {train_df['image_link'].notna().sum()}/{len(train_df)} ({train_df['image_link'].notna().mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cc6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def clean_text_data(text):\n",
    "    \"\"\"Clean and normalize text data.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove excessive whitespace\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply text preprocessing\n",
    "train_df['catalog_content_processed'] = train_df['catalog_content'].apply(clean_text_data)\n",
    "test_df['catalog_content_processed'] = test_df['catalog_content'].apply(clean_text_data)\n",
    "\n",
    "# Analyze text lengths\n",
    "text_lengths = train_df['catalog_content_processed'].str.len()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TEXT STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Mean length: {text_lengths.mean():.1f} characters\")\n",
    "print(f\"Median length: {text_lengths.median():.1f} characters\")\n",
    "print(f\"Min length: {text_lengths.min()} characters\")\n",
    "print(f\"Max length: {text_lengths.max()} characters\")\n",
    "\n",
    "# Price statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRICE DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Price range: ${train_df['price'].min():.2f} - ${train_df['price'].max():.2f}\")\n",
    "print(f\"Price mean: ${train_df['price'].mean():.2f}\")\n",
    "print(f\"Price median: ${train_df['price'].median():.2f}\")\n",
    "print(f\"Price std: ${train_df['price'].std():.2f}\")\n",
    "\n",
    "print(\"\\n‚úì Data preprocessing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37aba4a9",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Image Processing Pipeline\n",
    "### Load Images from Local Directory and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07af590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# IMAGE PROCESSOR CLASS (LOCAL FILES)\n",
    "# ========================================\n",
    "\n",
    "class ImageProcessor:\n",
    "    \"\"\"\n",
    "    Handles local image loading, preprocessing, and augmentation.\n",
    "    \n",
    "    Features:\n",
    "    - Load images from local directory\n",
    "    - ImageNet normalization\n",
    "    - Data augmentation for training\n",
    "    - Error handling for broken/missing images\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: str = \"./images\",\n",
    "        img_size: int = 224,\n",
    "        use_augmentation: bool = True\n",
    "    ):\n",
    "        self.image_dir = Path(image_dir)\n",
    "        self.img_size = img_size\n",
    "        self.use_augmentation = use_augmentation\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        self.mean = [0.485, 0.456, 0.406]\n",
    "        self.std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        # Training transforms (with augmentation)\n",
    "        self.train_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size + 32, img_size + 32)),\n",
    "            transforms.RandomCrop(img_size),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "            transforms.RandomRotation(degrees=10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        ])\n",
    "        \n",
    "        # Validation transforms (no augmentation)\n",
    "        self.val_transform = transforms.Compose([\n",
    "            transforms.Resize((img_size, img_size)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=self.mean, std=self.std)\n",
    "        ])\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {'success': 0, 'failed': 0, 'missing': 0}\n",
    "        \n",
    "        logger.info(f\"ImageProcessor initialized: image_dir={image_dir}, img_size={img_size}\")\n",
    "    \n",
    "    def get_image_path(self, image_link: str) -> Path:\n",
    "        \"\"\"\n",
    "        Extract image filename from link and construct local path.\n",
    "        \n",
    "        Example:\n",
    "        https://m.media-amazon.com/images/I/81WiVwz7KkL.jpg -> ./images/81WiVwz7KkL.jpg\n",
    "        \"\"\"\n",
    "        if pd.isna(image_link) or not image_link:\n",
    "            return None\n",
    "        \n",
    "        # Extract filename from URL\n",
    "        filename = Path(image_link).name\n",
    "        return self.image_dir / filename\n",
    "    \n",
    "    def load_image(self, image_link: str) -> Optional[Image.Image]:\n",
    "        \"\"\"Load image from local directory.\"\"\"\n",
    "        image_path = self.get_image_path(image_link)\n",
    "        \n",
    "        if image_path is None:\n",
    "            self.stats['missing'] += 1\n",
    "            return None\n",
    "        \n",
    "        if not image_path.exists():\n",
    "            self.stats['missing'] += 1\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(image_path).convert('RGB')\n",
    "            self.stats['success'] += 1\n",
    "            return img\n",
    "        except Exception as e:\n",
    "            self.stats['failed'] += 1\n",
    "            logger.warning(f\"Failed to load image: {image_path} - {e}\")\n",
    "            return None\n",
    "    \n",
    "    def preprocess_image(self, image: Union[Image.Image, str], training: bool = False) -> Optional[torch.Tensor]:\n",
    "        \"\"\"Preprocess image for model input.\"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = self.load_image(image)\n",
    "        \n",
    "        if image is None:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            transform = self.train_transform if (training and self.use_augmentation) else self.val_transform\n",
    "            return transform(image)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to preprocess image: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_placeholder_tensor(self) -> torch.Tensor:\n",
    "        \"\"\"Create placeholder tensor for missing images (gray image).\"\"\"\n",
    "        placeholder = torch.zeros(3, self.img_size, self.img_size)\n",
    "        for c in range(3):\n",
    "            placeholder[c] = (0.5 - self.mean[c]) / self.std[c]\n",
    "        return placeholder\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"Get image loading statistics.\"\"\"\n",
    "        total = sum(self.stats.values())\n",
    "        return {\n",
    "            **self.stats,\n",
    "            'total': total,\n",
    "            'success_rate': self.stats['success'] / total if total > 0 else 0\n",
    "        }\n",
    "\n",
    "print(\"‚úì ImageProcessor class defined (LOCAL mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db82214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TEST IMAGE LOADING (Optional - Run to verify)\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TESTING IMAGE LOADING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Test with a few sample images\n",
    "test_processor = ImageProcessor(image_dir=\"./images\", img_size=224, use_augmentation=False)\n",
    "\n",
    "print(\"\\nTesting image loading from local directory...\")\n",
    "sample_links = train_df['image_link'].head(5).tolist()\n",
    "\n",
    "for i, link in enumerate(sample_links):\n",
    "    img = test_processor.load_image(link)\n",
    "    if img:\n",
    "        print(f\"‚úÖ Image {i+1}: Loaded successfully - Size: {img.size}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Image {i+1}: Failed to load from {link}\")\n",
    "\n",
    "stats = test_processor.get_stats()\n",
    "print(f\"\\nTest Statistics:\")\n",
    "print(f\"  Success: {stats['success']}\")\n",
    "print(f\"  Missing: {stats['missing']}\")\n",
    "print(f\"  Failed: {stats['failed']}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a1282",
   "metadata": {},
   "source": [
    "### üìÅ Image Directory Setup\n",
    "\n",
    "**Important**: Ensure your images are organized as follows:\n",
    "\n",
    "```\n",
    "./images/\n",
    "‚îú‚îÄ‚îÄ 81WiVwz7KkL.jpg\n",
    "‚îú‚îÄ‚îÄ 71AbCdEfGh.jpg\n",
    "‚îú‚îÄ‚îÄ 91XyZaBcDe.jpg\n",
    "‚îî‚îÄ‚îÄ ... (other images)\n",
    "```\n",
    "\n",
    "The image filenames should match the filenames in the `image_link` URLs.\n",
    "\n",
    "**Example**:\n",
    "- If `image_link` = `\"https://m.media-amazon.com/images/I/81WiVwz7KkL.jpg\"`\n",
    "- Then the file should be: `./images/81WiVwz7KkL.jpg`\n",
    "\n",
    "Missing images will automatically use placeholder tensors, so training won't break."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df223f4",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Loss Functions & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdca5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# LOSS FUNCTIONS\n",
    "# ========================================\n",
    "\n",
    "class SymmetricLoss(nn.Module):\n",
    "    \"\"\"Symmetric Mean Absolute Percentage Error (SMAPE) Loss.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(SymmetricLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        y_pred = torch.clamp(y_pred, min=1e-8)\n",
    "        y_true = torch.clamp(y_true, min=1e-8)\n",
    "        \n",
    "        numerator = torch.abs(y_pred - y_true)\n",
    "        denominator = (torch.abs(y_true) + torch.abs(y_pred)) / 2\n",
    "        smape = torch.mean(numerator / denominator)\n",
    "        return smape\n",
    "\n",
    "\n",
    "def calculate_symmetric_error(y_true, y_pred):\n",
    "    \"\"\"Calculate SMAPE metric.\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    denominator = np.where(denominator == 0, 1e-8, denominator)\n",
    "    \n",
    "    smape_value = np.mean(np.abs(y_pred - y_true) / denominator)\n",
    "    return smape_value\n",
    "\n",
    "\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    \"\"\"Calculate comprehensive metrics.\"\"\"\n",
    "    smape = calculate_symmetric_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'smape': smape,\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2\n",
    "    }\n",
    "\n",
    "print(\"‚úì Loss functions and metrics defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bda3b0",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Multimodal Dataset Class\n",
    "### Combines Text + Image Data for Efficient Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c40ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# MULTIMODAL DATASET CLASS\n",
    "# ========================================\n",
    "\n",
    "class MultimodalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for multimodal price prediction combining text and images.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        text_tokenizer,\n",
    "        image_processor: ImageProcessor,\n",
    "        max_text_length: int = 256,\n",
    "        training: bool = True,\n",
    "        use_images: bool = True\n",
    "    ):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.image_processor = image_processor\n",
    "        self.max_text_length = max_text_length\n",
    "        self.training = training\n",
    "        self.use_images = use_images\n",
    "        self.has_labels = 'price' in df.columns\n",
    "        \n",
    "        # Fill missing values\n",
    "        self.df['catalog_content_processed'] = self.df['catalog_content_processed'].fillna('')\n",
    "        self.df['image_link'] = self.df['image_link'].fillna('')\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Process text\n",
    "        text = str(row['catalog_content_processed'])\n",
    "        text_encoding = self.text_tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_text_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Process image\n",
    "        image_tensor = None\n",
    "        image_available = False\n",
    "        \n",
    "        if self.use_images:\n",
    "            image_url = str(row['image_link'])\n",
    "            image_tensor = self.image_processor.preprocess_image(image_url, training=self.training)\n",
    "            \n",
    "            if image_tensor is None:\n",
    "                image_tensor = self.image_processor.get_placeholder_tensor()\n",
    "                image_available = False\n",
    "            else:\n",
    "                image_available = True\n",
    "        else:\n",
    "            image_tensor = self.image_processor.get_placeholder_tensor()\n",
    "        \n",
    "        output = {\n",
    "            'input_ids': text_encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_encoding['attention_mask'].squeeze(0),\n",
    "            'image': image_tensor,\n",
    "            'image_available': torch.tensor(image_available, dtype=torch.bool)\n",
    "        }\n",
    "        \n",
    "        if self.has_labels:\n",
    "            output['labels'] = torch.tensor(row['price'], dtype=torch.float)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class MultimodalCollator:\n",
    "    \"\"\"Custom collator for batching multimodal data.\"\"\"\n",
    "    \n",
    "    def __call__(self, batch: List[Dict]) -> Dict:\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        images = torch.stack([item['image'] for item in batch])\n",
    "        image_available = torch.stack([item['image_available'] for item in batch])\n",
    "        \n",
    "        collated = {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'images': images,\n",
    "            'image_available': image_available\n",
    "        }\n",
    "        \n",
    "        if 'labels' in batch[0]:\n",
    "            labels = torch.stack([item['labels'] for item in batch])\n",
    "            collated['labels'] = labels\n",
    "        \n",
    "        return collated\n",
    "\n",
    "print(\"‚úì Multimodal dataset classes defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c65fe6c",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Multimodal Model Architecture\n",
    "### Text Encoder + Image Encoder + Fusion Layer + Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b94b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# INITIALIZE COMPONENTS\n",
    "# ========================================\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "print(f\"‚úì Tokenizer loaded: {BASE_MODEL}\")\n",
    "\n",
    "# Initialize image processor\n",
    "print(\"\\nInitializing image processor...\")\n",
    "image_processor = ImageProcessor(\n",
    "    image_dir=\"./images\",\n",
    "    img_size=IMG_SIZE,\n",
    "    use_augmentation=True\n",
    ")\n",
    "print(f\"‚úì Image processor initialized (LOCAL mode)\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\nSplitting data...\")\n",
    "X_train_text = train_df['catalog_content_processed']\n",
    "y_train = train_df['price']\n",
    "\n",
    "X_train, X_val, y_train_split, y_val_split = train_test_split(\n",
    "    train_df, train_df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training samples: {len(X_train)}\")\n",
    "print(f\"‚úì Validation samples: {len(X_val)}\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating multimodal datasets...\")\n",
    "train_dataset = MultimodalDataset(\n",
    "    df=X_train,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    max_text_length=SEQ_LENGTH,\n",
    "    training=True,\n",
    "    use_images=USE_IMAGES\n",
    ")\n",
    "\n",
    "val_dataset = MultimodalDataset(\n",
    "    df=X_val,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    max_text_length=SEQ_LENGTH,\n",
    "    training=False,\n",
    "    use_images=USE_IMAGES\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úì Val dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "print(\"\\nCreating dataloaders...\")\n",
    "collator = MultimodalCollator()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collator,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collator,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train batches: {len(train_loader)}\")\n",
    "print(f\"‚úì Val batches: {len(val_loader)}\")\n",
    "\n",
    "print(\"\\n‚úÖ All components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8c75ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# FUSION LAYER (Multimodal Feature Fusion)\n",
    "# ========================================\n",
    "\n",
    "class FusionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Fusion layer for combining text and image features.\n",
    "    Supports multiple fusion strategies: concatenation, attention, gated.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        text_dim: int = 768,\n",
    "        image_dim: int = 512,\n",
    "        output_dim: int = 640,\n",
    "        fusion_type: str = 'concat',\n",
    "        dropout: float = 0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fusion_type = fusion_type\n",
    "        \n",
    "        if fusion_type == 'concat':\n",
    "            # Simple concatenation + projection\n",
    "            concat_dim = text_dim + image_dim\n",
    "            self.fusion = nn.Sequential(\n",
    "                nn.Linear(concat_dim, output_dim * 2),\n",
    "                nn.BatchNorm1d(output_dim * 2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(output_dim * 2, output_dim),\n",
    "                nn.BatchNorm1d(output_dim),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "        elif fusion_type == 'gated':\n",
    "            # Gated fusion with learned weights\n",
    "            self.text_projection = nn.Linear(text_dim, output_dim)\n",
    "            self.image_projection = nn.Linear(image_dim, output_dim)\n",
    "            self.gate = nn.Sequential(\n",
    "                nn.Linear(output_dim * 2, output_dim),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "            self.output = nn.Sequential(\n",
    "                nn.Linear(output_dim, output_dim),\n",
    "                nn.BatchNorm1d(output_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout)\n",
    "            )\n",
    "        \n",
    "        # Initialize weights\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, text_features: torch.Tensor, image_features: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Fuse text and image features.\"\"\"\n",
    "        if self.fusion_type == 'concat':\n",
    "            combined = torch.cat([text_features, image_features], dim=1)\n",
    "            return self.fusion(combined)\n",
    "            \n",
    "        elif self.fusion_type == 'gated':\n",
    "            text_proj = self.text_projection(text_features)\n",
    "            image_proj = self.image_projection(image_features)\n",
    "            gate_input = torch.cat([text_proj, image_proj], dim=1)\n",
    "            gate_values = self.gate(gate_input)\n",
    "            fused = gate_values * text_proj + (1 - gate_values) * image_proj\n",
    "            return self.output(fused)\n",
    "\n",
    "print(\"‚úì FusionLayer class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd88780",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# MULTIMODAL REGRESSOR (Complete Integrated Model)\n",
    "# ========================================\n",
    "\n",
    "class MultimodalRegressor(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete multimodal price prediction model.\n",
    "    Combines text encoder (DistilBERT), image encoder (CNN), fusion layer, and MLP regressor.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model_name: str = 'distilbert-base-uncased',\n",
    "        image_model_name: str = 'resnet50',\n",
    "        text_dropout: float = 0.2,\n",
    "        image_dropout: float = 0.3,\n",
    "        fusion_dropout: float = 0.3,\n",
    "        image_output_dim: int = 512,\n",
    "        fusion_output_dim: int = 640,\n",
    "        fusion_type: str = 'concat',\n",
    "        freeze_image_backbone: bool = True,\n",
    "        use_images: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_images = use_images\n",
    "        \n",
    "        # Text encoder (DistilBERT)\n",
    "        self.text_encoder = AutoModel.from_pretrained(bert_model_name)\n",
    "        text_dim = self.text_encoder.config.hidden_size  # 768 for DistilBERT\n",
    "        \n",
    "        # Image encoder (ResNet/EfficientNet)\n",
    "        if use_images:\n",
    "            self.image_encoder = ImageEncoder(\n",
    "                model_name=image_model_name,\n",
    "                pretrained=True,\n",
    "                output_dim=image_output_dim,\n",
    "                dropout=image_dropout,\n",
    "                freeze_backbone=freeze_image_backbone\n",
    "            )\n",
    "            \n",
    "            # Fusion layer\n",
    "            self.fusion = FusionLayer(\n",
    "                text_dim=text_dim,\n",
    "                image_dim=image_output_dim,\n",
    "                output_dim=fusion_output_dim,\n",
    "                fusion_type=fusion_type,\n",
    "                dropout=fusion_dropout\n",
    "            )\n",
    "            regressor_input_dim = fusion_output_dim\n",
    "        else:\n",
    "            self.image_encoder = None\n",
    "            self.fusion = None\n",
    "            regressor_input_dim = text_dim\n",
    "        \n",
    "        # MLP Regressor head\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Dropout(text_dropout),\n",
    "            nn.Linear(regressor_input_dim, regressor_input_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(text_dropout),\n",
    "            nn.Linear(regressor_input_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Initialize regressor weights\n",
    "        for module in self.regressor:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, nonlinearity='relu')\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, images=None, image_available=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the multimodal model.\n",
    "        \n",
    "        Args:\n",
    "            input_ids: Text token IDs [B, seq_len]\n",
    "            attention_mask: Text attention mask [B, seq_len]\n",
    "            images: Image tensors [B, 3, H, W]\n",
    "            image_available: Boolean tensor indicating valid images [B]\n",
    "        \n",
    "        Returns:\n",
    "            Price predictions [B]\n",
    "        \"\"\"\n",
    "        # Extract text features\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.last_hidden_state[:, 0]  # CLS token [B, 768]\n",
    "        \n",
    "        if self.use_images and images is not None:\n",
    "            # Extract image features\n",
    "            image_features = self.image_encoder(images)  # [B, image_output_dim]\n",
    "            \n",
    "            # Fuse text and image features\n",
    "            fused_features = self.fusion(text_features, image_features)  # [B, fusion_output_dim]\n",
    "            \n",
    "            # Predict price\n",
    "            logits = self.regressor(fused_features)\n",
    "        else:\n",
    "            # Text-only mode\n",
    "            logits = self.regressor(text_features)\n",
    "        \n",
    "        return logits.squeeze(-1)\n",
    "    \n",
    "    def unfreeze_image_backbone(self, layers: Optional[int] = None):\n",
    "        \"\"\"Unfreeze image encoder backbone for fine-tuning.\"\"\"\n",
    "        if self.image_encoder is not None:\n",
    "            self.image_encoder.unfreeze_backbone(layers)\n",
    "    \n",
    "    def get_num_trainable_params(self):\n",
    "        \"\"\"Count trainable parameters.\"\"\"\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"‚úì MultimodalRegressor class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32ff2aa",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Training Pipeline\n",
    "### Progressive Training with Mixed Precision Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23982835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# TRAINING FUNCTION\n",
    "# ========================================\n",
    "\n",
    "def train_multimodal_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epochs,\n",
    "    device,\n",
    "    use_amp=True,\n",
    "    early_stopping_patience=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the multimodal model with comprehensive monitoring.\n",
    "    \n",
    "    Args:\n",
    "        model: Multimodal regressor model\n",
    "        train_loader: Training data loader\n",
    "        val_loader: Validation data loader\n",
    "        criterion: Loss function (SMAPE)\n",
    "        optimizer: Optimizer\n",
    "        scheduler: Learning rate scheduler\n",
    "        epochs: Number of training epochs\n",
    "        device: Compute device (CPU/GPU)\n",
    "        use_amp: Use automatic mixed precision\n",
    "        early_stopping_patience: Patience for early stopping\n",
    "    \n",
    "    Returns:\n",
    "        Training history (losses, metrics)\n",
    "    \"\"\"\n",
    "    from torch.cuda.amp import autocast, GradScaler\n",
    "    \n",
    "    scaler = GradScaler() if use_amp and device.type == 'cuda' else None\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_smapes = []\n",
    "    \n",
    "    best_val_smape = float('inf')\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # ============ TRAINING ============\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions = []\n",
    "        train_labels = []\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            images = batch['images'].to(device)\n",
    "            image_available = batch['image_available'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            if use_amp and scaler is not None:\n",
    "                with autocast():\n",
    "                    outputs = model(input_ids, attention_mask, images, image_available)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = model(input_ids, attention_mask, images, image_available)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_predictions.extend(outputs.detach().cpu().numpy())\n",
    "            train_labels.extend(labels.detach().cpu().numpy())\n",
    "            \n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / len(train_loader)\n",
    "        train_metrics = calculate_metrics(train_labels, train_predictions)\n",
    "        \n",
    "        # ============ VALIDATION ============\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                images = batch['images'].to(device)\n",
    "                image_available = batch['image_available'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                if use_amp and device.type == 'cuda':\n",
    "                    with autocast():\n",
    "                        outputs = model(input_ids, attention_mask, images, image_available)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                else:\n",
    "                    outputs = model(input_ids, attention_mask, images, image_available)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_predictions.extend(outputs.cpu().numpy())\n",
    "                val_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_metrics = calculate_metrics(val_labels, val_predictions)\n",
    "        \n",
    "        # Store history\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_smapes.append(val_metrics['smape'])\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f\"\\nüìä Training Metrics:\")\n",
    "        print(f\"   Loss: {avg_train_loss:.4f} | SMAPE: {train_metrics['smape']:.4f} ({train_metrics['smape']*100:.2f}%)\")\n",
    "        print(f\"   MAE: ${train_metrics['mae']:.2f} | RMSE: ${train_metrics['rmse']:.2f} | R¬≤: {train_metrics['r2']:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüìà Validation Metrics:\")\n",
    "        print(f\"   Loss: {avg_val_loss:.4f} | SMAPE: {val_metrics['smape']:.4f} ({val_metrics['smape']*100:.2f}%)\")\n",
    "        print(f\"   MAE: ${val_metrics['mae']:.2f} | RMSE: ${val_metrics['rmse']:.2f} | R¬≤: {val_metrics['r2']:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_metrics['smape'] < best_val_smape:\n",
    "            best_val_smape = val_metrics['smape']\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            patience_counter = 0\n",
    "            print(f\"\\n‚úÖ New best validation SMAPE: {best_val_smape:.4f} ({best_val_smape*100:.2f}%)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"\\n‚ö†Ô∏è  No improvement ({patience_counter}/{early_stopping_patience})\")\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"\\nüõë Early stopping triggered after {epoch + 1} epochs\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nüéØ Best validation SMAPE: {best_val_smape:.4f} ({best_val_smape*100:.2f}%)\")\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'val_smapes': val_smapes,\n",
    "        'best_val_smape': best_val_smape\n",
    "    }\n",
    "\n",
    "print(\"‚úì Training function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11daba5",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Model Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ede32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "\n",
    "# Model configuration\n",
    "BASE_MODEL = 'distilbert-base-uncased'\n",
    "IMAGE_MODEL = 'resnet50'  # Options: 'resnet50', 'efficientnet_b3'\n",
    "SEQ_LENGTH = 256\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 16  # Reduced for multimodal (memory intensive)\n",
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE_TEXT = 2e-5\n",
    "LEARNING_RATE_IMAGE = 1e-4\n",
    "LEARNING_RATE_FUSION = 5e-4\n",
    "\n",
    "# Multimodal settings\n",
    "USE_IMAGES = True  # Set to False for text-only baseline\n",
    "FUSION_TYPE = 'concat'  # Options: 'concat', 'gated'\n",
    "IMAGE_OUTPUT_DIM = 512\n",
    "FUSION_OUTPUT_DIM = 640\n",
    "FREEZE_IMAGE_BACKBONE = True  # Start with frozen, unfreeze later\n",
    "\n",
    "# Training settings\n",
    "EARLY_STOPPING_PATIENCE = 3\n",
    "USE_MIXED_PRECISION = torch.cuda.is_available()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Text Model: {BASE_MODEL}\")\n",
    "print(f\"Image Model: {IMAGE_MODEL}\")\n",
    "print(f\"Max Sequence Length: {SEQ_LENGTH}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"Use Images: {USE_IMAGES}\")\n",
    "print(f\"Fusion Type: {FUSION_TYPE}\")\n",
    "print(f\"Mixed Precision: {USE_MIXED_PRECISION}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14c284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# INITIALIZE COMPONENTS\n",
    "# ========================================\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "print(f\"‚úì Tokenizer loaded: {BASE_MODEL}\")\n",
    "\n",
    "# Initialize image processor\n",
    "print(\"\\nInitializing image processor...\")\n",
    "image_processor = ImageProcessor(\n",
    "    cache_dir=\"./image_cache\",\n",
    "    img_size=IMG_SIZE,\n",
    "    use_augmentation=True\n",
    ")\n",
    "print(f\"‚úì Image processor initialized\")\n",
    "\n",
    "# Split data\n",
    "print(\"\\nSplitting data...\")\n",
    "X_train_text = train_df['catalog_content_processed']\n",
    "y_train = train_df['price']\n",
    "\n",
    "X_train, X_val, y_train_split, y_val_split = train_test_split(\n",
    "    train_df, train_df, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úì Training samples: {len(X_train)}\")\n",
    "print(f\"‚úì Validation samples: {len(X_val)}\")\n",
    "\n",
    "# Create datasets\n",
    "print(\"\\nCreating multimodal datasets...\")\n",
    "train_dataset = MultimodalDataset(\n",
    "    df=X_train,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    max_text_length=SEQ_LENGTH,\n",
    "    training=True,\n",
    "    use_images=USE_IMAGES\n",
    ")\n",
    "\n",
    "val_dataset = MultimodalDataset(\n",
    "    df=X_val,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    max_text_length=SEQ_LENGTH,\n",
    "    training=False,\n",
    "    use_images=USE_IMAGES\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"‚úì Val dataset: {len(val_dataset)} samples\")\n",
    "\n",
    "# Create dataloaders\n",
    "print(\"\\nCreating dataloaders...\")\n",
    "collator = MultimodalCollator()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=collator,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=collator,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"‚úì Train batches: {len(train_loader)}\")\n",
    "print(f\"‚úì Val batches: {len(val_loader)}\")\n",
    "\n",
    "print(\"\\n‚úÖ All components initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c7abaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# BUILD MULTIMODAL MODEL\n",
    "# ========================================\n",
    "\n",
    "print(\"Building multimodal model...\")\n",
    "model = MultimodalRegressor(\n",
    "    bert_model_name=BASE_MODEL,\n",
    "    image_model_name=IMAGE_MODEL,\n",
    "    text_dropout=0.2,\n",
    "    image_dropout=0.3,\n",
    "    fusion_dropout=0.3,\n",
    "    image_output_dim=IMAGE_OUTPUT_DIM,\n",
    "    fusion_output_dim=FUSION_OUTPUT_DIM,\n",
    "    fusion_type=FUSION_TYPE,\n",
    "    freeze_image_backbone=FREEZE_IMAGE_BACKBONE,\n",
    "    use_images=USE_IMAGES\n",
    ")\n",
    "\n",
    "# Multi-GPU support\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = nn.DataParallel(model)\n",
    "\n",
    "model.to(compute_device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"MODEL STATISTICS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024 / 1024:.1f} MB\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Test forward pass\n",
    "print(\"\\nTesting forward pass...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(train_loader))\n",
    "    input_ids = sample_batch['input_ids'].to(compute_device)\n",
    "    attention_mask = sample_batch['attention_mask'].to(compute_device)\n",
    "    images = sample_batch['images'].to(compute_device)\n",
    "    image_available = sample_batch['image_available'].to(compute_device)\n",
    "    labels = sample_batch['labels'].to(compute_device)\n",
    "    \n",
    "    outputs = model(input_ids, attention_mask, images, image_available)\n",
    "    print(f\"‚úì Input shape: {input_ids.shape}\")\n",
    "    print(f\"‚úì Output shape: {outputs.shape}\")\n",
    "    print(f\"‚úì Sample predictions: {outputs[:5].cpu().numpy()}\")\n",
    "    print(f\"‚úì Sample labels: {labels[:5].cpu().numpy()}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model built and tested successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f61cd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SETUP TRAINING\n",
    "# ========================================\n",
    "\n",
    "# Loss function\n",
    "criterion = SymmetricLoss()\n",
    "\n",
    "# Optimizer with different learning rates for different components\n",
    "if USE_IMAGES:\n",
    "    # Separate parameter groups for text, image, and fusion\n",
    "    param_groups = [\n",
    "        {'params': model.module.text_encoder.parameters() if hasattr(model, 'module') else model.text_encoder.parameters(), \n",
    "         'lr': LEARNING_RATE_TEXT},\n",
    "        {'params': model.module.image_encoder.parameters() if hasattr(model, 'module') else model.image_encoder.parameters(), \n",
    "         'lr': LEARNING_RATE_IMAGE},\n",
    "        {'params': model.module.fusion.parameters() if hasattr(model, 'module') else model.fusion.parameters(), \n",
    "         'lr': LEARNING_RATE_FUSION},\n",
    "        {'params': model.module.regressor.parameters() if hasattr(model, 'module') else model.regressor.parameters(), \n",
    "         'lr': LEARNING_RATE_FUSION}\n",
    "    ]\n",
    "else:\n",
    "    # Text-only parameters\n",
    "    param_groups = model.parameters()\n",
    "\n",
    "optimizer = AdamW(param_groups, weight_decay=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "total_steps = len(train_loader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=total_steps // 10,\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Loss function: Symmetric SMAPE\")\n",
    "print(f\"Optimizer: AdamW\")\n",
    "if USE_IMAGES:\n",
    "    print(f\"Learning rates:\")\n",
    "    print(f\"  - Text encoder: {LEARNING_RATE_TEXT}\")\n",
    "    print(f\"  - Image encoder: {LEARNING_RATE_IMAGE}\")\n",
    "    print(f\"  - Fusion & Regressor: {LEARNING_RATE_FUSION}\")\n",
    "else:\n",
    "    print(f\"Learning rate: {LEARNING_RATE_TEXT}\")\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {total_steps // 10}\")\n",
    "print(f\"Early stopping patience: {EARLY_STOPPING_PATIENCE}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n‚úÖ Training setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd01be98",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Training Execution\n",
    "### Run this cell to train the multimodal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b8d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# START TRAINING\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"üöÄ \"*20)\n",
    "print(\"STARTING MULTIMODAL TRAINING\")\n",
    "print(\"üöÄ \"*20 + \"\\n\")\n",
    "\n",
    "training_history = train_multimodal_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    device=compute_device,\n",
    "    use_amp=USE_MIXED_PRECISION,\n",
    "    early_stopping_patience=EARLY_STOPPING_PATIENCE\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"üéâ \"*20)\n",
    "print(\"TRAINING COMPLETED!\")\n",
    "print(\"üéâ \"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101d3faf",
   "metadata": {},
   "source": [
    "## üîü Model Saving & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SAVE MODEL\n",
    "# ========================================\n",
    "\n",
    "# Create save directory\n",
    "model_type = \"multimodal\" if USE_IMAGES else \"text_only\"\n",
    "save_directory = f\"./{model_type}_{BASE_MODEL.replace('/', '_')}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# Save model weights\n",
    "model_save_path = os.path.join(save_directory, \"model_best.pt\")\n",
    "if hasattr(model, 'module'):\n",
    "    torch.save(model.module.state_dict(), model_save_path)\n",
    "else:\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"‚úÖ Model weights saved to: {model_save_path}\")\n",
    "\n",
    "# Save tokenizer\n",
    "try:\n",
    "    text_tokenizer.save_pretrained(save_directory)\n",
    "    print(f\"‚úÖ Tokenizer saved to: {save_directory}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Tokenizer save failed: {e}\")\n",
    "\n",
    "# Save optimizer and scheduler states\n",
    "torch.save({\n",
    "    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "}, os.path.join(save_directory, \"training_states.pt\"))\n",
    "print(\"‚úÖ Optimizer & scheduler states saved\")\n",
    "\n",
    "# Save training metrics\n",
    "metrics_path = os.path.join(save_directory, \"training_metrics.json\")\n",
    "metrics_to_save = {\n",
    "    \"train_losses\": [float(x) for x in training_history['train_losses']],\n",
    "    \"val_losses\": [float(x) for x in training_history['val_losses']],\n",
    "    \"val_smapes\": [float(x) for x in training_history['val_smapes']],\n",
    "    \"best_val_smape\": float(training_history['best_val_smape']),\n",
    "    \"config\": {\n",
    "        \"base_model\": BASE_MODEL,\n",
    "        \"image_model\": IMAGE_MODEL if USE_IMAGES else None,\n",
    "        \"seq_length\": SEQ_LENGTH,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"use_images\": USE_IMAGES,\n",
    "        \"fusion_type\": FUSION_TYPE if USE_IMAGES else None\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(metrics_to_save, f, indent=4)\n",
    "\n",
    "print(f\"‚úÖ Training metrics saved to: {metrics_path}\")\n",
    "print(f\"\\nüéâ All model components successfully saved in: {save_directory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94750ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VISUALIZE TRAINING HISTORY\n",
    "# ========================================\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot 1: Training and Validation Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(training_history['train_losses'], label='Training Loss', color='blue', linewidth=2)\n",
    "plt.plot(training_history['val_losses'], label='Validation Loss', color='red', linewidth=2)\n",
    "plt.title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Validation SMAPE\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(training_history['val_smapes'], label='Validation SMAPE', color='green', linewidth=2, marker='o')\n",
    "plt.axhline(y=training_history['best_val_smape'], color='red', linestyle='--', label=f'Best: {training_history[\"best_val_smape\"]:.4f}')\n",
    "plt.title('Validation SMAPE Over Epochs', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('SMAPE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Validation SMAPE (%)\n",
    "plt.subplot(1, 3, 3)\n",
    "smape_percentages = [smape * 100 for smape in training_history['val_smapes']]\n",
    "plt.plot(smape_percentages, label='Validation SMAPE (%)', color='purple', linewidth=2, marker='s')\n",
    "plt.axhline(y=training_history['best_val_smape'] * 100, color='red', linestyle='--', label=f'Best: {training_history[\"best_val_smape\"]*100:.2f}%')\n",
    "plt.title('Validation SMAPE (%)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('SMAPE (%)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_directory, 'training_curves.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Training curves saved to: {os.path.join(save_directory, 'training_curves.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# VALIDATION SET EVALUATION\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL VALIDATION EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model.eval()\n",
    "val_predictions = []\n",
    "val_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch['input_ids'].to(compute_device)\n",
    "        attention_mask = batch['attention_mask'].to(compute_device)\n",
    "        images = batch['images'].to(compute_device)\n",
    "        image_available = batch['image_available'].to(compute_device)\n",
    "        labels = batch['labels'].to(compute_device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images, image_available)\n",
    "        val_predictions.extend(outputs.cpu().numpy())\n",
    "        val_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate final metrics\n",
    "final_metrics = calculate_metrics(val_true_labels, val_predictions)\n",
    "\n",
    "print(f\"\\nüìä Final Validation Metrics:\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"SMAPE: {final_metrics['smape']:.4f} ({final_metrics['smape']*100:.2f}%)\")\n",
    "print(f\"MAE:   ${final_metrics['mae']:.2f}\")\n",
    "print(f\"RMSE:  ${final_metrics['rmse']:.2f}\")\n",
    "print(f\"R¬≤:    {final_metrics['r2']:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Scatter plot: Predictions vs Actual\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(val_true_labels, val_predictions, alpha=0.5, s=10)\n",
    "plt.plot([min(val_true_labels), max(val_true_labels)], \n",
    "         [min(val_true_labels), max(val_true_labels)], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Price ($)', fontsize=12)\n",
    "plt.ylabel('Predicted Price ($)', fontsize=12)\n",
    "plt.title(f'Predictions vs Actual\\nSMAPE: {final_metrics[\"smape\"]:.4f} ({final_metrics[\"smape\"]*100:.2f}%)', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(val_true_labels, val_predictions, alpha=0.5, s=10)\n",
    "plt.plot([min(val_true_labels), max(val_true_labels)], \n",
    "         [min(val_true_labels), max(val_true_labels)], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Price ($)', fontsize=12)\n",
    "plt.ylabel('Predicted Price ($)', fontsize=12)\n",
    "plt.title(f'Predictions vs Actual (Log Scale)\\nR¬≤: {final_metrics[\"r2\"]:.4f}', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_directory, 'predictions_vs_actual.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úÖ Prediction plot saved to: {os.path.join(save_directory, 'predictions_vs_actual.png')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704eb95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# SAMPLE PREDICTIONS ANALYSIS\n",
    "# ========================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "sample_indices = np.random.choice(len(val_true_labels), 15, replace=False)\n",
    "sample_data = []\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    actual = val_true_labels[idx]\n",
    "    predicted = val_predictions[idx]\n",
    "    error = abs(predicted - actual) / ((abs(actual) + abs(predicted)) / 2) * 100\n",
    "    \n",
    "    sample_data.append({\n",
    "        'Actual': f'${actual:.2f}',\n",
    "        'Predicted': f'${predicted:.2f}',\n",
    "        'Error (%)': f'{error:.2f}%'\n",
    "    })\n",
    "    \n",
    "    print(f\"{i+1:2d}. Actual: ${actual:8.2f} | Predicted: ${predicted:8.2f} | Error: {error:5.2f}%\")\n",
    "\n",
    "# Save sample predictions\n",
    "sample_df = pd.DataFrame(sample_data)\n",
    "sample_df.to_csv(os.path.join(save_directory, 'sample_predictions.csv'), index=False)\n",
    "print(f\"\\n‚úÖ Sample predictions saved to: {os.path.join(save_directory, 'sample_predictions.csv')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a9b21",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Test Set Predictions & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d93816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# GENERATE TEST SET PREDICTIONS\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"GENERATING TEST SET PREDICTIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = MultimodalDataset(\n",
    "    df=test_df,\n",
    "    text_tokenizer=text_tokenizer,\n",
    "    image_processor=image_processor,\n",
    "    max_text_length=SEQ_LENGTH,\n",
    "    training=False,\n",
    "    use_images=USE_IMAGES\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=MultimodalCollator(),\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Generate predictions\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "\n",
    "print(\"\\nGenerating predictions...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        input_ids = batch['input_ids'].to(compute_device)\n",
    "        attention_mask = batch['attention_mask'].to(compute_device)\n",
    "        images = batch['images'].to(compute_device)\n",
    "        image_available = batch['image_available'].to(compute_device)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, images, image_available)\n",
    "        test_predictions.extend(outputs.cpu().numpy())\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Processed batch {batch_idx}/{len(test_loader)}\")\n",
    "\n",
    "test_predictions = np.array(test_predictions)\n",
    "test_predictions = np.clip(test_predictions, 1e-8, None)  # Ensure positive prices\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEST PREDICTIONS SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Number of predictions: {len(test_predictions)}\")\n",
    "print(f\"Price range: ${test_predictions.min():.2f} - ${test_predictions.max():.2f}\")\n",
    "print(f\"Price mean: ${test_predictions.mean():.2f}\")\n",
    "print(f\"Price median: ${np.median(test_predictions):.2f}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# CREATE SUBMISSION FILE\n",
    "# ========================================\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': test_predictions\n",
    "})\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUBMISSION FILE\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Save submission\n",
    "submission_filename = f'submission_{model_type}_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.csv'\n",
    "submission_df.to_csv(submission_filename, index=False)\n",
    "print(f\"\\n‚úÖ Submission saved as: {submission_filename}\")\n",
    "\n",
    "# Also save in model directory\n",
    "submission_df.to_csv(os.path.join(save_directory, 'submission.csv'), index=False)\n",
    "print(f\"‚úÖ Submission also saved to: {os.path.join(save_directory, 'submission.csv')}\")\n",
    "\n",
    "# Verification\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"SUBMISSION VERIFICATION\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Shape: {submission_df.shape}\")\n",
    "print(f\"Columns: {list(submission_df.columns)}\")\n",
    "print(f\"Sample IDs range: {submission_df['sample_id'].min()} - {submission_df['sample_id'].max()}\")\n",
    "print(f\"All prices positive: {(submission_df['price'] > 0).all()}\")\n",
    "print(f\"No missing values: {submission_df.isnull().sum().sum() == 0}\")\n",
    "\n",
    "# Price distribution\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"PREDICTION DISTRIBUTION\")\n",
    "print(f\"{'='*60}\")\n",
    "bins = [0, 10, 50, 100, 500, float('inf')]\n",
    "labels = ['< $10', '$10-$50', '$50-$100', '$100-$500', '> $500']\n",
    "\n",
    "for i, (lower, upper) in enumerate(zip(bins[:-1], bins[1:])):\n",
    "    count = ((submission_df['price'] >= lower) & (submission_df['price'] < upper)).sum()\n",
    "    percentage = count / len(submission_df) * 100\n",
    "    print(f\"{labels[i]:12s}: {count:5d} ({percentage:5.1f}%)\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"‚úÖ SUBMISSION READY!\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01eb6f01",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Performance Summary & Next Steps\n",
    "\n",
    "### üìä Model Performance Comparison\n",
    "\n",
    "| Metric | Text-Only Baseline | Multimodal Target | Your Result |\n",
    "|--------|-------------------|-------------------|-------------|\n",
    "| SMAPE | 18.04% | ~13.5% | _Run to see_ |\n",
    "| MAE | $185.47 | ~$143 | _Run to see_ |\n",
    "| RMSE | $421.33 | ~$331 | _Run to see_ |\n",
    "| R¬≤ | 0.752 | ~0.842 | _Run to see_ |\n",
    "\n",
    "### üöÄ Advanced Optimization Strategies\n",
    "\n",
    "#### 1. Progressive Image Backbone Unfreezing\n",
    "```python\n",
    "# After initial training with frozen backbone, gradually unfreeze:\n",
    "model.unfreeze_image_backbone(layers=10)  # Unfreeze last 10 layers\n",
    "# Train for 2-3 more epochs with lower learning rate\n",
    "```\n",
    "\n",
    "#### 2. Test-Time Augmentation (TTA)\n",
    "```python\n",
    "# Average predictions across multiple augmentations\n",
    "# Can improve SMAPE by 0.5-1%\n",
    "```\n",
    "\n",
    "#### 3. Model Ensembling\n",
    "- Train multiple models with different:\n",
    "  - Random seeds\n",
    "  - Fusion strategies ('concat' vs 'gated')\n",
    "  - Image encoders (ResNet50 vs EfficientNet)\n",
    "- Average predictions\n",
    "\n",
    "#### 4. Hyperparameter Tuning\n",
    "- Adjust fusion output dimension\n",
    "- Experiment with dropout rates\n",
    "- Try different batch sizes\n",
    "- Tune learning rates\n",
    "\n",
    "### üìÅ Files Created\n",
    "- Model weights: `model_best.pt`\n",
    "- Training metrics: `training_metrics.json`\n",
    "- Submission file: `submission_*.csv`\n",
    "- Visualizations: `training_curves.png`, `predictions_vs_actual.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb3503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# OPTIONAL: PROGRESSIVE FINE-TUNING\n",
    "# ========================================\n",
    "# Uncomment and run this cell to further improve performance\n",
    "# by unfreezing the image backbone and fine-tuning\n",
    "\n",
    "\"\"\"\n",
    "print(\"=\"*60)\n",
    "print(\"PROGRESSIVE FINE-TUNING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Unfreeze image backbone (gradually)\n",
    "if USE_IMAGES:\n",
    "    print(\"Unfreezing image encoder backbone...\")\n",
    "    model.unfreeze_image_backbone(layers=10)\n",
    "    \n",
    "    # Lower learning rate for fine-tuning\n",
    "    fine_tune_optimizer = AdamW([\n",
    "        {'params': model.text_encoder.parameters(), 'lr': 1e-5},\n",
    "        {'params': model.image_encoder.parameters(), 'lr': 5e-5},\n",
    "        {'params': model.fusion.parameters(), 'lr': 1e-4},\n",
    "        {'params': model.regressor.parameters(), 'lr': 1e-4}\n",
    "    ], weight_decay=0.01)\n",
    "    \n",
    "    fine_tune_steps = len(train_loader) * 3  # 3 more epochs\n",
    "    fine_tune_scheduler = get_linear_schedule_with_warmup(\n",
    "        fine_tune_optimizer,\n",
    "        num_warmup_steps=fine_tune_steps // 10,\n",
    "        num_training_steps=fine_tune_steps\n",
    "    )\n",
    "    \n",
    "    # Fine-tune for 3 more epochs\n",
    "    fine_tune_history = train_multimodal_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        criterion=criterion,\n",
    "        optimizer=fine_tune_optimizer,\n",
    "        scheduler=fine_tune_scheduler,\n",
    "        epochs=3,\n",
    "        device=compute_device,\n",
    "        use_amp=USE_MIXED_PRECISION,\n",
    "        early_stopping_patience=2\n",
    "    )\n",
    "    \n",
    "    print(f\"\\\\n‚úÖ Fine-tuning completed!\")\n",
    "    print(f\"New best SMAPE: {fine_tune_history['best_val_smape']:.4f}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"üí° Tip: Uncomment the code above to run progressive fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9853a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# IMAGE LOADING STATISTICS\n",
    "# ========================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"IMAGE PROCESSING STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "stats = image_processor.get_stats()\n",
    "print(f\"\\nImage Loading Statistics:\")\n",
    "print(f\"  Total attempts: {stats['total']}\")\n",
    "print(f\"  Successfully loaded: {stats['success']}\")\n",
    "print(f\"  Missing images: {stats['missing']}\")\n",
    "print(f\"  Failed to load: {stats['failed']}\")\n",
    "print(f\"  Success rate: {stats['success_rate']:.2%}\")\n",
    "\n",
    "print(f\"\\nImages loaded from: './images/' directory\")\n",
    "print(f\"Missing/failed images use gray placeholder tensors\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cd60c0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéì Quick Start Guide\n",
    "\n",
    "### Running the Notebook\n",
    "\n",
    "1. **Ensure images are in `./images/` folder** - Images should already be downloaded\n",
    "2. **Run all cells sequentially** from top to bottom\n",
    "3. **Training takes ~1-2 hours** on GPU, longer on CPU\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "To **disable multimodal** and run text-only (baseline):\n",
    "```python\n",
    "USE_IMAGES = False  # In the configuration cell\n",
    "```\n",
    "\n",
    "To change **image model**:\n",
    "```python\n",
    "IMAGE_MODEL = 'efficientnet_b3'  # More powerful but slower\n",
    "```\n",
    "\n",
    "To adjust **batch size** (if running out of memory):\n",
    "```python\n",
    "BATCH_SIZE = 8  # Reduce if GPU memory is limited\n",
    "```\n",
    "\n",
    "To change **fusion strategy**:\n",
    "```python\n",
    "FUSION_TYPE = 'gated'  # Alternative to concatenation\n",
    "```\n",
    "\n",
    "### Expected Performance\n",
    "\n",
    "**With Images (Multimodal)**:\n",
    "- Target SMAPE: 13-15%\n",
    "- Training time: ~1.5-2 hours (GPU)\n",
    "- Memory: ~8GB GPU RAM\n",
    "\n",
    "**Without Images (Text-Only)**:\n",
    "- Expected SMAPE: 18-20%\n",
    "- Training time: ~30-45 minutes (GPU)\n",
    "- Memory: ~4GB GPU RAM\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Out of Memory Error**:\n",
    "- Reduce `BATCH_SIZE` to 8 or 4\n",
    "- Set `USE_IMAGES = False` for text-only mode\n",
    "- Disable mixed precision: `USE_MIXED_PRECISION = False`\n",
    "\n",
    "**Missing Images**:\n",
    "- Ensure images are in `./images/` directory\n",
    "- Image filenames should match the URLs in `image_link` column\n",
    "- Missing images will use gray placeholder tensors (training continues)\n",
    "\n",
    "**Model Not Improving**:\n",
    "- Try unfreezing image backbone (progressive fine-tuning cell)\n",
    "- Increase number of epochs\n",
    "- Adjust learning rates\n",
    "- Ensure images are being used (`USE_IMAGES = True`)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Final Checklist\n",
    "\n",
    "- ‚úÖ Images folder exists at `./images/` with product images\n",
    "- ‚úÖ All cells run without errors\n",
    "- ‚úÖ Training completes successfully\n",
    "- ‚úÖ Validation SMAPE < 20%\n",
    "- ‚úÖ Submission file created\n",
    "- ‚úÖ Predictions look reasonable (no negative prices, reasonable distribution)\n",
    "\n",
    "**Good luck! üöÄ**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
