{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35443acf",
   "metadata": {},
   "source": [
    "# Smart Product Pricing Challenge - Multimodal ML Solution\n",
    "\n",
    "This notebook implements an end-to-end solution for predicting product prices using both text (catalog_content) and images (image_link).\n",
    "\n",
    "**Approach:**\n",
    "1. Extract features from text using DistilBERT transformer\n",
    "2. Extract features from images using ResNet50 CNN\n",
    "3. Extract Item Pack Quantity (IPQ) feature from text\n",
    "4. Fuse all features and train a regression model\n",
    "5. Optimize for SMAPE metric using MAE loss\n",
    "\n",
    "**Evaluation Metric:** SMAPE (Symmetric Mean Absolute Percentage Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b9f344",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d1a994",
   "metadata": {},
   "source": [
    "## ⚡ ULTRA-FAST TRAINING MODE ENABLED\n",
    "\n",
    "This notebook has been **heavily optimized** for speed:\n",
    "\n",
    "### Quick Start Guide:\n",
    "1. **Run Cell 3** - Check if GPU is available\n",
    "2. **If GPU shows \"False\"**:\n",
    "   - Install CUDA PyTorch: `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118`\n",
    "   - OR use Google Colab/Kaggle with free GPU\n",
    "3. **For testing**: Set `USE_SUBSET = True` in the data loading section\n",
    "4. **Run all cells** - Training should complete in minutes!\n",
    "\n",
    "### Current Settings:\n",
    "- 🎯 **Batch Size**: 128 (fast processing)\n",
    "- 🎯 **Epochs**: 3 (quick convergence)  \n",
    "- 🎯 **Model**: MobileNetV2 (ultra-light)\n",
    "- 🎯 **Mixed Precision**: Auto-enabled on GPU\n",
    "- 🎯 **Text Length**: 128 tokens (4x faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "18156502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "# Transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Utility functions\n",
    "sys.path.append('..')\n",
    "from utils import download_images\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0198d747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM DIAGNOSTICS\n",
      "============================================================\n",
      "PyTorch Version: 2.8.0+cu128\n",
      "CUDA Available: True\n",
      "✓ GPU ENABLED\n",
      "GPU Name: NVIDIA GeForce RTX 3050 6GB Laptop GPU\n",
      "GPU Count: 1\n",
      "CUDA Version: 12.8\n",
      "GPU Memory: 5.67 GB\n",
      "\n",
      "cuDNN Available: True\n",
      "cuDNN Enabled: True\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# GPU Verification and Optimization Check\n",
    "import torch\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SYSTEM DIAGNOSTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check PyTorch and CUDA\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU ENABLED\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️  WARNING: NO GPU DETECTED - Running on CPU will be VERY SLOW\")\n",
    "    print(\"\\nTo enable GPU:\")\n",
    "    print(\"1. Install CUDA toolkit from NVIDIA\")\n",
    "    print(\"2. Install PyTorch with CUDA support:\")\n",
    "    print(\"   pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\")\n",
    "    print(\"\\nOr use Google Colab/Kaggle for free GPU access\")\n",
    "\n",
    "# Check if cuDNN is available\n",
    "print(f\"\\ncuDNN Available: {torch.backends.cudnn.is_available()}\")\n",
    "print(f\"cuDNN Enabled: {torch.backends.cudnn.enabled}\")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b886a",
   "metadata": {},
   "source": [
    "## Step 2: Configuration and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5383c5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ GPU OPTIMIZATIONS ENABLED\n",
      "  - cuDNN Benchmark: True\n",
      "  - Pin Memory: True\n",
      "\n",
      "Device: cuda\n",
      "Batch Size: 128\n",
      "Epochs: 3\n",
      "Image Size: 224x224\n",
      "Text Length: 128 tokens\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    TRAIN_CSV = '../dataset/train.csv'\n",
    "    TEST_CSV = '../dataset/test.csv'\n",
    "    IMAGE_FOLDER = './images/'\n",
    "    SUBMISSION_FILE = 'submission.csv'\n",
    "    MODEL_SAVE_PATH = 'best_model.pth'\n",
    "    \n",
    "    # Image settings\n",
    "    IMG_SIZE = 224\n",
    "    \n",
    "    # Training settings - OPTIMIZED FOR SPEED\n",
    "    BATCH_SIZE = 128  # Increased for faster training (reduce if out of memory)\n",
    "    EPOCHS = 3  # Reduced for faster completion\n",
    "    LEARNING_RATE = 3e-4  # Higher for faster convergence\n",
    "    WEIGHT_DECAY = 1e-5\n",
    "    \n",
    "    # Model settings\n",
    "    IMAGE_EMBED_DIM = 64  # Reduced from 128\n",
    "    TEXT_EMBED_DIM = 64   # Reduced from 128\n",
    "    DROPOUT = 0.2  # Reduced dropout for faster training\n",
    "    \n",
    "    # Text settings\n",
    "    MAX_TEXT_LENGTH = 128  # Heavily reduced for speed (was 512)\n",
    "    TRANSFORMER_MODEL = 'distilbert-base-uncased'\n",
    "    \n",
    "    # Other settings\n",
    "    RANDOM_SEED = 42\n",
    "    VAL_SPLIT = 0.1\n",
    "    NUM_WORKERS = 0  # Set to 0 for stability, increase to 2-4 if multiprocessing works\n",
    "    \n",
    "    # Device - FORCE GPU IF AVAILABLE\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Performance settings\n",
    "    PIN_MEMORY = torch.cuda.is_available()  # Enable for GPU\n",
    "    PERSISTENT_WORKERS = False  # Set True if NUM_WORKERS > 0\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# Enable cuDNN benchmarking for faster training\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    print(\"✓ GPU OPTIMIZATIONS ENABLED\")\n",
    "    print(f\"  - cuDNN Benchmark: {torch.backends.cudnn.benchmark}\")\n",
    "    print(f\"  - Pin Memory: {config.PIN_MEMORY}\")\n",
    "\n",
    "print(f\"\\nDevice: {config.DEVICE}\")\n",
    "print(f\"Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"Epochs: {config.EPOCHS}\")\n",
    "print(f\"Image Size: {config.IMG_SIZE}x{config.IMG_SIZE}\")\n",
    "print(f\"Text Length: {config.MAX_TEXT_LENGTH} tokens\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(config.RANDOM_SEED)\n",
    "np.random.seed(config.RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(config.RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(config.RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791274a2",
   "metadata": {},
   "source": [
    "## Step 3: Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4ed7f9",
   "metadata": {},
   "source": [
    "## 🚀 EXTREME SPEED OPTIMIZATIONS APPLIED\n",
    "\n",
    "### What's Been Optimized:\n",
    "1. **Model Architecture**: MobileNetV2 (10x lighter than ResNet50)\n",
    "2. **Batch Size**: 128 (process more samples at once)\n",
    "3. **Epochs**: 3 (faster training, still good results)\n",
    "4. **Text Length**: 128 tokens (4x faster text processing)\n",
    "5. **Embeddings**: 64-dim instead of 128-dim\n",
    "6. **Frozen Layers**: All pretrained layers frozen (only train fusion head)\n",
    "7. **GPU Optimizations**: \n",
    "   - cuDNN benchmark enabled\n",
    "   - Pin memory enabled\n",
    "   - Mixed precision (AMP) enabled\n",
    "8. **Simplified Architecture**: Minimal trainable parameters\n",
    "\n",
    "### Expected Speed Improvement:\n",
    "- **With GPU**: ~2-5 min per epoch (vs 15-30 min before)\n",
    "- **With CPU**: ~10-20 min per epoch (vs 60+ min before)\n",
    "- **With Data Subset**: ~30 sec - 2 min per epoch\n",
    "\n",
    "### To Go Even Faster:\n",
    "- Set `USE_SUBSET = True` in the next section\n",
    "- Increase `BATCH_SIZE` to 256 (if you have enough GPU memory)\n",
    "- Reduce `EPOCHS` to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "38299521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 3)\n",
      "\n",
      "Train data sample:\n",
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
      "\n",
      "Test data sample:\n",
      "   sample_id                                    catalog_content  \\\n",
      "0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
      "1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n",
      "\n",
      "                                          image_link  \n",
      "0  https://m.media-amazon.com/images/I/71hoAn78AW...  \n",
      "1  https://m.media-amazon.com/images/I/61ex8NHCIj...  \n",
      "Train shape: (75000, 4)\n",
      "Test shape: (75000, 3)\n",
      "\n",
      "Train data sample:\n",
      "   sample_id                                    catalog_content  \\\n",
      "0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n",
      "1     198967  Item Name: Salerno Cookies, The Original Butte...   \n",
      "\n",
      "                                          image_link  price  \n",
      "0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  \n",
      "1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  \n",
      "\n",
      "Test data sample:\n",
      "   sample_id                                    catalog_content  \\\n",
      "0     100179  Item Name: Rani 14-Spice Eshamaya's Mango Chut...   \n",
      "1     245611  Item Name: Natural MILK TEA Flavoring extract ...   \n",
      "\n",
      "                                          image_link  \n",
      "0  https://m.media-amazon.com/images/I/71hoAn78AW...  \n",
      "1  https://m.media-amazon.com/images/I/61ex8NHCIj...  \n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_df = pd.read_csv(config.TRAIN_CSV)\n",
    "test_df = pd.read_csv(config.TEST_CSV)\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}\")\n",
    "print(f\"Test shape: {test_df.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nTrain data sample:\")\n",
    "print(train_df.head(2))\n",
    "\n",
    "print(\"\\nTest data sample:\")\n",
    "print(test_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e51afc",
   "metadata": {},
   "source": [
    "### ⚡ SPEED OPTIMIZATION: Use Data Subset\n",
    "**For ultra-fast testing/debugging**, enable this to use only a portion of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f4d1d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using full dataset - Train: 75000, Test: 75000\n"
     ]
    }
   ],
   "source": [
    "# ⚡ SPEED OPTIMIZATION: Use subset for ultra-fast training\n",
    "USE_SUBSET = False  # Set to True for 10x faster training (for testing)\n",
    "SUBSET_TRAIN_SIZE = 5000  # Number of training samples to use\n",
    "SUBSET_TEST_SIZE = 1000   # Number of test samples to use\n",
    "\n",
    "if USE_SUBSET:\n",
    "    print(\"⚡ USING DATA SUBSET FOR FAST TRAINING\")\n",
    "    if len(train_df) > SUBSET_TRAIN_SIZE:\n",
    "        train_df = train_df.sample(n=SUBSET_TRAIN_SIZE, random_state=config.RANDOM_SEED).reset_index(drop=True)\n",
    "    if len(test_df) > SUBSET_TEST_SIZE:\n",
    "        test_df = test_df.sample(n=SUBSET_TEST_SIZE, random_state=config.RANDOM_SEED).reset_index(drop=True)\n",
    "    print(f\"Train subset: {len(train_df)} samples\")\n",
    "    print(f\"Test subset: {len(test_df)} samples\")\n",
    "    print(\"Training will be 10-20x faster!\")\n",
    "else:\n",
    "    print(f\"Using full dataset - Train: {len(train_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "49ebd496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in train data:\n",
      "sample_id          0\n",
      "catalog_content    0\n",
      "image_link         0\n",
      "price              0\n",
      "dtype: int64\n",
      "\n",
      "Missing values in test data:\n",
      "sample_id          0\n",
      "catalog_content    0\n",
      "image_link         0\n",
      "dtype: int64\n",
      "\n",
      "Price statistics:\n",
      "count    75000.000000\n",
      "mean        23.647654\n",
      "std         33.376932\n",
      "min          0.130000\n",
      "25%          6.795000\n",
      "50%         14.000000\n",
      "75%         28.625000\n",
      "max       2796.000000\n",
      "Name: price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in train data:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "print(\"\\nMissing values in test data:\")\n",
    "print(test_df.isnull().sum())\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nPrice statistics:\")\n",
    "print(train_df['price'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dac1d7",
   "metadata": {},
   "source": [
    "## Step 4: Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58eb1a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train data...\n",
      "Extracting Item Pack Quantity (IPQ)...\n",
      "Cleaning text...\n",
      "Cleaning text...\n",
      "IPQ statistics:\n",
      "count    7.500000e+04\n",
      "mean     1.138032e+07\n",
      "std      3.116622e+09\n",
      "min      0.000000e+00\n",
      "25%      3.000000e+00\n",
      "50%      8.000000e+00\n",
      "75%      1.800000e+01\n",
      "max      8.535220e+11\n",
      "Name: ipq, dtype: float64\n",
      "\n",
      "Preprocessing test data...\n",
      "Extracting Item Pack Quantity (IPQ)...\n",
      "IPQ statistics:\n",
      "count    7.500000e+04\n",
      "mean     1.138032e+07\n",
      "std      3.116622e+09\n",
      "min      0.000000e+00\n",
      "25%      3.000000e+00\n",
      "50%      8.000000e+00\n",
      "75%      1.800000e+01\n",
      "max      8.535220e+11\n",
      "Name: ipq, dtype: float64\n",
      "\n",
      "Preprocessing test data...\n",
      "Extracting Item Pack Quantity (IPQ)...\n",
      "Cleaning text...\n",
      "Cleaning text...\n",
      "IPQ statistics:\n",
      "count    7.500000e+04\n",
      "mean     2.393837e+06\n",
      "std      3.961346e+08\n",
      "min      0.000000e+00\n",
      "25%      3.000000e+00\n",
      "50%      8.000000e+00\n",
      "75%      1.820000e+01\n",
      "max      7.520001e+10\n",
      "Name: ipq, dtype: float64\n",
      "IPQ statistics:\n",
      "count    7.500000e+04\n",
      "mean     2.393837e+06\n",
      "std      3.961346e+08\n",
      "min      0.000000e+00\n",
      "25%      3.000000e+00\n",
      "50%      8.000000e+00\n",
      "75%      1.820000e+01\n",
      "max      7.520001e+10\n",
      "Name: ipq, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def extract_ipq(text):\n",
    "    \"\"\"\n",
    "    Extract Item Pack Quantity (IPQ) from catalog content.\n",
    "    Looks for patterns like 'Item Pack Quantity: 30' or 'Pack of 5', etc.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return 1.0\n",
    "    \n",
    "    # Pattern 1: Item Pack Quantity: X\n",
    "    pattern1 = r'Item Pack Quantity[:\\s]+([0-9]+(?:\\.[0-9]+)?)'\n",
    "    match1 = re.search(pattern1, text, re.IGNORECASE)\n",
    "    if match1:\n",
    "        return float(match1.group(1))\n",
    "    \n",
    "    # Pattern 2: Pack of X\n",
    "    pattern2 = r'Pack of ([0-9]+)'\n",
    "    match2 = re.search(pattern2, text, re.IGNORECASE)\n",
    "    if match2:\n",
    "        return float(match2.group(1))\n",
    "    \n",
    "    # Pattern 3: X-Pack or X Pack\n",
    "    pattern3 = r'([0-9]+)[\\s-]*Pack'\n",
    "    match3 = re.search(pattern3, text, re.IGNORECASE)\n",
    "    if match3:\n",
    "        return float(match3.group(1))\n",
    "    \n",
    "    # Pattern 4: Count: X\n",
    "    pattern4 = r'Value[:\\s]+([0-9]+(?:\\.[0-9]+)?)'\n",
    "    match4 = re.search(pattern4, text, re.IGNORECASE)\n",
    "    if match4:\n",
    "        return float(match4.group(1))\n",
    "    \n",
    "    return 1.0\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean catalog content text.\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def preprocess_dataframe(df, is_train=True):\n",
    "    \"\"\"\n",
    "    Preprocess the dataframe:\n",
    "    1. Extract IPQ feature\n",
    "    2. Clean text\n",
    "    3. Handle missing values\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Extract IPQ\n",
    "    print(\"Extracting Item Pack Quantity (IPQ)...\")\n",
    "    df['ipq'] = df['catalog_content'].apply(extract_ipq)\n",
    "    \n",
    "    # Clean text\n",
    "    print(\"Cleaning text...\")\n",
    "    df['cleaned_text'] = df['catalog_content'].apply(clean_text)\n",
    "    \n",
    "    # Handle missing image links\n",
    "    df['image_link'] = df['image_link'].fillna('')\n",
    "    \n",
    "    # Log transform for IPQ\n",
    "    df['ipq_log'] = np.log1p(df['ipq'])\n",
    "    \n",
    "    print(f\"IPQ statistics:\")\n",
    "    print(df['ipq'].describe())\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Preprocess train and test data\n",
    "print(\"Preprocessing train data...\")\n",
    "train_df = preprocess_dataframe(train_df, is_train=True)\n",
    "\n",
    "print(\"\\nPreprocessing test data...\")\n",
    "test_df = preprocess_dataframe(test_df, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9de03727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature engineering complete!\n",
      "\n",
      "Train columns: ['sample_id', 'catalog_content', 'image_link', 'price', 'ipq', 'cleaned_text', 'ipq_log', 'ipq_normalized']\n"
     ]
    }
   ],
   "source": [
    "# Normalize IPQ feature\n",
    "scaler = StandardScaler()\n",
    "train_df['ipq_normalized'] = scaler.fit_transform(train_df[['ipq_log']])\n",
    "test_df['ipq_normalized'] = scaler.transform(test_df[['ipq_log']])\n",
    "\n",
    "print(\"Feature engineering complete!\")\n",
    "print(f\"\\nTrain columns: {train_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3eb80",
   "metadata": {},
   "source": [
    "## Step 5: Download Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96b5801d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique images to download: 140587\n",
      "Images already downloaded: 140585\n",
      "\n",
      "Downloading images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 33217/140587 [00:00<00:01, 56944.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not able to download - https://m.media-amazon.com/images/I/51mjZYDYjyL.jpg\n",
      "HTTP Error 404: Not Found\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 38912/140587 [00:01<00:08, 11628.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Not able to download - https://m.media-amazon.com/images/I/813CjSgHj0S.jpg\n",
      "HTTP Error 404: Not Found\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140587/140587 [00:02<00:00, 56051.42it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image download complete!\n"
     ]
    }
   ],
   "source": [
    "# Create image folder if it doesn't exist\n",
    "os.makedirs(config.IMAGE_FOLDER, exist_ok=True)\n",
    "\n",
    "# Get all unique image links\n",
    "all_image_links = pd.concat([train_df['image_link'], test_df['image_link']]).unique()\n",
    "all_image_links = [link for link in all_image_links if isinstance(link, str) and link.strip() != '']\n",
    "\n",
    "print(f\"Total unique images to download: {len(all_image_links)}\")\n",
    "\n",
    "# Check how many images are already downloaded\n",
    "existing_images = set(os.listdir(config.IMAGE_FOLDER))\n",
    "print(f\"Images already downloaded: {len(existing_images)}\")\n",
    "\n",
    "# Download images (this may take a while)\n",
    "print(\"\\nDownloading images...\")\n",
    "download_images(all_image_links, config.IMAGE_FOLDER)\n",
    "print(\"Image download complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029dfcaf",
   "metadata": {},
   "source": [
    "## Step 6: Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "666dd3e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class PriceDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for multimodal price prediction.\n",
    "    Handles both images and text data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, tokenizer, image_folder, transform=None, is_train=True):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Default image (black image) for missing images\n",
    "        self.default_image = Image.new('RGB', (config.IMG_SIZE, config.IMG_SIZE), color='black')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        # Load and process image\n",
    "        image = self._load_image(row['image_link'])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Tokenize text\n",
    "        text = row['cleaned_text']\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            max_length=config.MAX_TEXT_LENGTH,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = encoded['input_ids'].squeeze(0)\n",
    "        attention_mask = encoded['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Get IPQ feature\n",
    "        ipq = torch.tensor(row['ipq_normalized'], dtype=torch.float32)\n",
    "        \n",
    "        # Prepare output\n",
    "        sample = {\n",
    "            'image': image,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'ipq': ipq,\n",
    "            'sample_id': row['sample_id']\n",
    "        }\n",
    "        \n",
    "        # Add label if training\n",
    "        if self.is_train:\n",
    "            price = torch.tensor(row['price'], dtype=torch.float32)\n",
    "            sample['price'] = price\n",
    "        \n",
    "        return sample\n",
    "    \n",
    "    def _load_image(self, image_link):\n",
    "        \"\"\"\n",
    "        Load image from disk or return default image if not found.\n",
    "        \"\"\"\n",
    "        if pd.isna(image_link) or not isinstance(image_link, str) or image_link.strip() == '':\n",
    "            return self.default_image\n",
    "        \n",
    "        filename = Path(image_link).name\n",
    "        image_path = os.path.join(self.image_folder, filename)\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            # Return default image if loading fails\n",
    "            return self.default_image\n",
    "\n",
    "\n",
    "# Define image transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"Dataset class defined successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ffffb",
   "metadata": {},
   "source": [
    "## Step 7: Train/Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "96c62e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 67500\n",
      "Validation samples: 7500\n",
      "Test samples: 75000\n"
     ]
    }
   ],
   "source": [
    "# Split train data into train and validation\n",
    "train_data, val_data = train_test_split(\n",
    "    train_df, \n",
    "    test_size=config.VAL_SPLIT, \n",
    "    random_state=config.RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692c383b",
   "metadata": {},
   "source": [
    "## Step 8: Initialize Tokenizer and Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c798310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Creating datasets...\n",
      "Creating data loaders with GPU optimizations...\n",
      "✓ Data loaders created successfully!\n",
      "  Train batches: 528 (batch size: 128)\n",
      "  Validation batches: 59\n",
      "  Test batches: 586\n",
      "Creating datasets...\n",
      "Creating data loaders with GPU optimizations...\n",
      "✓ Data loaders created successfully!\n",
      "  Train batches: 528 (batch size: 128)\n",
      "  Validation batches: 59\n",
      "  Test batches: 586\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(config.TRANSFORMER_MODEL)\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = PriceDataset(train_data, tokenizer, config.IMAGE_FOLDER, transform=train_transform, is_train=True)\n",
    "val_dataset = PriceDataset(val_data, tokenizer, config.IMAGE_FOLDER, transform=val_transform, is_train=True)\n",
    "test_dataset = PriceDataset(test_df, tokenizer, config.IMAGE_FOLDER, transform=val_transform, is_train=False)\n",
    "\n",
    "# Create data loaders with GPU optimizations\n",
    "print(\"Creating data loaders with GPU optimizations...\")\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY,  # GPU optimization\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY,  # GPU optimization\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config.BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    num_workers=config.NUM_WORKERS,\n",
    "    pin_memory=config.PIN_MEMORY,  # GPU optimization\n",
    "    persistent_workers=config.PERSISTENT_WORKERS if config.NUM_WORKERS > 0 else False\n",
    ")\n",
    "\n",
    "print(f\"✓ Data loaders created successfully!\")\n",
    "print(f\"  Train batches: {len(train_loader)} (batch size: {config.BATCH_SIZE})\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c6f342",
   "metadata": {},
   "source": [
    "## Step 9: Multimodal Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "81c7e88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing OPTIMIZED model (MobileNetV2)...\n",
      "✓ Model initialized!\n",
      "  Total parameters: 68,728,385\n",
      "  Trainable parameters: 141,633 (0.2%)\n",
      "  Model size: ~262.2 MB\n",
      "  Device: cuda:0\n",
      "✓ Model initialized!\n",
      "  Total parameters: 68,728,385\n",
      "  Trainable parameters: 141,633 (0.2%)\n",
      "  Model size: ~262.2 MB\n",
      "  Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "class MultimodalPricePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    OPTIMIZED Multimodal model for price prediction.\n",
    "    Uses MobileNetV2 (ultra-light), DistilBERT (frozen), and IPQ feature.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super(MultimodalPricePredictor, self).__init__()\n",
    "        \n",
    "        # Image Tower - MobileNetV2 (MUCH lighter and faster than ResNet)\n",
    "        self.image_model = models.mobilenet_v2(pretrained=True)\n",
    "        # Freeze ALL layers for speed\n",
    "        for param in self.image_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get features before classifier\n",
    "        num_features = self.image_model.classifier[1].in_features\n",
    "        self.image_model.classifier = nn.Identity()\n",
    "        \n",
    "        # Ultra-simple image projection\n",
    "        self.image_projection = nn.Sequential(\n",
    "            nn.Linear(num_features, config.IMAGE_EMBED_DIM),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Text Tower - DistilBERT (frozen for speed)\n",
    "        self.text_model = DistilBertModel.from_pretrained(config.TRANSFORMER_MODEL)\n",
    "        # Freeze ALL transformer weights\n",
    "        for param in self.text_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Ultra-simple text projection\n",
    "        self.text_projection = nn.Sequential(\n",
    "            nn.Linear(768, config.TEXT_EMBED_DIM),  # DistilBERT hidden size is 768\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Simple Fusion Head\n",
    "        fusion_dim = config.IMAGE_EMBED_DIM + config.TEXT_EMBED_DIM + 1  # +1 for IPQ\n",
    "        \n",
    "        self.fusion_head = nn.Sequential(\n",
    "            nn.Linear(fusion_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(config.DROPOUT),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, image, input_ids, attention_mask, ipq):\n",
    "        # Image features\n",
    "        image_features = self.image_model(image)\n",
    "        image_embed = self.image_projection(image_features)\n",
    "        \n",
    "        # Text features (with no_grad for extra speed)\n",
    "        with torch.no_grad():\n",
    "            text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Use [CLS] token representation\n",
    "        text_features = text_output.last_hidden_state[:, 0, :]\n",
    "        text_embed = self.text_projection(text_features)\n",
    "        \n",
    "        # Expand IPQ to match batch dimension\n",
    "        ipq = ipq.unsqueeze(1)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        fused = torch.cat([image_embed, text_embed, ipq], dim=1)\n",
    "        \n",
    "        # Final prediction\n",
    "        output = self.fusion_head(fused)\n",
    "        \n",
    "        # Ensure positive output using softplus\n",
    "        output = F.softplus(output)\n",
    "        \n",
    "        return output.squeeze(1)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "print(\"Initializing OPTIMIZED model (MobileNetV2)...\")\n",
    "model = MultimodalPricePredictor(config).to(config.DEVICE)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"✓ Model initialized!\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.1f}%)\")\n",
    "print(f\"  Model size: ~{total_params*4/1024/1024:.1f} MB\")\n",
    "print(f\"  Device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aab585",
   "metadata": {},
   "source": [
    "## Step 10: Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "df3d78c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function and optimizer initialized!\n"
     ]
    }
   ],
   "source": [
    "def smape(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate Symmetric Mean Absolute Percentage Error (SMAPE).\n",
    "    \"\"\"\n",
    "    y_true = y_true.cpu().detach().numpy() if torch.is_tensor(y_true) else y_true\n",
    "    y_pred = y_pred.cpu().detach().numpy() if torch.is_tensor(y_pred) else y_pred\n",
    "    \n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2.0\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    smape_val = np.mean(diff / (denominator + 1e-10)) * 100\n",
    "    \n",
    "    return smape_val\n",
    "\n",
    "\n",
    "# Loss function - Mean Absolute Error (good proxy for SMAPE)\n",
    "criterion = nn.L1Loss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), \n",
    "    lr=config.LEARNING_RATE, \n",
    "    weight_decay=config.WEIGHT_DECAY\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='min', \n",
    "    factor=0.5, \n",
    "    patience=2\n",
    ")\n",
    "\n",
    "print(\"Loss function and optimizer initialized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35cdb12",
   "metadata": {},
   "source": [
    "## Step 11: Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fae6ae2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training functions defined!\n"
     ]
    }
   ],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"\n",
    "    Train the model for one epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Training')\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        # Move data to device\n",
    "        image = batch['image'].to(device)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        ipq = batch['ipq'].to(device)\n",
    "        price = batch['price'].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(image, input_ids, attention_mask, ipq)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, price)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        running_loss += loss.item() * image.size(0)\n",
    "        all_preds.extend(outputs.cpu().detach().numpy())\n",
    "        all_targets.extend(price.cpu().detach().numpy())\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_smape = smape(np.array(all_targets), np.array(all_preds))\n",
    "    \n",
    "    return epoch_loss, epoch_smape\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"\n",
    "    Validate the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Validation')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # Move data to device\n",
    "            image = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            ipq = batch['ipq'].to(device)\n",
    "            price = batch['price'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(image, input_ids, attention_mask, ipq)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, price)\n",
    "            \n",
    "            # Track metrics\n",
    "            running_loss += loss.item() * image.size(0)\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "            all_targets.extend(price.cpu().numpy())\n",
    "            \n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_smape = smape(np.array(all_targets), np.array(all_preds))\n",
    "    \n",
    "    return epoch_loss, epoch_smape\n",
    "\n",
    "\n",
    "print(\"Training functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a4f7d",
   "metadata": {},
   "source": [
    "## Step 12: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ac42ed5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Mixed Precision (AMP) Enabled - Training will be faster on GPU\n"
     ]
    }
   ],
   "source": [
    "# Setup Mixed Precision Training (AMP) for GPU\n",
    "use_amp = torch.cuda.is_available()  # Enable AMP only if GPU is available\n",
    "scaler = torch.cuda.amp.GradScaler() if use_amp else None\n",
    "\n",
    "if use_amp:\n",
    "    print(\"✓ Mixed Precision (AMP) Enabled - Training will be faster on GPU\")\n",
    "else:\n",
    "    print(\"⚠️  Mixed Precision Disabled - Running on CPU or AMP not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "80211475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STARTING ULTRA-FAST TRAINING\n",
      "============================================================\n",
      "Configuration:\n",
      "  Device: cuda\n",
      "  Batch Size: 128\n",
      "  Epochs: 3\n",
      "  Learning Rate: 0.0003\n",
      "  Mixed Precision: Enabled\n",
      "  Training samples: 67500\n",
      "  Validation samples: 7500\n",
      "============================================================\n",
      "\n",
      "Epoch 1/3\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "249382d7d7ea4cf9b23cc7bb793bbf8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/528 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m train_loss, train_smape = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     36\u001b[39m val_loss, val_smape = validate_epoch(model, val_loader, criterion, config.DEVICE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m      8\u001b[39m all_targets = []\n\u001b[32m     10\u001b[39m progress_bar = tqdm(dataloader, desc=\u001b[33m'\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/tqdm/notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mPriceDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m     21\u001b[39m row = \u001b[38;5;28mself\u001b[39m.df.iloc[idx]\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Load and process image\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m image = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimage_link\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform:\n\u001b[32m     26\u001b[39m     image = \u001b[38;5;28mself\u001b[39m.transform(image)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 71\u001b[39m, in \u001b[36mPriceDataset._load_image\u001b[39m\u001b[34m(self, image_link)\u001b[39m\n\u001b[32m     68\u001b[39m image_path = os.path.join(\u001b[38;5;28mself\u001b[39m.image_folder, filename)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m     image = \u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m.convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Return default image if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/PIL/Image.py:3559\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(fp, mode, formats)\u001b[39m\n\u001b[32m   3556\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   3557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3559\u001b[39m im = \u001b[43m_open_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3561\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m im \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m formats \u001b[38;5;129;01mis\u001b[39;00m ID:\n\u001b[32m   3562\u001b[39m     checked_formats = ID.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/PIL/Image.py:3547\u001b[39m, in \u001b[36mopen.<locals>._open_core\u001b[39m\u001b[34m(fp, filename, prefix, formats)\u001b[39m\n\u001b[32m   3545\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m result:\n\u001b[32m   3546\u001b[39m     fp.seek(\u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m3547\u001b[39m     im = \u001b[43mfactory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3548\u001b[39m     _decompression_bomb_check(im.size)\n\u001b[32m   3549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m im\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/PIL/JpegImagePlugin.py:870\u001b[39m, in \u001b[36mjpeg_factory\u001b[39m\u001b[34m(fp, filename)\u001b[39m\n\u001b[32m    867\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mjpeg_factory\u001b[39m(\n\u001b[32m    868\u001b[39m     fp: IO[\u001b[38;5;28mbytes\u001b[39m], filename: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28mbytes\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    869\u001b[39m ) -> JpegImageFile | MpoImageFile:\n\u001b[32m--> \u001b[39m\u001b[32m870\u001b[39m     im = \u001b[43mJpegImageFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    872\u001b[39m         mpheader = im._getmp()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/PIL/ImageFile.py:128\u001b[39m, in \u001b[36mImageFile.__init__\u001b[39m\u001b[34m(self, fp, filename)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;28mself\u001b[39m.tile: \u001b[38;5;28mlist\u001b[39m[_Tile] = []\n\u001b[32m    126\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\" A list of tile descriptors \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreadonly\u001b[49m = \u001b[32m1\u001b[39m  \u001b[38;5;66;03m# until we know better\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[38;5;28mself\u001b[39m.decoderconfig: \u001b[38;5;28mtuple\u001b[39m[Any, ...] = ()\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.decodermaxblock = MAXBLOCK\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/PIL/Image.py:584\u001b[39m, in \u001b[36mImage.readonly\u001b[39m\u001b[34m(self, readonly)\u001b[39m\n\u001b[32m    580\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    581\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadonly\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    582\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._im \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._im.readonly) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._readonly\n\u001b[32m--> \u001b[39m\u001b[32m584\u001b[39m \u001b[38;5;129m@readonly\u001b[39m.setter\n\u001b[32m    585\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreadonly\u001b[39m(\u001b[38;5;28mself\u001b[39m, readonly: \u001b[38;5;28mint\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    586\u001b[39m     \u001b[38;5;28mself\u001b[39m._readonly = readonly\n\u001b[32m    588\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_new\u001b[39m(\u001b[38;5;28mself\u001b[39m, im: core.ImagingCore) -> Image:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "import time\n",
    "\n",
    "best_val_smape = float('inf')\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_smapes = []\n",
    "val_smapes = []\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STARTING ULTRA-FAST TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Device: {config.DEVICE}\")\n",
    "print(f\"  Batch Size: {config.BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {config.EPOCHS}\")\n",
    "print(f\"  Learning Rate: {config.LEARNING_RATE}\")\n",
    "print(f\"  Mixed Precision: {'Enabled' if use_amp else 'Disabled'}\")\n",
    "print(f\"  Training samples: {len(train_data)}\")\n",
    "print(f\"  Validation samples: {len(val_data)}\")\n",
    "print(\"=\"*60)\n",
    "print()\n",
    "\n",
    "total_start_time = time.time()\n",
    "\n",
    "for epoch in range(config.EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{config.EPOCHS}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_smape = train_epoch(model, train_loader, criterion, optimizer, config.DEVICE)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_smape = validate_epoch(model, val_loader, criterion, config.DEVICE)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_smapes.append(train_smape)\n",
    "    val_smapes.append(val_smape)\n",
    "    \n",
    "    epoch_time = time.time() - epoch_start_time\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"\\n📊 Results:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train SMAPE: {train_smape:.4f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val SMAPE: {val_smape:.4f}%\")\n",
    "    print(f\"  ⏱️  Epoch time: {epoch_time:.1f}s ({epoch_time/60:.1f} min)\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(val_smape)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_smape < best_val_smape:\n",
    "        best_val_smape = val_smape\n",
    "        torch.save(model.state_dict(), config.MODEL_SAVE_PATH)\n",
    "        print(f\"  ✓ Best model saved! (Val SMAPE: {val_smape:.4f}%)\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "total_time = time.time() - total_start_time\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Best validation SMAPE: {best_val_smape:.4f}%\")\n",
    "print(f\"✓ Total training time: {total_time:.1f}s ({total_time/60:.1f} min)\")\n",
    "print(f\"✓ Average time per epoch: {total_time/config.EPOCHS:.1f}s\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9df8f",
   "metadata": {},
   "source": [
    "## Step 13: Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d04242db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABdEAAAHqCAYAAADrpwd3AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAbfNJREFUeJzt3Xd4FOX6//HPphNCEmpCpPcqvQREUAKhiDQRkBbgiCiISjmAIv0cVEQRkKISEAFBEFEEgQBiAaRKk3JQKSKGHkINIZnfH/zYr2syIYHdbDK8X9e1l8wzz8zcM3ey3tzMztoMwzAEAAAAAAAAAABS8HB3AAAAAAAAAAAAZFU00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRATywoqKiVKxYsXvadvTo0bLZbM4NKIs5duyYbDab5s6dm+nHttlsGj16tH157ty5stlsOnbs2F23LVasmKKiopwaz/38rAAAADzIqLnTRs39f6i5AWRlNNEBZDk2my1dr40bN7o71AfegAEDZLPZ9Ouvv5rOee2112Sz2bR3795MjCzjTp06pdGjR2v37t3uDsXuzl+q3n77bXeHAgAALIaaO/ug5na9s2fP6qWXXlK5cuWUI0cOFShQQLVr19bQoUN15coV+7yoqCjZbDYFBgbq+vXrKfZz5MgR+++OWQ2/atUq2Ww2hYWFKTk5OdU5xYoVc/g9LFCggBo0aKAvvvjCYV6jRo1Mf3fLlSt3H1cEwD95uTsAAPinTz75xGF53rx5iomJSTFevnz5+zrOhx9+aFq03M2IESM0bNiw+zq+FXTp0kVTp07VwoULNXLkyFTnfPrpp6pcubIefvjhez5Ot27d1KlTJ/n6+t7zPu7m1KlTGjNmjIoVK6aqVas6rLufnxUAAICsiJo7+6Dmdq0LFy6oZs2aio+PV69evVSuXDmdP39ee/fu1YwZM/T8888rICDAPt/Ly0vXrl3TihUr9PTTTzvsa8GCBfLz89ONGzdMj7dgwQIVK1ZMx44d04YNGxQREZHqvKpVq2rQoEGSbl+3WbNmqV27dpoxY4b69u1rn1eoUCFNmDAhxfZBQUEZug4A0kYTHUCW07VrV4fln376STExMSnG/+natWvy9/dP93G8vb3vKT7pduHk5cVbaJ06dVSqVCl9+umnqRb0W7Zs0dGjR/XGG2/c13E8PT3l6el5X/u4H/fzswIAAJAVUXNnH9TcrjV79mydOHFCmzZtUr169RzWxcfHy8fHx2HM19dX9evX16effpqiib5w4UK1bNlSn3/+earHunr1qr788ktNmDBBc+bM0YIFC0yb6A899JDD72P37t1VqlQpvfvuuw5N9KCgoLv+3gK4fzzOBUC21KhRI1WqVEk7d+7Uo48+Kn9/f7366quSpC+//FItW7ZUWFiYfH19VbJkSY0bN05JSUkO+/jnM/f+/uiMDz74QCVLlpSvr69q1aql7du3O2yb2vMZbTab+vfvr+XLl6tSpUry9fVVxYoVtXr16hTxb9y4UTVr1pSfn59KliypWbNmpfuZjz/88IM6dOigIkWKyNfXV4ULF9Yrr7yS4uOEUVFRCggI0J9//qk2bdooICBA+fPn1+DBg1Nci7i4OEVFRSkoKEjBwcHq0aOH4uLi7hqLdPvOmEOHDmnXrl0p1i1cuFA2m02dO3fWzZs3NXLkSNWoUUNBQUHKmTOnGjRooG+//faux0jt+YyGYWj8+PEqVKiQ/P399dhjj+mXX35Jse2FCxc0ePBgVa5cWQEBAQoMDFTz5s21Z88e+5yNGzeqVq1akqSePXvaPwJ559mUqT2f8erVqxo0aJAKFy4sX19flS1bVm+//bYMw3CYl5Gfi3t15swZ9e7dWyEhIfLz81OVKlX08ccfp5i3aNEi1ahRQ7ly5VJgYKAqV66s9957z74+MTFRY8aMUenSpeXn56e8efPqkUceUUxMjNNiBQAA2Qc1NzX3g1Bz//bbb/L09FTdunVTrAsMDJSfn1+K8WeeeUbffPONQ/62b9+uI0eO6JlnnjE91hdffKHr16+rQ4cO6tSpk5YtW5bmXet/FxoaqvLly+vo0aPpmg/AufgnXQDZ1vnz59W8eXN16tRJXbt2VUhIiKTbxV9AQIAGDhyogIAAbdiwQSNHjlR8fLwmTpx41/0uXLhQly9f1nPPPSebzaa33npL7dq10++//37XuyN+/PFHLVu2TC+88IJy5cqlKVOmqH379jpx4oTy5s0rSfr555/VrFkzFSxYUGPGjFFSUpLGjh2r/Pnzp+u8lyxZomvXrun5559X3rx5tW3bNk2dOlUnT57UkiVLHOYmJSUpMjJSderU0dtvv61169Zp0qRJKlmypJ5//nlJtwvj1q1b68cff1Tfvn1Vvnx5ffHFF+rRo0e64unSpYvGjBmjhQsXqnr16g7H/uyzz9SgQQMVKVJE586d00cffaTOnTvr2Wef1eXLlzV79mxFRkZq27ZtKT7OeTcjR47U+PHj1aJFC7Vo0UK7du1S06ZNdfPmTYd5v//+u5YvX64OHTqoePHiOn36tGbNmqWGDRvqwIEDCgsLU/ny5TV27FiNHDlSffr0UYMGDSQpxZ0odxiGoSeffFLffvutevfurapVq2rNmjUaMmSI/vzzT7377rsO89Pzc3Gvrl+/rkaNGunXX39V//79Vbx4cS1ZskRRUVGKi4vTSy+9JEmKiYlR586d1bhxY7355puSpIMHD2rTpk32OaNHj9aECRP0r3/9S7Vr11Z8fLx27NihXbt2qUmTJvcVJwAAyJ6ouam5rV5zFy1aVElJSfrkk0/SnY927dqpb9++WrZsmXr16iXp9s90uXLlHPLzTwsWLNBjjz2m0NBQderUScOGDdOKFSvUoUOHux4zMTFRf/zxR4pzSUpK0rlz51LMz5Ejh3LmzJmu8wGQDgYAZHH9+vUz/vl21bBhQ0OSMXPmzBTzr127lmLsueeeM/z9/Y0bN27Yx3r06GEULVrUvnz06FFDkpE3b17jwoUL9vEvv/zSkGSsWLHCPjZq1KgUMUkyfHx8jF9//dU+tmfPHkOSMXXqVPtYq1atDH9/f+PPP/+0jx05csTw8vJKsc/UpHZ+EyZMMGw2m3H8+HGH85NkjB071mFutWrVjBo1atiXly9fbkgy3nrrLfvYrVu3jAYNGhiSjDlz5tw1plq1ahmFChUykpKS7GOrV682JBmzZs2y7zMhIcFhu4sXLxohISFGr169HMYlGaNGjbIvz5kzx5BkHD161DAMwzhz5ozh4+NjtGzZ0khOTrbPe/XVVw1JRo8ePexjN27ccIjLMG7n2tfX1+HabN++3fR8//mzcueajR8/3mHeU089ZdhsNoefgfT+XKTmzs/kxIkTTedMnjzZkGTMnz/fPnbz5k0jPDzcCAgIMOLj4w3DMIyXXnrJCAwMNG7dumW6rypVqhgtW7ZMMyYAAGBN1Nx3Pz9q7tusVnPHxsYa+fPnNyQZ5cqVM/r27WssXLjQiIuLSzXGnDlz2uNo3LixYRiGkZSUZISGhhpjxowxreFPnz5teHl5GR9++KF9rF69ekbr1q1THKdo0aJG06ZNjbNnzxpnz5419uzZY3Tq1MmQZLz44ov2eXd+R1N7Pffcc2meN4CM4XEuALItX19f9ezZM8V4jhw57H++fPmyzp07pwYNGujatWs6dOjQXffbsWNH5c6d27585w6J33///a7bRkREqGTJkvblhx9+WIGBgfZtk5KStG7dOrVp00ZhYWH2eaVKlVLz5s3vun/J8fyuXr2qc+fOqV69ejIMQz///HOK+X9/Xt6d8/n7uaxatUpeXl72u2Sk289DfPHFF9MVj3T7mZonT57U999/bx9buHChfHx87HdVeHp62p8nmJycrAsXLujWrVuqWbNmqh9LTcu6det08+ZNvfjiiw4fx3355ZdTzPX19ZWHx+3/3SUlJen8+fMKCAhQ2bJlM3zcO1atWiVPT08NGDDAYXzQoEEyDEPffPONw/jdfi7ux6pVqxQaGqrOnTvbx7y9vTVgwABduXJF3333nSQpODhYV69eTfPRLMHBwfrll1905MiR+44LAABYAzU3NbfVa+6QkBDt2bNHffv21cWLFzVz5kw988wzKlCggMaNG5fi0TF3PPPMM9q4caNiY2O1YcMGxcbGpvkol0WLFsnDw0Pt27e3j3Xu3FnffPONLl68mGL+2rVrlT9/fuXPn19VqlTRkiVL1K1bN/unSu8oVqyYYmJiUrxSyxOAe0cTHUC29dBDD6X4khdJ+uWXX9S2bVsFBQUpMDBQ+fPnt3/RyqVLl+663yJFijgs3ynuUyts7rbtne3vbHvmzBldv35dpUqVSjEvtbHUnDhxQlFRUcqTJ4/9mYsNGzaUlPL8/Pz8Unxk9e/xSNLx48dVsGBBh2+cl6SyZcumKx5J6tSpkzw9PbVw4UJJ0o0bN/TFF1+oefPmDn85+vjjj/Xwww/bn7edP39+rVy5Ml15+bvjx49LkkqXLu0wnj9/fofjSbf/8vDuu++qdOnS8vX1Vb58+ZQ/f37t3bs3w8f9+/HDwsKUK1cuh/Hy5cs7xHfH3X4u7sfx48dVunRp+19azGJ54YUXVKZMGTVv3lyFChVSr169UjwjcuzYsYqLi1OZMmVUuXJlDRkyRHv37r3vGAEAQPZFzU3N/SDU3AULFtSMGTP0119/6fDhw5oyZYry58+vkSNHavbs2alu06JFC+XKlUuLFy/WggULVKtWrTR/vubPn6/atWvr/Pnz+vXXX/Xrr7+qWrVqunnzZopHBEm3v1A2JiZG69at0+bNm3Xu3DnNmzfP4R94JClnzpyKiIhI8SpXrtxdzxtA+tFEB5Bt/bN4kG5/WU/Dhg21Z88ejR07VitWrFBMTIz9X+uTk5Pvul+zb6Q3uwPBWdumR1JSkpo0aaKVK1dq6NChWr58uWJiYuxfxvPP8zOLx9kKFCigJk2a6PPPP1diYqJWrFihy5cvq0uXLvY58+fPV1RUlEqWLKnZs2dr9erViomJ0eOPP56uvNyr//73vxo4cKAeffRRzZ8/X2vWrFFMTIwqVqzo0uP+nat/LtKjQIEC2r17t7766iv7syWbN2/u8NzHRx99VL/99puio6NVqVIlffTRR6pevbo++uijTIsTAABkLdTc1NzpYZWa22azqUyZMnrxxRf1/fffy8PDQwsWLEh1rq+vr9q1a6ePP/5YX3zxRZp3oR85ckTbt2/Xjz/+qNKlS9tfjzzyiCSleox8+fIpIiJCjRs3Vnh4uIKDg9N9HgCcjy8WBWApGzdu1Pnz57Vs2TI9+uij9vGs8g3mBQoUkJ+fn3799dcU61Ib+6d9+/bpf//7nz7++GN1797dPp7WIzrupmjRolq/fr2uXLnicGfM4cOHM7SfLl26aPXq1frmm2+0cOFCBQYGqlWrVvb1S5cuVYkSJbRs2TKHj4OOGjXqnmKWbhejJUqUsI+fPXs2xZ0mS5cu1WOPPZbiDpK4uDjly5fPvvz3mNJz/HXr1uny5csOd8bc+ejynfgyQ9GiRbV3714lJyc73I2eWiw+Pj5q1aqVWrVqpeTkZL3wwguaNWuWXn/9dftdM3ny5FHPnj3Vs2dPXblyRY8++qhGjx6tf/3rX5l2TgAAIGuj5s44au7bslPNXaJECeXOnVt//fWX6ZxnnnlG0dHR8vDwUKdOnUznLViwQN7e3vrkk09SNPt//PFHTZkyRSdOnEj1bnoAWQN3ogOwlDsFyd/vNrh586amT5/urpAceHp6KiIiQsuXL9epU6fs47/++muKZ/qZbS85np9hGHrvvffuOaYWLVro1q1bmjFjhn0sKSlJU6dOzdB+2rRpI39/f02fPl3ffPON2rVrJz8/vzRj37p1q7Zs2ZLhmCMiIuTt7a2pU6c67G/y5Mkp5np6eqa4+2TJkiX6888/HcbufHN9XFzcXY/fokULJSUladq0aQ7j7777rmw2W7qftekMLVq0UGxsrBYvXmwfu3XrlqZOnaqAgAD7x47Pnz/vsJ2Hh4cefvhhSVJCQkKqcwICAlSqVCn7egAAAIma+15Qc9+WFWvurVu36urVqynGt23bpvPnz6f5yJ3HHntM48aN07Rp0xQaGmo6b8GCBWrQoIE6duyop556yuE1ZMgQSdKnn356/ycDwGW4Ex2ApdSrV0+5c+dWjx49NGDAANlsNn3yySeZ+tiMuxk9erTWrl2r+vXr6/nnn7cXhpUqVdLu3bvT3LZcuXIqWbKkBg8erD///FOBgYH6/PPP7+vZ2q1atVL9+vU1bNgwHTt2TBUqVNCyZcsy/OzCgIAAtWnTxv6Mxr9/rFSSnnjiCS1btkxt27ZVy5YtdfToUc2cOVMVKlTQlStXMnSs/Pnza/DgwZowYYKeeOIJtWjRQj///LO++eYbhztd7hx37Nix6tmzp+rVq6d9+/ZpwYIFDnfTSFLJkiUVHBysmTNnKleuXMqZM6fq1Kmj4sWLpzh+q1at9Nhjj+m1117TsWPHVKVKFa1du1ZffvmlXn75ZYcvNHKG9evX68aNGynG27Rpoz59+mjWrFmKiorSzp07VaxYMS1dulSbNm3S5MmT7Xft/Otf/9KFCxf0+OOPq1ChQjp+/LimTp2qqlWr2p8rWaFCBTVq1Eg1atRQnjx5tGPHDi1dulT9+/d36vkAAIDsjZo746i5b8uKNfcnn3yiBQsWqG3btqpRo4Z8fHx08OBBRUdHy8/PT6+++qrpth4eHhoxYkSa+9+6dat+/fVX05r6oYceUvXq1bVgwQINHTo0w/FfunRJ8+fPT3Xdne8pAHD/aKIDsJS8efPq66+/1qBBgzRixAjlzp1bXbt2VePGjRUZGenu8CRJNWrU0DfffKPBgwfr9ddfV+HChTV27FgdPHjQ/tFEM97e3lqxYoUGDBigCRMmyM/PT23btlX//v1VpUqVe4rHw8NDX331lV5++WXNnz9fNptNTz75pCZNmqRq1aplaF9dunTRwoULVbBgQT3++OMO66KiohQbG6tZs2ZpzZo1qlChgubPn68lS5Zo48aNGY57/Pjx8vPz08yZM/Xtt9+qTp06Wrt2rVq2bOkw79VXX9XVq1e1cOFCLV68WNWrV9fKlSs1bNgwh3ne3t76+OOPNXz4cPXt21e3bt3SnDlzUi3o71yzkSNHavHixZozZ46KFSumiRMnatCgQRk+l7tZvXp1ii8BlaRixYqpUqVK2rhxo4YNG6aPP/5Y8fHxKlu2rObMmaOoqCj73K5du+qDDz7Q9OnTFRcXp9DQUHXs2FGjR4+2PwZmwIAB+uqrr7R27VolJCSoaNGiGj9+vP3uGAAAAIma+15Qc9+WFWvu5557Tv7+/lq/fr2+/PJLxcfHK3/+/GratKmGDx+e4fz8053nnf/9sTv/1KpVK40ePVp79+61f1o0vU6ePKlu3bqluo4mOuA8NiMr/VMxADzA2rRpo19++UVHjhxxdygAAACAJVFzAwDuBc9EBwA3uH79usPykSNHtGrVKjVq1Mg9AQEAAAAWQ80NAHAW7kQHADcoWLCgoqKiVKJECR0/flwzZsxQQkKCfv75Z5UuXdrd4QEAAADZHjU3AMBZeCY6ALhBs2bN9Omnnyo2Nla+vr4KDw/Xf//7X4p5AAAAwEmouQEAzsKd6AAAAAAAAAAAmOCZ6AAAAAAAAAAAmKCJDgAAAAAAAACACZ6J7gTJyck6deqUcuXKJZvN5u5wAAAAkMUZhqHLly8rLCxMHh7c1+Is1OUAAADIiPTW5TTRneDUqVMqXLiwu8MAAABANvPHH3+oUKFC7g7DMqjLAQAAcC/uVpfTRHeCXLlySbp9sQMDA90cjTUlJiZq7dq1atq0qby9vd0dDpyI3Fob+bUucmtd5DZzxMfHq3DhwvY6Es5BXe56vEdYG/m1LnJrXeTW2siv66W3LqeJ7gR3PioaGBhIse4iiYmJ8vf3V2BgIG8aFkNurY38Whe5tS5ym7l45IhzUZe7Hu8R1kZ+rYvcWhe5tTbym3nuVpfzAEYAAAAAAAAAAEzQRAcAAAAAAAAAwARNdAAAAAAAAAAATPBMdAAAgCwiKSlJiYmJbo0hMTFRXl5eunHjhpKSktwaS3bm7e0tT09Pd4cBAADwQMoKdbUzUJvfP2fV5TTRAQAA3MwwDMXGxiouLs7docgwDIWGhuqPP/7gSy/vU3BwsEJDQ7mOAAAAmSQr1dXOQG3uHM6oy2miAwAAuNmdQr9AgQLy9/d3a4GcnJysK1euKCAgQB4ePPnvXhiGoWvXrunMmTOSpIIFC7o5IgAAgAdDVqqrnYHa/P44sy6niQ4AAOBGSUlJ9kI/b9687g5HycnJunnzpvz8/CjU70OOHDkkSWfOnFGBAgV4tAsAAICLZbW62hmoze+fs+pyrj4AAIAb3XlWo7+/v5sjgbPdyakVnscJAACQ1VFXw4wz6nKa6AAAAFlAdv+oKVIipwAAAJmPGgz/5IyfCZroAAAAAAAAAACYoIkOAACALKNYsWKaPHmyu8MAAAAAsi1qauejiQ4AAGARScmGtvx2Xl/u/lNbfjuvpGTDZcey2WxpvkaPHn1P+92+fbv69OlzX7E1atRIL7/88n3tAwAAAA+mrFRTjxkz5p7264ya+ujRo3rmmWcUFhYmPz8/FSpUSK1bt9ahQ4dSxP/TTz85bJuQkKC8efPKZrNp48aNKfb93HPPydPTU0uWLEmxbvTo0fb9enl5qVixYnrllVd05coVSdKxY8dMr9c/43AmL5ftGQAAAJlm9f6/NGbFAf116YZ9rGCQn0a1qqBmlQo6/Xh//fWX/c+LFy/WyJEjdfjwYftYQECA/c+GYSgpKUleXncvPfPnz+/cQAEAAIB0ymo1tb+/v5KTkyVlbk2dmJioJk2aqGzZslq2bJkKFiyokydP6ptvvlFcXJzD3MKFC2vOnDmqW7eufeyLL75QQECALly4kGLf165d06JFi/Tvf/9b0dHR6tChQ4o5FStW1Lp163Tr1i1t2rRJvXr10rVr1zRr1iz7nHXr1qlixYoO2+XNm/e+zjst3IkOAACQza3e/5een7/LodiXpNhLN/T8/F1avf8vky3vXWhoqP0VFBQkm81mXz506JBy5cqlb775RjVq1JCvr69+/PFH/fbbb2rdurVCQkIUEBCgWrVqad26dQ77/edHT202mz766CO1bdtW/v7+Kl26tL766qv7iv3zzz9XxYoV5evrq2LFimnSpEkO66dPn67SpUvLz89PISEheuqpp+zrli5dqsqVKytHjhzKmzevIiIidPXq1fuKBwAAAO6XFWvqoKAgxcTEqFatWplaU//yyy/67bffNH36dNWtW1dFixZV/fr1NX78eIdmuST16NFDixYt0vXr1+1j0dHR6tGjR6r7XrJkiSpUqKBhw4bp+++/1x9//JFijpeXl0JDQ1WoUCF17NhRXbp0SRFv3rx5Ha5faGiovL29Tc/pftFEBwAAyGIMw9C1m7fS9bp8I1GjvvpFqX3I9M7Y6K8O6PKNxHTtzzCc93HVYcOG6Y033tDBgwf18MMP68qVK2rRooXWr1+vn3/+Wc2aNVOrVq104sSJNPczZswYPf3009q7d69atGihLl26pHpXS3rs3LlTTz/9tDp16qR9+/Zp9OjRev311zV37lxJ0o4dOzRgwACNHTtWhw8f1urVq/Xoo49Kun2nUOfOndWrVy8dPHhQGzduVLt27Zx6zQAAAOAcVqmpx4wZo//+97+ZWlPnz59fHh4eWrp0qZKSktLcb40aNVSsWDF9/vnnkqQTJ07o+++/V7du3VKdP3v2bHXt2lVBQUFq3ry5vQ5PS44cOXTz5s27znMlHucCAACQxVxPTFKFkWucsi9DUmz8DVUevTZd8/ePbuKU40rS2LFj1aTJ/+0vT548qlKlin153Lhx+uKLL/TVV1+pf//+pvuJiopS586dJUn//e9/NWXKFG3btk3NmjXLcEzvvPOOGjdurNdff12SVKZMGR04cEATJ05UVFSUTpw4oZw5c+qJJ55Qrly5VLRoUVWrVk3S7Sb6rVu31K5dOxUtWlSSVLly5QzHAAAAANdzZ019YGyk/H2c03Z99dVX1aRJE3l43L4XOjNq6oceekhTpkzRv//9b40ZM0Y1a9bUY489pi5duqhEiRIp5vfq1UvR0dHq2rWr5s6dqxYtWqT6SJkjR47op59+0rJlyyRJXbt21cCBAzVixAjZbLZU4965c6cWLlyoxx9/3GG8Xr169mtyx53nprsCd6IDAADAJWrWrOmwfOXKFQ0ePFjly5dXcHCwAgICdPDgwbveNfPwww/b/5wzZ04FBgbqzJkz9xTTwYMHVb9+fYex+vXr68iRI0pKSlKTJk1UtGhRlShRQt26ddOCBQt07do1SVKVKlXUuHFjVa5cWR06dNCHH36oixcv3lMcAAAAQHpUrVrVYTmzaup+/fopNjZWCxYsUHh4uJYsWaKKFSsqJiYmxdyuXbtqy5Yt+v333zV37lz16tUr1X1GR0crMjJS+fLlkyS1aNFCly5d0oYNGxzm7du3TwEBAcqRI4dq166t8PBwTZs2zWHO4sWLtXv3boeXK3EnOgAAQBaTw9tTB8ZGpmvutqMXFDVn+13nze1ZS7WL57nrPF9Pmy7fuOu0dMmZM6fD8uDBgxUTE6O3335bpUqVUo4cOfTUU0/d9aOZ/3y2oc1ms3/BkrPlypVLu3bt0saNG7V27VqNHDlSo0eP1vbt2xUcHKyYmBht3rxZa9eu1dSpU/Xaa69p69atKl68uEviAQAAwL1xZ02dw9szXcdND3fW1Lly5VKrVq3UqlUrjR8/XpGRkRo/frzDp02l288nf+KJJ9S7d2/duHFDzZs31+XLlx3mJCUl6eOPP1ZsbKzDl6MmJSUpOjpajRs3to+VLVtWX331lby8vBQWFiYfH58UsRUuXFilSpVKM35nookOAACQxdhstnR//LNB6fwqGOSn2Es3Un2Go01SaJCfGpTOL0+P1D8i+Xeuak5L0qZNmxQVFaW2bdtKun0XzbFjx1x2vNSUL19emzZtShFXmTJl5Ol5+y87Xl5eioiIUEREhEaNGqXg4GBt2LBB7dq1k81mU/369VW/fn2NHDlSRYsW1RdffKGBAwdm6nkAAAAgbe6sqV3JXTW1zWZTuXLltHnz5lTX9+rVSy1atNDQoUPtdfXfrVq1SpcvX9bPP//ssH7//v3q2bOn4uLiFBwcLEny8fHJ1AZ5etBEBwAAyMY8PWwa1aqCnp+/SzbJoei/U96PalXB7cW+JJUuXVrLli1Tq1atZLPZ9Prrr7usaX/27NkUH+ksWLCgBg0apFq1amncuHHq2LGjtmzZomnTpmn69OmSpK+//lq///67Hn30UeXOnVurVq1ScnKyypYtq61bt2r9+vVq2rSpChQooK1bt+rs2bMqX768S84BAAAAmYOa2tHu3bs1atQodevWTRUqVJCPj4++++47RUdHa+jQoalu06xZM509e1aBgYGprp89e7Zatmzp8Dx3SapQoYJeeeUVLViwQP369Ut3jOfPn1dsbKzDWHBwsPz8/NK9j4zgmegAAADZXLNKBTWja3WFBjkWjKFBfprRtbqaVSropsgcvfPOO8qdO7fq1aunVq1aKTIyUtWrV3fJsRYuXKhq1ao5vD788ENVr15dn332mRYtWqRKlSpp5MiRGjt2rKKioiTdLryXLVumxx9/XOXLl9fMmTP16aefqmLFigoMDNT333+vFi1aqEyZMhoxYoQmTZqk5s2bu+QcAAAAkHmoqf9PoUKFVKxYMY0ZM0Z16tRR9erV9d5772nMmDF67bXXUt3GZrMpX758qT565fTp01q5cqXat2+fYp2Hh4fatm2r2bNnZyjGiIgIFSxY0OG1fPnyDO0jI2yGYaT2KQVkQHx8vIKCgnTp0iXTf23B/UlMTNSqVavUokWLFM9wQvZGbq2N/FoXuXWeGzdu6OjRoypevPh93zWRlGxo29ELOnP5hgrk8lPt4nkyfLdMcnKy4uPjFRgYmOLb7pExaeWW+tE1uK6ux/u/tZFf6yK31kVu/4+z6mpn1NTOQm3uHM6oy3mcCwAAgEV4etgUXjKvu8MAAAAAsi1qaqSGf8IAAAAAAAAAAMAETXQAAAAAAAAAAEzQRAcAAAAAAAAAwARNdAAAAAAAAAAATNBEBwAAAAAAAADABE10AAAAAAAAAABM0EQHAAAAAAAAAMAETXQAAAAAAAAAAEzQRAcAAIDbNGrUSC+//LK7wwAAAACyLWpq16OJDgAAkN3F/SGd2m3+ivvD6Yds1aqVmjVrluq6H374QTabTXv37r3v48ydO1fBwcH3vR8AAAAgTVmwpvb09NT+/fvv+zhJSUl64403VK5cOeXIkUN58uRRnTp19NFHH9nnREVFyWazqW/fvim279evn2w2m6KiolKs27Jlizw9PdWyZcsU644dOyabzWZ/5c2bV02bNtXPP/9sn9OoUSOHOXdeqcXhTl7uDgAAAAD3Ie4PaVoN6VaC+RwvX6n/Tim4sNMO27t3b7Vv314nT55UoUKFHNbNmTNHNWvW1MMPP+y04wEAAAAuk4Vr6kqVKt33ccaMGaNZs2Zp2rRpqlmzpuLj47Vjxw5dvHjRYV7hwoW1aNEivfvuu8qRI4ck6caNG1q4cKGKFCmS6r5nz56tF198UbNnz9apU6cUFhaWYs66detUsWJFnTx5UgMGDFDz5s116NAh+80yzz77rMaOHeuwjb+//32ftzNxJzoAAEB2du182sW+dHv9tfNOPewTTzyh/Pnza+7cuQ7jV65c0ZIlS9S7d2+dP39enTt31kMPPSR/f39VrlxZn376qVPjOHHihFq3bq2AgAAFBgbq6aef1unTp+3r9+zZo8cee0y5cuVSYGCgatSooR07dkiSjh8/rlatWil37tzKmTOnKlasqFWrVjk1PgAAAGQDWbSm7tmzpy5cuKBnnnnmvmrqr776Si+88II6dOig4sWLq0qVKurdu7cGDx7sMK969eoqXLiwli1bZh9btmyZihQpomrVqqXY75UrV7R48WI9//zzatmyZYrzuCNv3rwKDQ1VzZo19fbbb+v06dPaunWrfb2/v79CQ0MdXoGBgRk6R1ejiQ4AAJDVGIZ082r6Xreup2+ft66nb3+Gka7deXl5qXv37po7d66Mv22zZMkSJSUlqXPnzrpx44Zq1KihlStXav/+/erTp4+6deumbdu23ctVSSE5OVmtW7fWhQsX9N133ykmJka///67OnbsaJ/TpUsXFSpUSNu3b9fOnTs1bNgweXt7S7r9sdSEhAR9//332rdvn958800FBAQ4JTYAAAC4GTW1XWhoqDZs2KCzZ8/edW6vXr00Z84c+3J0dLR69uyZ6tzPPvtM5cqVU9myZdW1a1dFR0c7nEdq7tzhfvPmzXTHnxXwOBcAAICsJvGa9N+UH4O8L9GpP2sxhWEn073LXr16aeLEifruu+/UqFEjSbc/dtq+fXsFBQUpKCjI4e6WF198UWvWrNFnn32m2rVrZyT6VK1fv1779u3T0aNHVbjw7Y/Vzps3TxUrVtT27dtVq1YtnThxQkOGDFG5cuUkSaVLl7Zvf+LECbVv316VK1eWJJUoUeK+YwIAAEAW4c6a+tVTkk/OdE29W01ts9k0aNAgeXjcvhf6Xmrqd955R0899ZRCQ0NVsWJF1atXT61bt1bz5s1TzO3atauGDx+u48ePS5I2bdqkRYsWaePGjSnmzp49W127dpUkNWvWTJcuXXI4j3+Ki4vTuHHjFBAQ4BD79OnTHZ7PLkmzZs1Sly5d0nV+mYE70QEAAHBPypUrp3r16ik6OlqS9Ouvv+qHH35Q7969Jd3+AqNx48apcuXKypMnjwICArRmzRqdOHHCKcc/ePCgChcubG+gS1KFChUUHBysgwcPSpIGDhyof/3rX4qIiNAbb7yh3377zT53wIABGj9+vOrXr69Ro0Y55YtQAQAAgIxIT009fvz4+6qpK1SooP379+unn35Sr169dObMGbVq1Ur/+te/UszNnz+//dEsc+bMUcuWLZUvX74U8w4fPqxt27apc+fOkm7fVd+xY0fNnj07xdx69eopICBAuXPn1p49e7R48WKFhITY13fp0kW7d+92eD355JPpPr/MwJ3oAAAAWY23/+27V9Ijdm/67ojptVoKTccXfXr6STcup+/Yuv1lSC+++KLef/99zZkzRyVLllTDhg0lSRMnTtR7772nyZMnq3LlysqZM6defvnlTP3o5ujRo/XMM89o5cqV+uabbzRq1CgtWrRIbdu21b/+9S9FRkZq5cqVWrt2rSZMmKBJkybpxRdfzLT4AAAA4CLurKm9M/almGY1tWEYmjJlit5///37rqk9PDxUq1Yt1apVSy+//LLmz5+vbt266bXXXlPx4sUd5vbq1Uv9+/eXJL3//vup7m/27Nm6deuWwxeJGoYhX19fTZs2TUFBQfbxxYsXq0KFCsqbN6/9y0T/LigoSKVKlcrQ+WQ27kQHAADIamy22x//TM/LK0f69umVI337s9kyFOrTTz8tDw8PLVy4UPPmzVOvXr1k+//72LRpk1q3bq2uXbuqSpUqKlGihP73v/9l9GqYKl++vP744w/98ccf9rEDBw4oLi5OFSpUsI+VKVNGr7zyitauXat27do5POOxcOHC6tu3r5YtW6ZBgwbpww8/dFp8AAAAcCOL1NRbt27Vk08+6fSa+k69fPXq1RTrmjVrpps3byoxMVGRkZEp1t+6dUvz5s3TpEmTHO4e37Nnj8LCwlJ88WnhwoVVsmTJVBvo2QV3ogMAAOCeBQQEqGPHjho+fLji4+MVFRVlX1e6dGktXbpUmzdvVu7cufXOO+/o9OnTDg3u9EhKStLu3bsdxnx9fRUREaHKlSurS5cumjx5sm7duqUXXnhBDRs2VM2aNXX9+nUNGTJETz31lIoXL66TJ09q+/btat++vSTp5ZdfVvPmzVWmTBldvHhR3377rcqXL3+/lwQAAADIkLRq6pIlS2rFihX3VVM/9dRTql+/vurVq6fQ0FAdPXpUw4cPV5kyZezfHfR3np6e9scjenp6plj/9ddf6+LFi+rdu7fDHeeS1L59e82ePVt9+/ZNd3zXrl1TbGysw5ivr69y586d7n24GneiAwAAZGf+eSUv37TnePnenucivXv31sWLFxUZGenwcc4RI0aoevXqioyMVKNGjRQaGqo2bdpkeP9XrlxRtWrVHF6tWrWSzWbTl19+qdy5c+vRRx9VRESESpQoocWLF0u6XfCfP39e3bt3V5kyZfT000+refPmGjNmjKTbzfl+/fqpfPnyatasmcqUKaPp06c75ZoAAAAgG8nCNfXgwYNVrVq1+6qpIyMjtWLFCrVq1UplypRRjx49VK5cOa1du1ZeXqnfYx0YGKjAwMBU182ePVsREREpGujS7Sb6jh07MvR9Qx9++KEKFizo8LrzrPWsgjvRAQAAsrPgwlL/ndK18+Zz/PPenuci4eHhMgwjxXiePHm0fPnyNLfduHFjmuujoqIc7sT5pyJFiujLL79MdZ2Pj0+Kj5L+3dSpU9M8NgAAAB4QWbimzp07t7744gt5eJjfC323mvrZZ5/Vs88+m+acuXPnprn+73X9ihUrTOfVrl3b4TxSO6e/u1vsWQVNdAAAgOwuuLBLC3oAAADA8qipkQYe5wIAAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYyHZN9Pfff1/FihWTn5+f6tSpo23btqU5f8mSJSpXrpz8/PxUuXJlrVq1ynRu3759ZbPZNHnyZCdHDQAAkLbk5GR3hwAnexBySm0OAACymgehBkPGOONnwssJcWSaxYsXa+DAgZo5c6bq1KmjyZMnKzIyUocPH1aBAgVSzN+8ebM6d+6sCRMm6IknntDChQvVpk0b7dq1S5UqVXKY+8UXX+inn35SWFhYZp0OAACAfHx85OHhoVOnTil//vzy8fGRzWZzWzzJycm6efOmbty4IQ+PbHe/RZZgGIZu3ryps2fPysPDQz4+Pu4OySWozQEAQFaS1epqZ6A2vz/OrMuzVRP9nXfe0bPPPquePXtKkmbOnKmVK1cqOjpaw4YNSzH/vffeU7NmzTRkyBBJ0rhx4xQTE6Np06Zp5syZ9nl//vmnXnzxRa1Zs0YtW7bMnJMBAACQ5OHhoeLFi+uvv/7SqVOn3B2ODMPQ9evXlSNHjmz/lw538/f3V5EiRSz7Fx5qcwAAkJVktbraGajNncMZdXm2aaLfvHlTO3fu1PDhw+1jHh4eioiI0JYtW1LdZsuWLRo4cKDDWGRkpJYvX25fTk5OVrdu3TRkyBBVrFgxXbEkJCQoISHBvhwfHy9JSkxMVGJiYnpPCRlw57pyfa2H3Fob+bUucutcNptNBQsWVFJSkpKSkmQYhttiuXXrljZv3qx69erJyyvblIpZis1mk6enpzw9PWWz2VL9PcnuvztZpTanLs98vP9bG/m1LnJrXeTWUVaqq52B2vz+OLMuzzZX/9y5c0pKSlJISIjDeEhIiA4dOpTqNrGxsanOj42NtS+/+eab8vLy0oABA9Idy4QJEzRmzJgU42vXrpW/v3+694OMi4mJcXcIcBFya23k17rIrXV9//337g7B0q5du+buEO5LVqnNqcvdh/d/ayO/1kVurYvcWhu1ueukty7PNk10V9i5c6fee+897dq1K0MfiRg+fLjDXTTx8fEqXLiwmjZtqsDAQFeE+sBLTExUTEyMmjRpIm9vb3eHAycit9ZGfq2L3FoXuc0cd+6Yxv+5l9qcujzz8R5hbeTXusitdZFbayO/rpfeujzbNNHz5csnT09PnT592mH89OnTCg0NTXWb0NDQNOf/8MMPOnPmjIoUKWJfn5SUpEGDBmny5Mk6duxYqvv19fWVr69vinFvb29+oF2Ma2xd5NbayK91kVvrIreuld2vbVapzanL3YdrbG3k17rIrXWRW2sjv66T3uuabb7lyMfHRzVq1ND69evtY8nJyVq/fr3Cw8NT3SY8PNxhvnT74y135nfr1k179+7V7t277a+wsDANGTJEa9ascd3JAAAAANkYtTkAAAAeJNnmTnRJGjhwoHr06KGaNWuqdu3amjx5sq5evaqePXtKkrp3766HHnpIEyZMkCS99NJLatiwoSZNmqSWLVtq0aJF2rFjhz744ANJUt68eZU3b16HY3h7eys0NFRly5bN3JMDAAAAshFqcwAAADwoslUTvWPHjjp79qxGjhyp2NhYVa1aVatXr7Z/QdGJEyfk4fF/N9fXq1dPCxcu1IgRI/Tqq6+qdOnSWr58uSpVquSuUwAAAAAsgdocAAAAD4ps1USXpP79+6t///6prtu4cWOKsQ4dOqhDhw7p3r/Zc9ABAAAAOKI2BwAAwIMg2zwTHQAAAAAAAACAzEYTHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwke2a6O+//76KFSsmPz8/1alTR9u2bUtz/pIlS1SuXDn5+fmpcuXKWrVqlX1dYmKihg4dqsqVKytnzpwKCwtT9+7dderUKVefBgAAAJDtUZsDAADgQZCtmuiLFy/WwIEDNWrUKO3atUtVqlRRZGSkzpw5k+r8zZs3q3Pnzurdu7d+/vlntWnTRm3atNH+/fslSdeuXdOuXbv0+uuva9euXVq2bJkOHz6sJ598MjNPCwAAAMh2qM0BAADwoMhWTfR33nlHzz77rHr27KkKFSpo5syZ8vf3V3R0dKrz33vvPTVr1kxDhgxR+fLlNW7cOFWvXl3Tpk2TJAUFBSkmJkZPP/20ypYtq7p162ratGnauXOnTpw4kZmnBgAAAGQr1OYAAAB4UHi5O4D0unnzpnbu3Knhw4fbxzw8PBQREaEtW7akus2WLVs0cOBAh7HIyEgtX77c9DiXLl2SzWZTcHCw6ZyEhAQlJCTYl+Pj4yXd/ghqYmJiOs4GGXXnunJ9rYfcWhv5tS5ya13kNnNk9+ubVWpz6vLMx3uEtZFf6yK31kVurY38ul56r222aaKfO3dOSUlJCgkJcRgPCQnRoUOHUt0mNjY21fmxsbGpzr9x44aGDh2qzp07KzAw0DSWCRMmaMyYMSnG165dK39//7udCu5DTEyMu0OAi5BbayO/1kVurYvcuta1a9fcHcJ9ySq1OXW5+/AeYW3k17rIrXWRW2sjv66T3ro82zTRXS0xMVFPP/20DMPQjBkz0pw7fPhwh7to4uPjVbhwYTVt2jTN5jvuXWJiomJiYtSkSRN5e3u7Oxw4Ebm1NvJrXeTWusht5rhzxzRSl97anLo88/EeYW3k17rIrXWRW2sjv66X3ro82zTR8+XLJ09PT50+fdph/PTp0woNDU11m9DQ0HTNv1OkHz9+XBs2bLhrwe3r6ytfX98U497e3vxAuxjX2LrIrbWRX+sit9ZFbl0ru1/brFKbU5e7D9fY2sivdZFb6yK31kZ+XSe91zXbfLGoj4+PatSoofXr19vHkpOTtX79eoWHh6e6TXh4uMN86fbHH/4+/06RfuTIEa1bt0558+Z1zQkAAAAAFkFtDgAAgAdJtrkTXZIGDhyoHj16qGbNmqpdu7YmT56sq1evqmfPnpKk7t2766GHHtKECRMkSS+99JIaNmyoSZMmqWXLllq0aJF27NihDz74QNLtIv2pp57Srl279PXXXyspKcn+TMY8efLIx8fHPScKAAAAZHHU5gAAAHhQZKsmeseOHXX27FmNHDlSsbGxqlq1qlavXm3/gqITJ07Iw+P/bq6vV6+eFi5cqBEjRujVV19V6dKltXz5clWqVEmS9Oeff+qrr76SJFWtWtXhWN9++60aNWqUKecFAAAAZDfU5gAAAHhQZKsmuiT1799f/fv3T3Xdxo0bU4x16NBBHTp0SHV+sWLFZBiGM8MDAAAAHhjU5gAAAHgQZJtnogMAAAAAAAAAkNloogMAAAAAAAAAYIImOgAAAAAAAAAAJmiiAwAAAAAAAABggiY6AAAAAAAAAAAmaKIDAAAAAAAAAGCCJjoAAAAAAAAAACZoogMAAAAAAAAAYIImOgAAAAAAAAAAJmiiAwAAAAAAAABggiY6AAAAAAAAAAAmaKIDAAAAAAAAAGCCJjoAAAAAAAAAACZoogMAAAAAAAAAYIImOgAAAAAAAAAAJmiiAwAAAAAAAABggiY6AAAAAAAAAAAmaKIDAAAAAAAAAGCCJjoAAAAAAAAAACZoogMAAAAAAAAAYIImOgAAAAAAAAAAJmiiAwAAAAAAAABggiY6AAAAAAAAAAAmaKIDAAAAAAAAAGCCJjoAAAAAAAAAACZoogMAAAAAAAAAYIImOgAAAAAAAAAAJmiiAwAAAAAAAABggiY6AAAAAAAAAAAmaKIDAAAAAAAAAGDCK6MbHD16VD/88IOOHz+ua9euKX/+/KpWrZrCw8Pl5+fnihgBAAAApILaHAAAAHC9dDfRFyxYoPfee087duxQSEiIwsLClCNHDl24cEG//fab/Pz81KVLFw0dOlRFixZ1ZcwAAADAA43aHAAAAMg86WqiV6tWTT4+PoqKitLnn3+uwoULO6xPSEjQli1btGjRItWsWVPTp09Xhw4dXBIwAAAA8CCjNgcAAAAyV7qa6G+88YYiIyNN1/v6+qpRo0Zq1KiR/vOf/+jYsWPOig8AAADA31CbAwAAAJkrXU30tIr0f8qbN6/y5s17zwEBAAAAMEdtDgAAAGQuj/RO/Oyzz3Tz5k378smTJ5WcnGxfvnbtmt566y3nRgcAAAAgXVauXKkhQ4Zo4MCB+vzzz90dDgAAAGAZ6W6id+7cWXFxcfblChUqOHw09PLlyxo+fLgzYwMAAACQDq+//rr+/e9/y2azyTAMvfLKK3rxxRfdHRYAAABgCel6nIskGYaR5jIAAACAzLFjxw7VrFnTvrx48WLt2bNHOXLkkCRFRUWpUaNGmjp1qrtCBAAAACwj3XeiAwAAAMga+vbtq5dfflnXrl2TJJUoUUKTJk3S4cOHtW/fPs2YMUNlypRxc5QAAACANdBEBwAAALKZrVu3qmDBgqpevbpWrFih6Oho/fzzz6pXr54aNGigkydPauHChe4OEwAAALCEdD/ORZLWrFmjoKAgSVJycrLWr1+v/fv3S5LD89IBAAAAuI6np6eGDh2qDh066Pnnn1fOnDk1bdo0hYWFuTs0AAAAwHIy1ETv0aOHw/Jzzz3n1GAAAAAApF+JEiW0Zs0affLJJ3r00Uf1yiuvqF+/fu4OCwAAALCUdD/OJTk5OV0vAAAAAK4VFxenf//732rVqpVGjBihtm3bauvWrdq+fbvq1q2rffv2uTtEAAAAwDKc9kz05ORkff31187aHQAAAAATPXr00NatW9WyZUsdPnxYzz//vPLmzau5c+fqP//5jzp27KihQ4e6O0wAAADAEjL0OJfU/Prrr4qOjtbcuXN19uxZJSYmOiMuAAAAACY2bNign3/+WaVKldKzzz6rUqVK2dc1btxYu3bt0tixY90YIQAAAGAd93Qn+vXr1zVv3jw9+uijKlu2rDZv3qyRI0fq5MmTzo4PAAAAwD+ULl1aH3zwgf73v/9p5syZKlq0qMN6Pz8//fe//3VTdAAAAIC1ZKiJvn37dj333HMKDQ3V5MmT1bp1a9lsNk2fPl19+/ZVSEiIq+IEAAAA8P9FR0drw4YNqlatmhYuXKgZM2a4OyQAAADAstL9OJeHH35Y8fHxeuaZZ7R582ZVrFhRkjRs2DCXBQcAAAAgpapVq2rHjh3uDgMAAAB4IKT7TvTDhw/r0Ucf1WOPPaYKFSq4MiYAAAAAJgzDcHcIAAAAwAMl3U3033//XWXLltXzzz+vQoUKafDgwfr5559ls9lcGR8AAACAv6lYsaIWLVqkmzdvpjnvyJEjev755/XGG29kUmQAAACANaW7if7QQw/ptdde06+//qpPPvlEsbGxql+/vm7duqW5c+fqf//7nyvjBAAAACBp6tSpevvttxUaGqqOHTtq4sSJWrBggT7//HN99NFHGjhwoGrXrq2qVasqMDBQzz//vLtDBgAAALK1dD8T/e8ef/xxPf7447p06ZIWLFig6Ohovf3226pUqZL27t3r7BgBAAAA/H+NGzfWjh079OOPP2rx4sVasGCBjh8/ruvXrytfvnyqVq2aunfvri5duih37tzuDhcAAADI9u6piX5HUFCQXnjhBb3wwgvavXu3oqOjnRUXAAAAgDQ88sgjeuSRR9wdBgAAAGB56X6cy91UrVpVU6ZMcdbuAAAAAAAAAABwu3Tfif7444/fdY7NZtP69evvKyAAAAAAAAAAALKKdDfRN27cqKJFi6ply5by9vZ2ZUwAAAAAAAAAAGQJ6W6iv/nmm5ozZ46WLFmiLl26qFevXqpUqZIrYwMAAAAAAAAAwK3S/Uz0IUOG6MCBA1q+fLkuX76s+vXrq3bt2po5c6bi4+NdGSMAAAAAAAAAAG6R4S8WDQ8P14cffqi//vpL/fr1U3R0tMLCwmikAwAAAJnks88+082bN+3LJ0+eVHJysn352rVreuutt9wRGgAAAGA5GW6i37Fr1y599913OnjwoCpVqsRz0gEAAIBM0rlzZ8XFxdmXK1SooGPHjtmXL1++rOHDh2d+YAAAAIAFZaiJfurUKf33v/9VmTJl9NRTTylPnjzaunWrfvrpJ+XIkcNVMQIAAAD4G8Mw0lwGAAAA4Dzp/mLRFi1a6Ntvv1XTpk01ceJEtWzZUl5e6d4cAAAAAAAAAIBsJ913oq9evVp58uTRiRMnNGbMGNWuXVvVq1dP8XK1999/X8WKFZOfn5/q1Kmjbdu2pTl/yZIlKleunPz8/FS5cmWtWrXKYb1hGBo5cqQKFiyoHDlyKCIiQkeOHHHlKQAAAACWQG0OAACAB0G6byUfNWqUK+NIl8WLF2vgwIGaOXOm6tSpo8mTJysyMlKHDx9WgQIFUszfvHmzOnfurAkTJuiJJ57QwoUL1aZNG+3atUuVKlWSJL311luaMmWKPv74YxUvXlyvv/66IiMjdeDAAfn5+WX2KQIAAADpsmbNGgUFBUmSkpOTtX79eu3fv1+SHJ6X7irU5gAAAHhQZKsm+jvvvKNnn31WPXv2lCTNnDlTK1euVHR0tIYNG5Zi/nvvvadmzZppyJAhkqRx48YpJiZG06ZN08yZM2UYhiZPnqwRI0aodevWkqR58+YpJCREy5cvV6dOnTLv5AAAAIAM6NGjh8Pyc88957Bss9lcenxqcwAAADwoMvTFou508+ZN7dy5UxEREfYxDw8PRUREaMuWLalus2XLFof5khQZGWmff/ToUcXGxjrMCQoKUp06dUz3CQAAALhbcnLyXV9JSUkuOz61OQAAAB4k6boTvVmzZho9erTq1q2b5rzLly9r+vTpCggIUL9+/ZwS4B3nzp1TUlKSQkJCHMZDQkJ06NChVLeJjY1NdX5sbKx9/Z0xszmpSUhIUEJCgn05Pj5ekpSYmKjExMR0nhEy4s515fpaD7m1NvJrXeTWusht5nDW9U1ISNCtW7eUM2dOp+wvvbJKbU5dnvl4j7A28mtd5Na6yK21kV/XS++1TVcTvUOHDmrfvr2CgoLUqlUr1axZU2FhYfLz89PFixd14MAB/fjjj1q1apVatmypiRMn3lfwWd2ECRM0ZsyYFONr166Vv7+/GyJ6cMTExLg7BLgIubU28mtd5Na6yK1rXbt27b62P3v2rLp3765169YpOTlZtWrV0vz581WqVCknRZg9UJe7D+8R1kZ+rYvcWhe5tTby6zrprcvT1UTv3bu3unbtqiVLlmjx4sX64IMPdOnSJUm3n7VYoUIFRUZGavv27Spfvvy9R52GfPnyydPTU6dPn3YYP336tEJDQ1PdJjQ0NM35d/57+vRpFSxY0GFO1apVTWMZPny4Bg4caF+Oj49X4cKF1bRpUwUGBmbovJA+iYmJiomJUZMmTeTt7e3ucOBE5NbayK91kVvrIreZ484d0/dq6NCh2r17t8aOHSs/Pz/NmjVLzz77rL799lsnRZi2rFKbU5dnPt4jrI38Whe5tS5ya23k1/XSW5en+4tFfX191bVrV3Xt2lWSdOnSJV2/fl158+bNlCT6+PioRo0aWr9+vdq0aSPp9rMg169fr/79+6e6TXh4uNavX6+XX37ZPhYTE6Pw8HBJUvHixRUaGqr169fbC/P4+Hht3bpVzz//vGksvr6+8vX1TTHu7e3ND7SLcY2ti9xaG/m1LnJrXeTWte732sbExGju3LmKjIyUJD3xxBMqX768EhISUq1TnS2r1ObU5e7DNbY28mtd5Na6yK21kV/XSe91TXcT/Z+CgoIUFBR0r5vfk4EDB6pHjx6qWbOmateurcmTJ+vq1avq2bOnJKl79+566KGHNGHCBEnSSy+9pIYNG2rSpElq2bKlFi1apB07duiDDz6QdPsu+pdfflnjx49X6dKlVbx4cb3++usKCwuz/2UAAAAAyGpOnTqlKlWq2JdLly4tX19f/fXXXypWrFimxEBtDgAAgAfFPTfR3aFjx446e/asRo4cqdjYWFWtWlWrV6+2f/nQiRMn5OHhYZ9fr149LVy4UCNGjNCrr76q0qVLa/ny5apUqZJ9zr///W9dvXpVffr0UVxcnB555BGtXr1afn5+mX5+AAAAQHp5enqmWDYMI9OOT20OAACAB0W2aqJLUv/+/U0/Irpx48YUYx06dFCHDh1M92ez2TR27FiNHTvWWSECAAAALmUYhsqUKSObzWYfu3LliqpVq+bQuL5w4YJL46A2BwAAwIMg2zXRAQAAgAfdnDlz3B0CAAAA8MCgiQ4AAABkMz169LjrnKSkpEyIBAAAALA+j7tPcfTHH3/o5MmT9uVt27bp5Zdftn8hEAAAAAD3+d///qehQ4eqUKFC7g4FAAAAsIQMN9GfeeYZffvtt5Kk2NhYNWnSRNu2bdNrr73GswsBAAAAN7h27ZrmzJmjBg0aqEKFCvruu+80cOBAd4cFAAAAWEKGH+eyf/9+1a5dW5L02WefqVKlStq0aZPWrl2rvn37auTIkU4PEgAAAEBKP/30kz766CMtWbJERYoU0cGDB/Xtt9+qQYMG7g4NAAAAsIwM34memJgoX19fSdK6dev05JNPSpLKlSunv/76y7nRAQAAAEhh0qRJqlixop566inlzp1b33//vfbt2yebzaa8efO6OzwAAADAUjLcRK9YsaJmzpypH374QTExMWrWrJkk6dSpUxTsAAAAQCYYOnSo2rRpo+PHj2vixImqUqWKu0MCAAAALCvDTfQ333xTs2bNUqNGjdS5c2d7wf7VV1/ZH/MCAAAAwHXGjRunJUuWqHjx4ho6dKj279/v7pAAAAAAy8rwM9EbNWqkc+fOKT4+Xrlz57aP9+nTR/7+/k4NDgAAAEBKw4cP1/Dhw/Xdd98pOjpaderUUalSpWQYhi5evOju8AAAAABLyfCd6NevX1dCQoK9gX78+HFNnjxZhw8fVoECBZweIAAAAIDUNWzYUB9//LFiY2P1wgsvqEaNGmrYsKHq1aund955x93hAQAAAJaQ4SZ669atNW/ePElSXFyc6tSpo0mTJqlNmzaaMWOG0wMEAAAAkLZcuXLpueee09atW/Xzzz+rdu3aeuONN9wdFgAAAGAJGW6i79q1Sw0aNJAkLV26VCEhITp+/LjmzZunKVOmOD1AAAAAAOlXuXJlTZ48WX/++ae7QwEAAAAsIcPPRL927Zpy5colSVq7dq3atWsnDw8P1a1bV8ePH3d6gAAAAAAc3flkaFpsNpu6deuWCdEAAAAA1pbhJnqpUqW0fPlytW3bVmvWrNErr7wiSTpz5owCAwOdHiAAAAAAR1FRUQoICJCXl5cMw0h1Dk10AAAAwDky/DiXkSNHavDgwSpWrJhq166t8PBwSbfvSq9WrZrTAwQAAADgqHz58vLx8VH37t313Xff6eLFiyleFy5ccHeYAAAAgCVkuIn+1FNP6cSJE9qxY4fWrFljH2/cuLHeffddpwYHAAAAIKVffvlFK1eu1PXr1/Xoo4+qZs2amjFjhuLj490dGgAAAGA5GW6iS1JoaKiqVaumU6dO6eTJk5Kk2rVrq1y5ck4NDgAAAEDq6tSpo1mzZumvv/7SgAED9Nlnn6lgwYLq0qWLEhIS3B0eAAAAYBkZbqInJydr7NixCgoKUtGiRVW0aFEFBwdr3LhxSk5OdkWMAAAAAEzkyJFD3bt315gxY1S7dm0tWrRI165dc3dYAAAAgGVk+ItFX3vtNc2ePVtvvPGG6tevL0n68ccfNXr0aN24cUP/+c9/nB4kAAAAgJT+/PNPffzxx5ozZ46uXr2qrl27asaMGcqdO7e7QwMAAAAsI8NN9I8//lgfffSRnnzySfvYww8/rIceekgvvPACTXQAAADAxT777DPNmTNH3333nSIjIzVp0iS1bNlSnp6e7g4NAAAAsJwMN9EvXLiQ6rPPy5UrpwsXLjglKAAAAADmOnXqpCJFiuiVV15RSEiIjh07pvfffz/FvAEDBrghOgAAAMBaMtxEr1KliqZNm6YpU6Y4jE+bNk1VqlRxWmAAAAAAUlekSBHZbDYtXLjQdI7NZqOJDgAAADhBhpvob731llq2bKl169YpPDxckrRlyxb98ccfWrVqldMDBAAAAODo2LFj7g4BAAAAeGB4ZHSDhg0b6n//+5/atm2ruLg4xcXFqV27djp8+LAaNGjgihgBAAAAAAAAAHCLDDfRJSksLEz/+c9/9Pnnn+vzzz/X+PHjlZycrD59+jg7PgAAAAD/sGXLFn399dcOY/PmzVPx4sVVoEAB9enTRwkJCW6KDgAAALCWe2qip+b8+fOaPXu2s3YHAAAAwMTYsWP1yy+/2Jf37dun3r17KyIiQsOGDdOKFSs0YcIEN0YIAAAAWIfTmugAAAAAMsfu3bvVuHFj+/KiRYtUp04dffjhhxo4cKCmTJmizz77zI0RAgAAANZBEx0AAADIZi5evKiQkBD78nfffafmzZvbl2vVqqU//vjDHaEBAAAAlkMTHQAAAMhmQkJCdPToUUnSzZs3tWvXLtWtW9e+/vLly/L29nZXeAAAAICleKV3Yrt27dJcHxcXd7+xAAAAAEiHFi1aaNiwYXrzzTe1fPly+fv7q0GDBvb1e/fuVcmSJd0YIQAAAGAd6W6iBwUF3XV99+7d7zsgAAAAAGkbN26c2rVrp4YNGyogIEAff/yxfHx87Oujo6PVtGlTN0YIAAAAWEe6m+hz5sxxZRwAAAAA0ilfvnz6/vvvdenSJQUEBMjT09Nh/ZIlSxQQEOCm6AAAAABrSXcTHQAAAEDWYvZp0Tx58mRyJAAAAIB18cWiAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACAiWzTRL9w4YK6dOmiwMBABQcHq3fv3rpy5Uqa29y4cUP9+vVT3rx5FRAQoPbt2+v06dP29Xv27FHnzp1VuHBh5ciRQ+XLl9d7773n6lMBAAAAsjVqcwAAADxIsk0TvUuXLvrll18UExOjr7/+Wt9//7369OmT5javvPKKVqxYoSVLlui7777TqVOn1K5dO/v6nTt3qkCBApo/f75++eUXvfbaaxo+fLimTZvm6tMBAAAAsi1qcwAAADxIvNwdQHocPHhQq1ev1vbt21WzZk1J0tSpU9WiRQu9/fbbCgsLS7HNpUuXNHv2bC1cuFCPP/64JGnOnDkqX768fvrpJ9WtW1e9evVy2KZEiRLasmWLli1bpv79+7v+xAAAAIBshtocAAAAD5ps0UTfsmWLgoOD7UW6JEVERMjDw0Nbt25V27ZtU2yzc+dOJSYmKiIiwj5Wrlw5FSlSRFu2bFHdunVTPdalS5eUJ0+eNONJSEhQQkKCfTk+Pl6SlJiYqMTExAydG9LnznXl+loPubU28mtd5Na6yG3myM7XNyvV5tTlmY/3CGsjv9ZFbq2L3Fob+XW99F7bbNFEj42NVYECBRzGvLy8lCdPHsXGxppu4+Pjo+DgYIfxkJAQ0202b96sxYsXa+XKlWnGM2HCBI0ZMybF+Nq1a+Xv75/mtrg/MTEx7g4BLkJurY38Whe5tS5y61rXrl1zdwj3LCvV5tTl7sN7hLWRX+sit9ZFbq2N/LpOeutytzbRhw0bpjfffDPNOQcPHsyUWPbv36/WrVtr1KhRatq0aZpzhw8froEDB9qX4+PjVbhwYTVt2lSBgYGuDvWBlJiYqJiYGDVp0kTe3t7uDgdORG6tjfxaF7m1LnKbOe7cMZ2VZMfanLo88/EeYW3k17rIrXWRW2sjv66X3rrcrU30QYMGKSoqKs05JUqUUGhoqM6cOeMwfuvWLV24cEGhoaGpbhcaGqqbN28qLi7O4Y6X06dPp9jmwIEDaty4sfr06aMRI0bcNW5fX1/5+vqmGPf29uYH2sW4xtZFbq2N/FoXubUucutaWfHaZsfanLrcfbjG1kZ+rYvcWhe5tTby6zrpva5ubaLnz59f+fPnv+u88PBwxcXFaefOnapRo4YkacOGDUpOTladOnVS3aZGjRry9vbW+vXr1b59e0nS4cOHdeLECYWHh9vn/fLLL3r88cfVo0cP/ec//3HCWQEAAADZD7U5AAAAkDoPdweQHuXLl1ezZs307LPPatu2bdq0aZP69++vTp06KSwsTJL0559/qly5ctq2bZskKSgoSL1799bAgQP17bffaufOnerZs6fCw8PtX1y0f/9+PfbYY2ratKkGDhyo2NhYxcbG6uzZs247VwAAACArozYHAADAgyZbfLGoJC1YsED9+/dX48aN5eHhofbt22vKlCn29YmJiTp8+LDDw+Dfffdd+9yEhARFRkZq+vTp9vVLly7V2bNnNX/+fM2fP98+XrRoUR07dixTzgsAAADIbqjNAQAA8CDJNk30PHnyaOHChabrixUrJsMwHMb8/Pz0/vvv6/333091m9GjR2v06NHODBMAAACwPGpzAAAAPEiyxeNcAAAAAAAAAABwB5roAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYyDZN9AsXLqhLly4KDAxUcHCwevfurStXrqS5zY0bN9SvXz/lzZtXAQEBat++vU6fPp3q3PPnz6tQoUKy2WyKi4tzwRkAAAAA1kBtDgAAgAdJtmmid+nSRb/88otiYmL09ddf6/vvv1efPn3S3OaVV17RihUrtGTJEn333Xc6deqU2rVrl+rc3r176+GHH3ZF6AAAAIClUJsDAADgQZItmugHDx7U6tWr9dFHH6lOnTp65JFHNHXqVC1atEinTp1KdZtLly5p9uzZeuedd/T444+rRo0amjNnjjZv3qyffvrJYe6MGTMUFxenwYMHZ8bpAAAAANkWtTkAAAAeNNmiib5lyxYFBwerZs2a9rGIiAh5eHho69atqW6zc+dOJSYmKiIiwj5Wrlw5FSlSRFu2bLGPHThwQGPHjtW8efPk4ZEtLgcAAADgNtTmAAAAeNB4uTuA9IiNjVWBAgUcxry8vJQnTx7FxsaabuPj46Pg4GCH8ZCQEPs2CQkJ6ty5syZOnKgiRYro999/T1c8CQkJSkhIsC/Hx8dLkhITE5WYmJje00IG3LmuXF/rIbfWRn6ti9xaF7nNHNn5+mal2py6PPPxHmFt5Ne6yK11kVtrI7+ul95r69Ym+rBhw/Tmm2+mOefgwYMuO/7w4cNVvnx5de3aNUPbTZgwQWPGjEkxvnbtWvn7+zsrPKQiJibG3SHARcittZFf6yK31kVuXevatWvuDiGF7FibU5e7D+8R1kZ+rYvcWhe5tTby6zrprcvd2kQfNGiQoqKi0pxTokQJhYaG6syZMw7jt27d0oULFxQaGprqdqGhobp586bi4uIc7ng5ffq0fZsNGzZo3759Wrp0qSTJMAxJUr58+fTaa6+lWpBLtwv8gQMH2pfj4+NVuHBhNW3aVIGBgWmeD+5NYmKiYmJi1KRJE3l7e7s7HDgRubU28mtd5Na6yG3muHPHdFaSHWtz6vLMx3uEtZFf6yK31kVurY38ul5663K3NtHz58+v/Pnz33VeeHi44uLitHPnTtWoUUPS7SI7OTlZderUSXWbGjVqyNvbW+vXr1f79u0lSYcPH9aJEycUHh4uSfr88891/fp1+zbbt29Xr1699MMPP6hkyZKm8fj6+srX1zfFuLe3Nz/QLsY1ti5ya23k17rIrXWRW9fKitc2O9bm1OXuwzW2NvJrXeTWusittZFf10nvdc0Wz0QvX768mjVrpmeffVYzZ85UYmKi+vfvr06dOiksLEyS9Oeff6px48aaN2+eateuraCgIPXu3VsDBw5Unjx5FBgYqBdffFHh4eGqW7euJKUoxs+dO2c/3j+f1wgAAACA2hwAAAAPnmzRRJekBQsWqH///mrcuLE8PDzUvn17TZkyxb4+MTFRhw8fdniOzbvvvmufm5CQoMjISE2fPt0d4QMAAACWQW0OAACAB0m2aaLnyZNHCxcuNF1frFgx+3MT7/Dz89P777+v999/P13HaNSoUYp9AAAAAHBEbQ4AAIAHiYe7AwAAAAAAAAAAIKuiiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJmugAAAAAAAAAAJigiQ4AAAAAAAAAgAma6AAAAAAAAAAAmKCJDgAAAAAAAACACZroAAAAAAAAAACYoIkOAAAAAAAAAIAJL3cHYAWGYUiS4uPj3RyJdSUmJuratWuKj4+Xt7e3u8OBE5FbayO/1kVurYvcZo47deOdOhLOQV3uerxHWBv5tS5ya13k1trIr+ulty6nie4Ely9fliQVLlzYzZEAAAAgO7l8+bKCgoLcHYZlUJcDAADgXtytLrcZ3P5y35KTk3Xq1CnlypVLNpvN3eFYUnx8vAoXLqw//vhDgYGB7g4HTkRurY38Whe5tS5ymzkMw9Dly5cVFhYmDw+esOgs1OWux3uEtZFf6yK31kVurY38ul5663LuRHcCDw8PFSpUyN1hPBACAwN507Aocmtt5Ne6yK11kVvX4w5056Muzzy8R1gb+bUucmtd5NbayK9rpacu57YXAAAAAAAAAABM0EQHAAAAAAAAAMAETXRkC76+vho1apR8fX3dHQqcjNxaG/m1LnJrXeQWQFp4j7A28mtd5Na6yK21kd+sgy8WBQAAAAAAAADABHeiAwAAAAAAAABggiY6AAAAAAAAAAAmaKIDAAAAAAAAAGCCJjqyhAsXLqhLly4KDAxUcHCwevfurStXrqS5zY0bN9SvXz/lzZtXAQEBat++vU6fPp3q3PPnz6tQoUKy2WyKi4tzwRkgLa7I7549e9S5c2cVLlxYOXLkUPny5fXee++5+lQeeO+//76KFSsmPz8/1alTR9u2bUtz/pIlS1SuXDn5+fmpcuXKWrVqlcN6wzA0cuRIFSxYUDly5FBERISOHDniylOACWfmNjExUUOHDlXlypWVM2dOhYWFqXv37jp16pSrTwMmnP27+3d9+/aVzWbT5MmTnRw1AHehNrcu6nJroTa3Lmpz66Iuz8YMIAto1qyZUaVKFeOnn34yfvjhB6NUqVJG586d09ymb9++RuHChY3169cbO3bsMOrWrWvUq1cv1bmtW7c2mjdvbkgyLl686IIzQFpckd/Zs2cbAwYMMDZu3Gj89ttvxieffGLkyJHDmDp1qqtP54G1aNEiw8fHx4iOjjZ++eUX49lnnzWCg4ON06dPpzp/06ZNhqenp/HWW28ZBw4cMEaMGGF4e3sb+/bts8954403jKCgIGP58uXGnj17jCeffNIoXry4cf369cw6LRjOz21cXJwRERFhLF682Dh06JCxZcsWo3bt2kaNGjUy87Tw/7nid/eOZcuWGVWqVDHCwsKMd99918VnAiCzUJtbF3W5dVCbWxe1uXVRl2dvNNHhdgcOHDAkGdu3b7ePffPNN4bNZjP+/PPPVLeJi4szvL29jSVLltjHDh48aEgytmzZ4jB3+vTpRsOGDY3169dTqLuBq/P7dy+88ILx2GOPOS94OKhdu7bRr18/+3JSUpIRFhZmTJgwIdX5Tz/9tNGyZUuHsTp16hjPPfecYRiGkZycbISGhhoTJ060r4+LizN8fX2NTz/91AVnADPOzm1qtm3bZkgyjh8/7pygkW6uyu/JkyeNhx56yNi/f79RtGhRinXAIqjNrYu63Fqoza2L2ty6qMuzNx7nArfbsmWLgoODVbNmTftYRESEPDw8tHXr1lS32blzpxITExUREWEfK1eunIoUKaItW7bYxw4cOKCxY8dq3rx58vDgx90dXJnff7p06ZLy5MnjvOBhd/PmTe3cudMhJx4eHoqIiDDNyZYtWxzmS1JkZKR9/tGjRxUbG+swJygoSHXq1Ekzz3AuV+Q2NZcuXZLNZlNwcLBT4kb6uCq/ycnJ6tatm4YMGaKKFSu6JngAbkFtbl3U5dZBbW5d1ObWRV2e/VG5wO1iY2NVoEABhzEvLy/lyZNHsbGxptv4+PikeMMPCQmxb5OQkKDOnTtr4sSJKlKkiEtix925Kr//tHnzZi1evFh9+vRxStxwdO7cOSUlJSkkJMRhPK2cxMbGpjn/zn8zsk84nyty+083btzQ0KFD1blzZwUGBjoncKSLq/L75ptvysvLSwMGDHB+0ADcitrcuqjLrYPa3Lqoza2Lujz7o4kOlxk2bJhsNluar0OHDrns+MOHD1f58uXVtWtXlx3jQebu/P7d/v371bp1a40aNUpNmzbNlGMCSJ/ExEQ9/fTTMgxDM2bMcHc4cIKdO3fqvffe09y5c2Wz2dwdDoB0cnftRm3uOu7O7d9RlwNZG7W5tVCXZy4vdwcA6xo0aJCioqLSnFOiRAmFhobqzJkzDuO3bt3ShQsXFBoamup2oaGhunnzpuLi4hzuijh9+rR9mw0bNmjfvn1aunSppNvfNC5J+fLl02uvvaYxY8bc45lBcn9+7zhw4IAaN26sPn36aMSIEfd0Lri7fPnyydPTU6dPn3YYTy0nd4SGhqY5/85/T58+rYIFCzrMqVq1qhOjR1pckds77hTpx48f14YNG7jTxQ1ckd8ffvhBZ86ccbiTNCkpSYMGDdLkyZN17Ngx554EAKdwd+1Gbe467s7tHdTlmYfa3Lqoza2LutwC3PtIduD/vuBmx44d9rE1a9ak6wtuli5dah87dOiQwxfc/Prrr8a+ffvsr+joaEOSsXnzZtNvPobzuSq/hmEY+/fvNwoUKGAMGTLEdScAu9q1axv9+/e3LyclJRkPPfRQml+C8sQTTziMhYeHp/jyorffftu+/tKlS3x5kRs4O7eGYRg3b9402rRpY1SsWNE4c+aMawJHujg7v+fOnXP4/+u+ffuMsLAwY+jQocahQ4dcdyIAMgW1uXVRl1sLtbl1UZtbF3V59kYTHVlCs2bNjGrVqhlbt241fvzxR6N06dJG586d7etPnjxplC1b1ti6dat9rG/fvkaRIkWMDRs2GDt27DDCw8ON8PBw02N8++23hiTj4sWLrjwVpMIV+d23b5+RP39+o2vXrsZff/1lf1EQuM6iRYsMX19fY+7cucaBAweMPn36GMHBwUZsbKxhGIbRrVs3Y9iwYfb5mzZtMry8vIy3337bOHjwoDFq1CjD29vb2Ldvn33OG2+8YQQHBxtffvmlsXfvXqN169ZG8eLFjevXr2f6+T3InJ3bmzdvGk8++aRRqFAhY/fu3Q6/owkJCW45xweZK353/6lo0aLGu+++6+pTAZBJqM2ti7rcOqjNrYva3Lqoy7M3mujIEs6fP2907tzZCAgIMAIDA42ePXsaly9ftq8/evSoIcn49ttv7WPXr183XnjhBSN37tyGv7+/0bZtW+Ovv/4yPQaFuvu4Ir+jRo0yJKV4FS1aNBPP7MEzdepUo0iRIoaPj49Ru3Zt46effrKva9iwodGjRw+H+Z999plRpkwZw8fHx6hYsaKxcuVKh/XJycnG66+/boSEhBi+vr5G48aNjcOHD2fGqeAfnJnbO7/Tqb3+/nuOzOPs391/olgHrIXa3Lqoy62F2ty6qM2ti7o8+7IZxv9/GB0AAAAAAAAAAHDg4e4AAAAAAAAAAADIqmiiAwAAAAAAAABggiY6AAAAAAAAAAAmaKIDAAAAAAAAAGCCJjoAAAAAAAAAACZoogMAAAAAAAAAYIImOgAAAAAAAAAAJmiiAwAAAAAAAABggiY6ACDLsdlsWr58ubvDAAAAAB5o1OUAcBtNdACAg6ioKNlsthSvZs2auTs0AAAA4IFBXQ4AWYeXuwMAAGQ9zZo105w5cxzGfH193RQNAAAA8GCiLgeArIE70QEAKfj6+io0NNThlTt3bkm3P9I5Y8YMNW/eXDly5FCJEiW0dOlSh+337dunxx9/XDly5FDevHnVp08fXblyxWFOdHS0KlasKF9fXxUsWFD9+/d3WH/u3Dm1bdtW/v7+Kl26tL766ivXnjQAAACQxVCXA0DWQBMdAJBhr7/+utq3b689e/aoS5cu6tSpkw4ePChJunr1qiIjI5U7d25t375dS5Ys0bp16xyK8RkzZqhfv37q06eP9u3bp6+++kqlSpVyOMaYMWP09NNPa+/evWrRooW6dOmiCxcuZOp5AgAAAFkZdTkAZA6bYRiGu4MAAGQdUVFRmj9/vvz8/BzGX331Vb366quy2Wzq27evZsyYYV9Xt25dVa9eXdOnT9eHH36ooUOH6o8//lDOnDklSatWrVKrVq106tQphYSE6KGHHlLPnj01fvz4VGOw2WwaMWKExo0bJ+n2XwACAgL0zTff8AxIAAAAPBCoywEg6+CZ6ACAFB577DGHYlyS8uTJY/9zeHi4w7rw8HDt3r1bknTw4EFVqVLFXqhLUv369ZWcnKzDhw/LZrPp1KlTaty4cZoxPPzww/Y/58yZU4GBgTpz5sy9nhIAAACQ7VCXA0DWQBMdAJBCzpw5U3yM01ly5MiRrnne3t4OyzabTcnJya4ICQAAAMiSqMsBIGvgmegAgAz76aefUiyXL19eklS+fHnt2bNHV69eta/ftGmTPDw8VLZsWeXKlUvFihXT+vXrMzVmAAAAwGqoywEgc3AnOgAghYSEBMXGxjqMeXl5KV++fJKkJUuWqGbNmnrkkUe0YMECbdu2TbNnz5YkdenSRaNGjVKPHj00evRonT17Vi+++KK6deumkJAQSdLo0aPVt29fFShQQM2bN9fly5e1adMmvfjii5l7ogAAAEAWRl0OAFkDTXQAQAqrV69WwYIFHcbKli2rQ4cOSZLGjBmjRYsW6YUXXlDBggX16aefqkKFCpIkf39/rVmzRi+99JJq1aolf39/tW/fXu+88459Xz169NCNGzf07rvvavDgwcqXL5+eeuqpzDtBAAAAIBugLgeArMFmGIbh7iAAANmHzWbTF198oTZt2rg7FAAAAOCBRV0OAJmHZ6IDAAAAAAAAAGCCJjoAAAAAAAAAACZ4nAsAAAAAAAAAACa4Ex0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAEzTRAQAAAAAAAAAwQRMdAAAAAAAAAAATNNEBAAAAAAAAADBBEx0AAAAAAAAAABM00QEAAAAAAAAAMEETHQAAAAAAAAAAE/8PCOxlocd/YG8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history plotted!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(train_losses, label='Train Loss', marker='o')\n",
    "axes[0].plot(val_losses, label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MAE)')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# SMAPE plot\n",
    "axes[1].plot(train_smapes, label='Train SMAPE', marker='o')\n",
    "axes[1].plot(val_smapes, label='Val SMAPE', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('SMAPE (%)')\n",
    "axes[1].set_title('Training and Validation SMAPE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plotted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457895f5",
   "metadata": {},
   "source": [
    "## Step 14: Load Best Model and Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d791f1ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading best model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MultimodalPricePredictor:\n\tMissing key(s) in state_dict: \"image_model.features.0.0.weight\", \"image_model.features.0.1.weight\", \"image_model.features.0.1.bias\", \"image_model.features.0.1.running_mean\", \"image_model.features.0.1.running_var\", \"image_model.features.1.conv.0.0.weight\", \"image_model.features.1.conv.0.1.weight\", \"image_model.features.1.conv.0.1.bias\", \"image_model.features.1.conv.0.1.running_mean\", \"image_model.features.1.conv.0.1.running_var\", \"image_model.features.1.conv.1.weight\", \"image_model.features.1.conv.2.weight\", \"image_model.features.1.conv.2.bias\", \"image_model.features.1.conv.2.running_mean\", \"image_model.features.1.conv.2.running_var\", \"image_model.features.2.conv.0.0.weight\", \"image_model.features.2.conv.0.1.weight\", \"image_model.features.2.conv.0.1.bias\", \"image_model.features.2.conv.0.1.running_mean\", \"image_model.features.2.conv.0.1.running_var\", \"image_model.features.2.conv.1.0.weight\", \"image_model.features.2.conv.1.1.weight\", \"image_model.features.2.conv.1.1.bias\", \"image_model.features.2.conv.1.1.running_mean\", \"image_model.features.2.conv.1.1.running_var\", \"image_model.features.2.conv.2.weight\", \"image_model.features.2.conv.3.weight\", \"image_model.features.2.conv.3.bias\", \"image_model.features.2.conv.3.running_mean\", \"image_model.features.2.conv.3.running_var\", \"image_model.features.3.conv.0.0.weight\", \"image_model.features.3.conv.0.1.weight\", \"image_model.features.3.conv.0.1.bias\", \"image_model.features.3.conv.0.1.running_mean\", \"image_model.features.3.conv.0.1.running_var\", \"image_model.features.3.conv.1.0.weight\", \"image_model.features.3.conv.1.1.weight\", \"image_model.features.3.conv.1.1.bias\", \"image_model.features.3.conv.1.1.running_mean\", \"image_model.features.3.conv.1.1.running_var\", \"image_model.features.3.conv.2.weight\", \"image_model.features.3.conv.3.weight\", \"image_model.features.3.conv.3.bias\", \"image_model.features.3.conv.3.running_mean\", \"image_model.features.3.conv.3.running_var\", \"image_model.features.4.conv.0.0.weight\", \"image_model.features.4.conv.0.1.weight\", \"image_model.features.4.conv.0.1.bias\", \"image_model.features.4.conv.0.1.running_mean\", \"image_model.features.4.conv.0.1.running_var\", \"image_model.features.4.conv.1.0.weight\", \"image_model.features.4.conv.1.1.weight\", \"image_model.features.4.conv.1.1.bias\", \"image_model.features.4.conv.1.1.running_mean\", \"image_model.features.4.conv.1.1.running_var\", \"image_model.features.4.conv.2.weight\", \"image_model.features.4.conv.3.weight\", \"image_model.features.4.conv.3.bias\", \"image_model.features.4.conv.3.running_mean\", \"image_model.features.4.conv.3.running_var\", \"image_model.features.5.conv.0.0.weight\", \"image_model.features.5.conv.0.1.weight\", \"image_model.features.5.conv.0.1.bias\", \"image_model.features.5.conv.0.1.running_mean\", \"image_model.features.5.conv.0.1.running_var\", \"image_model.features.5.conv.1.0.weight\", \"image_model.features.5.conv.1.1.weight\", \"image_model.features.5.conv.1.1.bias\", \"image_model.features.5.conv.1.1.running_mean\", \"image_model.features.5.conv.1.1.running_var\", \"image_model.features.5.conv.2.weight\", \"image_model.features.5.conv.3.weight\", \"image_model.features.5.conv.3.bias\", \"image_model.features.5.conv.3.running_mean\", \"image_model.features.5.conv.3.running_var\", \"image_model.features.6.conv.0.0.weight\", \"image_model.features.6.conv.0.1.weight\", \"image_model.features.6.conv.0.1.bias\", \"image_model.features.6.conv.0.1.running_mean\", \"image_model.features.6.conv.0.1.running_var\", \"image_model.features.6.conv.1.0.weight\", \"image_model.features.6.conv.1.1.weight\", \"image_model.features.6.conv.1.1.bias\", \"image_model.features.6.conv.1.1.running_mean\", \"image_model.features.6.conv.1.1.running_var\", \"image_model.features.6.conv.2.weight\", \"image_model.features.6.conv.3.weight\", \"image_model.features.6.conv.3.bias\", \"image_model.features.6.conv.3.running_mean\", \"image_model.features.6.conv.3.running_var\", \"image_model.features.7.conv.0.0.weight\", \"image_model.features.7.conv.0.1.weight\", \"image_model.features.7.conv.0.1.bias\", \"image_model.features.7.conv.0.1.running_mean\", \"image_model.features.7.conv.0.1.running_var\", \"image_model.features.7.conv.1.0.weight\", \"image_model.features.7.conv.1.1.weight\", \"image_model.features.7.conv.1.1.bias\", \"image_model.features.7.conv.1.1.running_mean\", \"image_model.features.7.conv.1.1.running_var\", \"image_model.features.7.conv.2.weight\", \"image_model.features.7.conv.3.weight\", \"image_model.features.7.conv.3.bias\", \"image_model.features.7.conv.3.running_mean\", \"image_model.features.7.conv.3.running_var\", \"image_model.features.8.conv.0.0.weight\", \"image_model.features.8.conv.0.1.weight\", \"image_model.features.8.conv.0.1.bias\", \"image_model.features.8.conv.0.1.running_mean\", \"image_model.features.8.conv.0.1.running_var\", \"image_model.features.8.conv.1.0.weight\", \"image_model.features.8.conv.1.1.weight\", \"image_model.features.8.conv.1.1.bias\", \"image_model.features.8.conv.1.1.running_mean\", \"image_model.features.8.conv.1.1.running_var\", \"image_model.features.8.conv.2.weight\", \"image_model.features.8.conv.3.weight\", \"image_model.features.8.conv.3.bias\", \"image_model.features.8.conv.3.running_mean\", \"image_model.features.8.conv.3.running_var\", \"image_model.features.9.conv.0.0.weight\", \"image_model.features.9.conv.0.1.weight\", \"image_model.features.9.conv.0.1.bias\", \"image_model.features.9.conv.0.1.running_mean\", \"image_model.features.9.conv.0.1.running_var\", \"image_model.features.9.conv.1.0.weight\", \"image_model.features.9.conv.1.1.weight\", \"image_model.features.9.conv.1.1.bias\", \"image_model.features.9.conv.1.1.running_mean\", \"image_model.features.9.conv.1.1.running_var\", \"image_model.features.9.conv.2.weight\", \"image_model.features.9.conv.3.weight\", \"image_model.features.9.conv.3.bias\", \"image_model.features.9.conv.3.running_mean\", \"image_model.features.9.conv.3.running_var\", \"image_model.features.10.conv.0.0.weight\", \"image_model.features.10.conv.0.1.weight\", \"image_model.features.10.conv.0.1.bias\", \"image_model.features.10.conv.0.1.running_mean\", \"image_model.features.10.conv.0.1.running_var\", \"image_model.features.10.conv.1.0.weight\", \"image_model.features.10.conv.1.1.weight\", \"image_model.features.10.conv.1.1.bias\", \"image_model.features.10.conv.1.1.running_mean\", \"image_model.features.10.conv.1.1.running_var\", \"image_model.features.10.conv.2.weight\", \"image_model.features.10.conv.3.weight\", \"image_model.features.10.conv.3.bias\", \"image_model.features.10.conv.3.running_mean\", \"image_model.features.10.conv.3.running_var\", \"image_model.features.11.conv.0.0.weight\", \"image_model.features.11.conv.0.1.weight\", \"image_model.features.11.conv.0.1.bias\", \"image_model.features.11.conv.0.1.running_mean\", \"image_model.features.11.conv.0.1.running_var\", \"image_model.features.11.conv.1.0.weight\", \"image_model.features.11.conv.1.1.weight\", \"image_model.features.11.conv.1.1.bias\", \"image_model.features.11.conv.1.1.running_mean\", \"image_model.features.11.conv.1.1.running_var\", \"image_model.features.11.conv.2.weight\", \"image_model.features.11.conv.3.weight\", \"image_model.features.11.conv.3.bias\", \"image_model.features.11.conv.3.running_mean\", \"image_model.features.11.conv.3.running_var\", \"image_model.features.12.conv.0.0.weight\", \"image_model.features.12.conv.0.1.weight\", \"image_model.features.12.conv.0.1.bias\", \"image_model.features.12.conv.0.1.running_mean\", \"image_model.features.12.conv.0.1.running_var\", \"image_model.features.12.conv.1.0.weight\", \"image_model.features.12.conv.1.1.weight\", \"image_model.features.12.conv.1.1.bias\", \"image_model.features.12.conv.1.1.running_mean\", \"image_model.features.12.conv.1.1.running_var\", \"image_model.features.12.conv.2.weight\", \"image_model.features.12.conv.3.weight\", \"image_model.features.12.conv.3.bias\", \"image_model.features.12.conv.3.running_mean\", \"image_model.features.12.conv.3.running_var\", \"image_model.features.13.conv.0.0.weight\", \"image_model.features.13.conv.0.1.weight\", \"image_model.features.13.conv.0.1.bias\", \"image_model.features.13.conv.0.1.running_mean\", \"image_model.features.13.conv.0.1.running_var\", \"image_model.features.13.conv.1.0.weight\", \"image_model.features.13.conv.1.1.weight\", \"image_model.features.13.conv.1.1.bias\", \"image_model.features.13.conv.1.1.running_mean\", \"image_model.features.13.conv.1.1.running_var\", \"image_model.features.13.conv.2.weight\", \"image_model.features.13.conv.3.weight\", \"image_model.features.13.conv.3.bias\", \"image_model.features.13.conv.3.running_mean\", \"image_model.features.13.conv.3.running_var\", \"image_model.features.14.conv.0.0.weight\", \"image_model.features.14.conv.0.1.weight\", \"image_model.features.14.conv.0.1.bias\", \"image_model.features.14.conv.0.1.running_mean\", \"image_model.features.14.conv.0.1.running_var\", \"image_model.features.14.conv.1.0.weight\", \"image_model.features.14.conv.1.1.weight\", \"image_model.features.14.conv.1.1.bias\", \"image_model.features.14.conv.1.1.running_mean\", \"image_model.features.14.conv.1.1.running_var\", \"image_model.features.14.conv.2.weight\", \"image_model.features.14.conv.3.weight\", \"image_model.features.14.conv.3.bias\", \"image_model.features.14.conv.3.running_mean\", \"image_model.features.14.conv.3.running_var\", \"image_model.features.15.conv.0.0.weight\", \"image_model.features.15.conv.0.1.weight\", \"image_model.features.15.conv.0.1.bias\", \"image_model.features.15.conv.0.1.running_mean\", \"image_model.features.15.conv.0.1.running_var\", \"image_model.features.15.conv.1.0.weight\", \"image_model.features.15.conv.1.1.weight\", \"image_model.features.15.conv.1.1.bias\", \"image_model.features.15.conv.1.1.running_mean\", \"image_model.features.15.conv.1.1.running_var\", \"image_model.features.15.conv.2.weight\", \"image_model.features.15.conv.3.weight\", \"image_model.features.15.conv.3.bias\", \"image_model.features.15.conv.3.running_mean\", \"image_model.features.15.conv.3.running_var\", \"image_model.features.16.conv.0.0.weight\", \"image_model.features.16.conv.0.1.weight\", \"image_model.features.16.conv.0.1.bias\", \"image_model.features.16.conv.0.1.running_mean\", \"image_model.features.16.conv.0.1.running_var\", \"image_model.features.16.conv.1.0.weight\", \"image_model.features.16.conv.1.1.weight\", \"image_model.features.16.conv.1.1.bias\", \"image_model.features.16.conv.1.1.running_mean\", \"image_model.features.16.conv.1.1.running_var\", \"image_model.features.16.conv.2.weight\", \"image_model.features.16.conv.3.weight\", \"image_model.features.16.conv.3.bias\", \"image_model.features.16.conv.3.running_mean\", \"image_model.features.16.conv.3.running_var\", \"image_model.features.17.conv.0.0.weight\", \"image_model.features.17.conv.0.1.weight\", \"image_model.features.17.conv.0.1.bias\", \"image_model.features.17.conv.0.1.running_mean\", \"image_model.features.17.conv.0.1.running_var\", \"image_model.features.17.conv.1.0.weight\", \"image_model.features.17.conv.1.1.weight\", \"image_model.features.17.conv.1.1.bias\", \"image_model.features.17.conv.1.1.running_mean\", \"image_model.features.17.conv.1.1.running_var\", \"image_model.features.17.conv.2.weight\", \"image_model.features.17.conv.3.weight\", \"image_model.features.17.conv.3.bias\", \"image_model.features.17.conv.3.running_mean\", \"image_model.features.17.conv.3.running_var\", \"image_model.features.18.0.weight\", \"image_model.features.18.1.weight\", \"image_model.features.18.1.bias\", \"image_model.features.18.1.running_mean\", \"image_model.features.18.1.running_var\", \"fusion_head.5.weight\", \"fusion_head.5.bias\". \n\tUnexpected key(s) in state_dict: \"image_model.conv1.weight\", \"image_model.bn1.weight\", \"image_model.bn1.bias\", \"image_model.bn1.running_mean\", \"image_model.bn1.running_var\", \"image_model.bn1.num_batches_tracked\", \"image_model.layer1.0.conv1.weight\", \"image_model.layer1.0.bn1.weight\", \"image_model.layer1.0.bn1.bias\", \"image_model.layer1.0.bn1.running_mean\", \"image_model.layer1.0.bn1.running_var\", \"image_model.layer1.0.bn1.num_batches_tracked\", \"image_model.layer1.0.conv2.weight\", \"image_model.layer1.0.bn2.weight\", \"image_model.layer1.0.bn2.bias\", \"image_model.layer1.0.bn2.running_mean\", \"image_model.layer1.0.bn2.running_var\", \"image_model.layer1.0.bn2.num_batches_tracked\", \"image_model.layer1.0.conv3.weight\", \"image_model.layer1.0.bn3.weight\", \"image_model.layer1.0.bn3.bias\", \"image_model.layer1.0.bn3.running_mean\", \"image_model.layer1.0.bn3.running_var\", \"image_model.layer1.0.bn3.num_batches_tracked\", \"image_model.layer1.0.downsample.0.weight\", \"image_model.layer1.0.downsample.1.weight\", \"image_model.layer1.0.downsample.1.bias\", \"image_model.layer1.0.downsample.1.running_mean\", \"image_model.layer1.0.downsample.1.running_var\", \"image_model.layer1.0.downsample.1.num_batches_tracked\", \"image_model.layer1.1.conv1.weight\", \"image_model.layer1.1.bn1.weight\", \"image_model.layer1.1.bn1.bias\", \"image_model.layer1.1.bn1.running_mean\", \"image_model.layer1.1.bn1.running_var\", \"image_model.layer1.1.bn1.num_batches_tracked\", \"image_model.layer1.1.conv2.weight\", \"image_model.layer1.1.bn2.weight\", \"image_model.layer1.1.bn2.bias\", \"image_model.layer1.1.bn2.running_mean\", \"image_model.layer1.1.bn2.running_var\", \"image_model.layer1.1.bn2.num_batches_tracked\", \"image_model.layer1.1.conv3.weight\", \"image_model.layer1.1.bn3.weight\", \"image_model.layer1.1.bn3.bias\", \"image_model.layer1.1.bn3.running_mean\", \"image_model.layer1.1.bn3.running_var\", \"image_model.layer1.1.bn3.num_batches_tracked\", \"image_model.layer1.2.conv1.weight\", \"image_model.layer1.2.bn1.weight\", \"image_model.layer1.2.bn1.bias\", \"image_model.layer1.2.bn1.running_mean\", \"image_model.layer1.2.bn1.running_var\", \"image_model.layer1.2.bn1.num_batches_tracked\", \"image_model.layer1.2.conv2.weight\", \"image_model.layer1.2.bn2.weight\", \"image_model.layer1.2.bn2.bias\", \"image_model.layer1.2.bn2.running_mean\", \"image_model.layer1.2.bn2.running_var\", \"image_model.layer1.2.bn2.num_batches_tracked\", \"image_model.layer1.2.conv3.weight\", \"image_model.layer1.2.bn3.weight\", \"image_model.layer1.2.bn3.bias\", \"image_model.layer1.2.bn3.running_mean\", \"image_model.layer1.2.bn3.running_var\", \"image_model.layer1.2.bn3.num_batches_tracked\", \"image_model.layer2.0.conv1.weight\", \"image_model.layer2.0.bn1.weight\", \"image_model.layer2.0.bn1.bias\", \"image_model.layer2.0.bn1.running_mean\", \"image_model.layer2.0.bn1.running_var\", \"image_model.layer2.0.bn1.num_batches_tracked\", \"image_model.layer2.0.conv2.weight\", \"image_model.layer2.0.bn2.weight\", \"image_model.layer2.0.bn2.bias\", \"image_model.layer2.0.bn2.running_mean\", \"image_model.layer2.0.bn2.running_var\", \"image_model.layer2.0.bn2.num_batches_tracked\", \"image_model.layer2.0.conv3.weight\", \"image_model.layer2.0.bn3.weight\", \"image_model.layer2.0.bn3.bias\", \"image_model.layer2.0.bn3.running_mean\", \"image_model.layer2.0.bn3.running_var\", \"image_model.layer2.0.bn3.num_batches_tracked\", \"image_model.layer2.0.downsample.0.weight\", \"image_model.layer2.0.downsample.1.weight\", \"image_model.layer2.0.downsample.1.bias\", \"image_model.layer2.0.downsample.1.running_mean\", \"image_model.layer2.0.downsample.1.running_var\", \"image_model.layer2.0.downsample.1.num_batches_tracked\", \"image_model.layer2.1.conv1.weight\", \"image_model.layer2.1.bn1.weight\", \"image_model.layer2.1.bn1.bias\", \"image_model.layer2.1.bn1.running_mean\", \"image_model.layer2.1.bn1.running_var\", \"image_model.layer2.1.bn1.num_batches_tracked\", \"image_model.layer2.1.conv2.weight\", \"image_model.layer2.1.bn2.weight\", \"image_model.layer2.1.bn2.bias\", \"image_model.layer2.1.bn2.running_mean\", \"image_model.layer2.1.bn2.running_var\", \"image_model.layer2.1.bn2.num_batches_tracked\", \"image_model.layer2.1.conv3.weight\", \"image_model.layer2.1.bn3.weight\", \"image_model.layer2.1.bn3.bias\", \"image_model.layer2.1.bn3.running_mean\", \"image_model.layer2.1.bn3.running_var\", \"image_model.layer2.1.bn3.num_batches_tracked\", \"image_model.layer2.2.conv1.weight\", \"image_model.layer2.2.bn1.weight\", \"image_model.layer2.2.bn1.bias\", \"image_model.layer2.2.bn1.running_mean\", \"image_model.layer2.2.bn1.running_var\", \"image_model.layer2.2.bn1.num_batches_tracked\", \"image_model.layer2.2.conv2.weight\", \"image_model.layer2.2.bn2.weight\", \"image_model.layer2.2.bn2.bias\", \"image_model.layer2.2.bn2.running_mean\", \"image_model.layer2.2.bn2.running_var\", \"image_model.layer2.2.bn2.num_batches_tracked\", \"image_model.layer2.2.conv3.weight\", \"image_model.layer2.2.bn3.weight\", \"image_model.layer2.2.bn3.bias\", \"image_model.layer2.2.bn3.running_mean\", \"image_model.layer2.2.bn3.running_var\", \"image_model.layer2.2.bn3.num_batches_tracked\", \"image_model.layer2.3.conv1.weight\", \"image_model.layer2.3.bn1.weight\", \"image_model.layer2.3.bn1.bias\", \"image_model.layer2.3.bn1.running_mean\", \"image_model.layer2.3.bn1.running_var\", \"image_model.layer2.3.bn1.num_batches_tracked\", \"image_model.layer2.3.conv2.weight\", \"image_model.layer2.3.bn2.weight\", \"image_model.layer2.3.bn2.bias\", \"image_model.layer2.3.bn2.running_mean\", \"image_model.layer2.3.bn2.running_var\", \"image_model.layer2.3.bn2.num_batches_tracked\", \"image_model.layer2.3.conv3.weight\", \"image_model.layer2.3.bn3.weight\", \"image_model.layer2.3.bn3.bias\", \"image_model.layer2.3.bn3.running_mean\", \"image_model.layer2.3.bn3.running_var\", \"image_model.layer2.3.bn3.num_batches_tracked\", \"image_model.layer3.0.conv1.weight\", \"image_model.layer3.0.bn1.weight\", \"image_model.layer3.0.bn1.bias\", \"image_model.layer3.0.bn1.running_mean\", \"image_model.layer3.0.bn1.running_var\", \"image_model.layer3.0.bn1.num_batches_tracked\", \"image_model.layer3.0.conv2.weight\", \"image_model.layer3.0.bn2.weight\", \"image_model.layer3.0.bn2.bias\", \"image_model.layer3.0.bn2.running_mean\", \"image_model.layer3.0.bn2.running_var\", \"image_model.layer3.0.bn2.num_batches_tracked\", \"image_model.layer3.0.conv3.weight\", \"image_model.layer3.0.bn3.weight\", \"image_model.layer3.0.bn3.bias\", \"image_model.layer3.0.bn3.running_mean\", \"image_model.layer3.0.bn3.running_var\", \"image_model.layer3.0.bn3.num_batches_tracked\", \"image_model.layer3.0.downsample.0.weight\", \"image_model.layer3.0.downsample.1.weight\", \"image_model.layer3.0.downsample.1.bias\", \"image_model.layer3.0.downsample.1.running_mean\", \"image_model.layer3.0.downsample.1.running_var\", \"image_model.layer3.0.downsample.1.num_batches_tracked\", \"image_model.layer3.1.conv1.weight\", \"image_model.layer3.1.bn1.weight\", \"image_model.layer3.1.bn1.bias\", \"image_model.layer3.1.bn1.running_mean\", \"image_model.layer3.1.bn1.running_var\", \"image_model.layer3.1.bn1.num_batches_tracked\", \"image_model.layer3.1.conv2.weight\", \"image_model.layer3.1.bn2.weight\", \"image_model.layer3.1.bn2.bias\", \"image_model.layer3.1.bn2.running_mean\", \"image_model.layer3.1.bn2.running_var\", \"image_model.layer3.1.bn2.num_batches_tracked\", \"image_model.layer3.1.conv3.weight\", \"image_model.layer3.1.bn3.weight\", \"image_model.layer3.1.bn3.bias\", \"image_model.layer3.1.bn3.running_mean\", \"image_model.layer3.1.bn3.running_var\", \"image_model.layer3.1.bn3.num_batches_tracked\", \"image_model.layer3.2.conv1.weight\", \"image_model.layer3.2.bn1.weight\", \"image_model.layer3.2.bn1.bias\", \"image_model.layer3.2.bn1.running_mean\", \"image_model.layer3.2.bn1.running_var\", \"image_model.layer3.2.bn1.num_batches_tracked\", \"image_model.layer3.2.conv2.weight\", \"image_model.layer3.2.bn2.weight\", \"image_model.layer3.2.bn2.bias\", \"image_model.layer3.2.bn2.running_mean\", \"image_model.layer3.2.bn2.running_var\", \"image_model.layer3.2.bn2.num_batches_tracked\", \"image_model.layer3.2.conv3.weight\", \"image_model.layer3.2.bn3.weight\", \"image_model.layer3.2.bn3.bias\", \"image_model.layer3.2.bn3.running_mean\", \"image_model.layer3.2.bn3.running_var\", \"image_model.layer3.2.bn3.num_batches_tracked\", \"image_model.layer3.3.conv1.weight\", \"image_model.layer3.3.bn1.weight\", \"image_model.layer3.3.bn1.bias\", \"image_model.layer3.3.bn1.running_mean\", \"image_model.layer3.3.bn1.running_var\", \"image_model.layer3.3.bn1.num_batches_tracked\", \"image_model.layer3.3.conv2.weight\", \"image_model.layer3.3.bn2.weight\", \"image_model.layer3.3.bn2.bias\", \"image_model.layer3.3.bn2.running_mean\", \"image_model.layer3.3.bn2.running_var\", \"image_model.layer3.3.bn2.num_batches_tracked\", \"image_model.layer3.3.conv3.weight\", \"image_model.layer3.3.bn3.weight\", \"image_model.layer3.3.bn3.bias\", \"image_model.layer3.3.bn3.running_mean\", \"image_model.layer3.3.bn3.running_var\", \"image_model.layer3.3.bn3.num_batches_tracked\", \"image_model.layer3.4.conv1.weight\", \"image_model.layer3.4.bn1.weight\", \"image_model.layer3.4.bn1.bias\", \"image_model.layer3.4.bn1.running_mean\", \"image_model.layer3.4.bn1.running_var\", \"image_model.layer3.4.bn1.num_batches_tracked\", \"image_model.layer3.4.conv2.weight\", \"image_model.layer3.4.bn2.weight\", \"image_model.layer3.4.bn2.bias\", \"image_model.layer3.4.bn2.running_mean\", \"image_model.layer3.4.bn2.running_var\", \"image_model.layer3.4.bn2.num_batches_tracked\", \"image_model.layer3.4.conv3.weight\", \"image_model.layer3.4.bn3.weight\", \"image_model.layer3.4.bn3.bias\", \"image_model.layer3.4.bn3.running_mean\", \"image_model.layer3.4.bn3.running_var\", \"image_model.layer3.4.bn3.num_batches_tracked\", \"image_model.layer3.5.conv1.weight\", \"image_model.layer3.5.bn1.weight\", \"image_model.layer3.5.bn1.bias\", \"image_model.layer3.5.bn1.running_mean\", \"image_model.layer3.5.bn1.running_var\", \"image_model.layer3.5.bn1.num_batches_tracked\", \"image_model.layer3.5.conv2.weight\", \"image_model.layer3.5.bn2.weight\", \"image_model.layer3.5.bn2.bias\", \"image_model.layer3.5.bn2.running_mean\", \"image_model.layer3.5.bn2.running_var\", \"image_model.layer3.5.bn2.num_batches_tracked\", \"image_model.layer3.5.conv3.weight\", \"image_model.layer3.5.bn3.weight\", \"image_model.layer3.5.bn3.bias\", \"image_model.layer3.5.bn3.running_mean\", \"image_model.layer3.5.bn3.running_var\", \"image_model.layer3.5.bn3.num_batches_tracked\", \"image_model.layer4.0.conv1.weight\", \"image_model.layer4.0.bn1.weight\", \"image_model.layer4.0.bn1.bias\", \"image_model.layer4.0.bn1.running_mean\", \"image_model.layer4.0.bn1.running_var\", \"image_model.layer4.0.bn1.num_batches_tracked\", \"image_model.layer4.0.conv2.weight\", \"image_model.layer4.0.bn2.weight\", \"image_model.layer4.0.bn2.bias\", \"image_model.layer4.0.bn2.running_mean\", \"image_model.layer4.0.bn2.running_var\", \"image_model.layer4.0.bn2.num_batches_tracked\", \"image_model.layer4.0.conv3.weight\", \"image_model.layer4.0.bn3.weight\", \"image_model.layer4.0.bn3.bias\", \"image_model.layer4.0.bn3.running_mean\", \"image_model.layer4.0.bn3.running_var\", \"image_model.layer4.0.bn3.num_batches_tracked\", \"image_model.layer4.0.downsample.0.weight\", \"image_model.layer4.0.downsample.1.weight\", \"image_model.layer4.0.downsample.1.bias\", \"image_model.layer4.0.downsample.1.running_mean\", \"image_model.layer4.0.downsample.1.running_var\", \"image_model.layer4.0.downsample.1.num_batches_tracked\", \"image_model.layer4.1.conv1.weight\", \"image_model.layer4.1.bn1.weight\", \"image_model.layer4.1.bn1.bias\", \"image_model.layer4.1.bn1.running_mean\", \"image_model.layer4.1.bn1.running_var\", \"image_model.layer4.1.bn1.num_batches_tracked\", \"image_model.layer4.1.conv2.weight\", \"image_model.layer4.1.bn2.weight\", \"image_model.layer4.1.bn2.bias\", \"image_model.layer4.1.bn2.running_mean\", \"image_model.layer4.1.bn2.running_var\", \"image_model.layer4.1.bn2.num_batches_tracked\", \"image_model.layer4.1.conv3.weight\", \"image_model.layer4.1.bn3.weight\", \"image_model.layer4.1.bn3.bias\", \"image_model.layer4.1.bn3.running_mean\", \"image_model.layer4.1.bn3.running_var\", \"image_model.layer4.1.bn3.num_batches_tracked\", \"image_model.layer4.2.conv1.weight\", \"image_model.layer4.2.bn1.weight\", \"image_model.layer4.2.bn1.bias\", \"image_model.layer4.2.bn1.running_mean\", \"image_model.layer4.2.bn1.running_var\", \"image_model.layer4.2.bn1.num_batches_tracked\", \"image_model.layer4.2.conv2.weight\", \"image_model.layer4.2.bn2.weight\", \"image_model.layer4.2.bn2.bias\", \"image_model.layer4.2.bn2.running_mean\", \"image_model.layer4.2.bn2.running_var\", \"image_model.layer4.2.bn2.num_batches_tracked\", \"image_model.layer4.2.conv3.weight\", \"image_model.layer4.2.bn3.weight\", \"image_model.layer4.2.bn3.bias\", \"image_model.layer4.2.bn3.running_mean\", \"image_model.layer4.2.bn3.running_var\", \"image_model.layer4.2.bn3.num_batches_tracked\", \"image_projection.3.weight\", \"image_projection.3.bias\", \"text_projection.3.weight\", \"text_projection.3.bias\", \"fusion_head.6.weight\", \"fusion_head.6.bias\", \"fusion_head.9.weight\", \"fusion_head.9.bias\". \n\tsize mismatch for image_projection.0.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([64, 1280]).\n\tsize mismatch for image_projection.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for text_projection.0.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([64, 768]).\n\tsize mismatch for text_projection.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fusion_head.0.weight: copying a param with shape torch.Size([256, 257]) from checkpoint, the shape in current model is torch.Size([64, 129]).\n\tsize mismatch for fusion_head.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fusion_head.3.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for fusion_head.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading best model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m checkpoint = torch.load(config.MODEL_SAVE_PATH, weights_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel_state_dict\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest model loaded from epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest validation SMAPE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint[\u001b[33m'\u001b[39m\u001b[33mval_smape\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github-repos/amazon-ml-challenge/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2624\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2616\u001b[39m         error_msgs.insert(\n\u001b[32m   2617\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2618\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2620\u001b[39m             ),\n\u001b[32m   2621\u001b[39m         )\n\u001b[32m   2623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2624\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2625\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2627\u001b[39m         )\n\u001b[32m   2628\u001b[39m     )\n\u001b[32m   2629\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for MultimodalPricePredictor:\n\tMissing key(s) in state_dict: \"image_model.features.0.0.weight\", \"image_model.features.0.1.weight\", \"image_model.features.0.1.bias\", \"image_model.features.0.1.running_mean\", \"image_model.features.0.1.running_var\", \"image_model.features.1.conv.0.0.weight\", \"image_model.features.1.conv.0.1.weight\", \"image_model.features.1.conv.0.1.bias\", \"image_model.features.1.conv.0.1.running_mean\", \"image_model.features.1.conv.0.1.running_var\", \"image_model.features.1.conv.1.weight\", \"image_model.features.1.conv.2.weight\", \"image_model.features.1.conv.2.bias\", \"image_model.features.1.conv.2.running_mean\", \"image_model.features.1.conv.2.running_var\", \"image_model.features.2.conv.0.0.weight\", \"image_model.features.2.conv.0.1.weight\", \"image_model.features.2.conv.0.1.bias\", \"image_model.features.2.conv.0.1.running_mean\", \"image_model.features.2.conv.0.1.running_var\", \"image_model.features.2.conv.1.0.weight\", \"image_model.features.2.conv.1.1.weight\", \"image_model.features.2.conv.1.1.bias\", \"image_model.features.2.conv.1.1.running_mean\", \"image_model.features.2.conv.1.1.running_var\", \"image_model.features.2.conv.2.weight\", \"image_model.features.2.conv.3.weight\", \"image_model.features.2.conv.3.bias\", \"image_model.features.2.conv.3.running_mean\", \"image_model.features.2.conv.3.running_var\", \"image_model.features.3.conv.0.0.weight\", \"image_model.features.3.conv.0.1.weight\", \"image_model.features.3.conv.0.1.bias\", \"image_model.features.3.conv.0.1.running_mean\", \"image_model.features.3.conv.0.1.running_var\", \"image_model.features.3.conv.1.0.weight\", \"image_model.features.3.conv.1.1.weight\", \"image_model.features.3.conv.1.1.bias\", \"image_model.features.3.conv.1.1.running_mean\", \"image_model.features.3.conv.1.1.running_var\", \"image_model.features.3.conv.2.weight\", \"image_model.features.3.conv.3.weight\", \"image_model.features.3.conv.3.bias\", \"image_model.features.3.conv.3.running_mean\", \"image_model.features.3.conv.3.running_var\", \"image_model.features.4.conv.0.0.weight\", \"image_model.features.4.conv.0.1.weight\", \"image_model.features.4.conv.0.1.bias\", \"image_model.features.4.conv.0.1.running_mean\", \"image_model.features.4.conv.0.1.running_var\", \"image_model.features.4.conv.1.0.weight\", \"image_model.features.4.conv.1.1.weight\", \"image_model.features.4.conv.1.1.bias\", \"image_model.features.4.conv.1.1.running_mean\", \"image_model.features.4.conv.1.1.running_var\", \"image_model.features.4.conv.2.weight\", \"image_model.features.4.conv.3.weight\", \"image_model.features.4.conv.3.bias\", \"image_model.features.4.conv.3.running_mean\", \"image_model.features.4.conv.3.running_var\", \"image_model.features.5.conv.0.0.weight\", \"image_model.features.5.conv.0.1.weight\", \"image_model.features.5.conv.0.1.bias\", \"image_model.features.5.conv.0.1.running_mean\", \"image_model.features.5.conv.0.1.running_var\", \"image_model.features.5.conv.1.0.weight\", \"image_model.features.5.conv.1.1.weight\", \"image_model.features.5.conv.1.1.bias\", \"image_model.features.5.conv.1.1.running_mean\", \"image_model.features.5.conv.1.1.running_var\", \"image_model.features.5.conv.2.weight\", \"image_model.features.5.conv.3.weight\", \"image_model.features.5.conv.3.bias\", \"image_model.features.5.conv.3.running_mean\", \"image_model.features.5.conv.3.running_var\", \"image_model.features.6.conv.0.0.weight\", \"image_model.features.6.conv.0.1.weight\", \"image_model.features.6.conv.0.1.bias\", \"image_model.features.6.conv.0.1.running_mean\", \"image_model.features.6.conv.0.1.running_var\", \"image_model.features.6.conv.1.0.weight\", \"image_model.features.6.conv.1.1.weight\", \"image_model.features.6.conv.1.1.bias\", \"image_model.features.6.conv.1.1.running_mean\", \"image_model.features.6.conv.1.1.running_var\", \"image_model.features.6.conv.2.weight\", \"image_model.features.6.conv.3.weight\", \"image_model.features.6.conv.3.bias\", \"image_model.features.6.conv.3.running_mean\", \"image_model.features.6.conv.3.running_var\", \"image_model.features.7.conv.0.0.weight\", \"image_model.features.7.conv.0.1.weight\", \"image_model.features.7.conv.0.1.bias\", \"image_model.features.7.conv.0.1.running_mean\", \"image_model.features.7.conv.0.1.running_var\", \"image_model.features.7.conv.1.0.weight\", \"image_model.features.7.conv.1.1.weight\", \"image_model.features.7.conv.1.1.bias\", \"image_model.features.7.conv.1.1.running_mean\", \"image_model.features.7.conv.1.1.running_var\", \"image_model.features.7.conv.2.weight\", \"image_model.features.7.conv.3.weight\", \"image_model.features.7.conv.3.bias\", \"image_model.features.7.conv.3.running_mean\", \"image_model.features.7.conv.3.running_var\", \"image_model.features.8.conv.0.0.weight\", \"image_model.features.8.conv.0.1.weight\", \"image_model.features.8.conv.0.1.bias\", \"image_model.features.8.conv.0.1.running_mean\", \"image_model.features.8.conv.0.1.running_var\", \"image_model.features.8.conv.1.0.weight\", \"image_model.features.8.conv.1.1.weight\", \"image_model.features.8.conv.1.1.bias\", \"image_model.features.8.conv.1.1.running_mean\", \"image_model.features.8.conv.1.1.running_var\", \"image_model.features.8.conv.2.weight\", \"image_model.features.8.conv.3.weight\", \"image_model.features.8.conv.3.bias\", \"image_model.features.8.conv.3.running_mean\", \"image_model.features.8.conv.3.running_var\", \"image_model.features.9.conv.0.0.weight\", \"image_model.features.9.conv.0.1.weight\", \"image_model.features.9.conv.0.1.bias\", \"image_model.features.9.conv.0.1.running_mean\", \"image_model.features.9.conv.0.1.running_var\", \"image_model.features.9.conv.1.0.weight\", \"image_model.features.9.conv.1.1.weight\", \"image_model.features.9.conv.1.1.bias\", \"image_model.features.9.conv.1.1.running_mean\", \"image_model.features.9.conv.1.1.running_var\", \"image_model.features.9.conv.2.weight\", \"image_model.features.9.conv.3.weight\", \"image_model.features.9.conv.3.bias\", \"image_model.features.9.conv.3.running_mean\", \"image_model.features.9.conv.3.running_var\", \"image_model.features.10.conv.0.0.weight\", \"image_model.features.10.conv.0.1.weight\", \"image_model.features.10.conv.0.1.bias\", \"image_model.features.10.conv.0.1.running_mean\", \"image_model.features.10.conv.0.1.running_var\", \"image_model.features.10.conv.1.0.weight\", \"image_model.features.10.conv.1.1.weight\", \"image_model.features.10.conv.1.1.bias\", \"image_model.features.10.conv.1.1.running_mean\", \"image_model.features.10.conv.1.1.running_var\", \"image_model.features.10.conv.2.weight\", \"image_model.features.10.conv.3.weight\", \"image_model.features.10.conv.3.bias\", \"image_model.features.10.conv.3.running_mean\", \"image_model.features.10.conv.3.running_var\", \"image_model.features.11.conv.0.0.weight\", \"image_model.features.11.conv.0.1.weight\", \"image_model.features.11.conv.0.1.bias\", \"image_model.features.11.conv.0.1.running_mean\", \"image_model.features.11.conv.0.1.running_var\", \"image_model.features.11.conv.1.0.weight\", \"image_model.features.11.conv.1.1.weight\", \"image_model.features.11.conv.1.1.bias\", \"image_model.features.11.conv.1.1.running_mean\", \"image_model.features.11.conv.1.1.running_var\", \"image_model.features.11.conv.2.weight\", \"image_model.features.11.conv.3.weight\", \"image_model.features.11.conv.3.bias\", \"image_model.features.11.conv.3.running_mean\", \"image_model.features.11.conv.3.running_var\", \"image_model.features.12.conv.0.0.weight\", \"image_model.features.12.conv.0.1.weight\", \"image_model.features.12.conv.0.1.bias\", \"image_model.features.12.conv.0.1.running_mean\", \"image_model.features.12.conv.0.1.running_var\", \"image_model.features.12.conv.1.0.weight\", \"image_model.features.12.conv.1.1.weight\", \"image_model.features.12.conv.1.1.bias\", \"image_model.features.12.conv.1.1.running_mean\", \"image_model.features.12.conv.1.1.running_var\", \"image_model.features.12.conv.2.weight\", \"image_model.features.12.conv.3.weight\", \"image_model.features.12.conv.3.bias\", \"image_model.features.12.conv.3.running_mean\", \"image_model.features.12.conv.3.running_var\", \"image_model.features.13.conv.0.0.weight\", \"image_model.features.13.conv.0.1.weight\", \"image_model.features.13.conv.0.1.bias\", \"image_model.features.13.conv.0.1.running_mean\", \"image_model.features.13.conv.0.1.running_var\", \"image_model.features.13.conv.1.0.weight\", \"image_model.features.13.conv.1.1.weight\", \"image_model.features.13.conv.1.1.bias\", \"image_model.features.13.conv.1.1.running_mean\", \"image_model.features.13.conv.1.1.running_var\", \"image_model.features.13.conv.2.weight\", \"image_model.features.13.conv.3.weight\", \"image_model.features.13.conv.3.bias\", \"image_model.features.13.conv.3.running_mean\", \"image_model.features.13.conv.3.running_var\", \"image_model.features.14.conv.0.0.weight\", \"image_model.features.14.conv.0.1.weight\", \"image_model.features.14.conv.0.1.bias\", \"image_model.features.14.conv.0.1.running_mean\", \"image_model.features.14.conv.0.1.running_var\", \"image_model.features.14.conv.1.0.weight\", \"image_model.features.14.conv.1.1.weight\", \"image_model.features.14.conv.1.1.bias\", \"image_model.features.14.conv.1.1.running_mean\", \"image_model.features.14.conv.1.1.running_var\", \"image_model.features.14.conv.2.weight\", \"image_model.features.14.conv.3.weight\", \"image_model.features.14.conv.3.bias\", \"image_model.features.14.conv.3.running_mean\", \"image_model.features.14.conv.3.running_var\", \"image_model.features.15.conv.0.0.weight\", \"image_model.features.15.conv.0.1.weight\", \"image_model.features.15.conv.0.1.bias\", \"image_model.features.15.conv.0.1.running_mean\", \"image_model.features.15.conv.0.1.running_var\", \"image_model.features.15.conv.1.0.weight\", \"image_model.features.15.conv.1.1.weight\", \"image_model.features.15.conv.1.1.bias\", \"image_model.features.15.conv.1.1.running_mean\", \"image_model.features.15.conv.1.1.running_var\", \"image_model.features.15.conv.2.weight\", \"image_model.features.15.conv.3.weight\", \"image_model.features.15.conv.3.bias\", \"image_model.features.15.conv.3.running_mean\", \"image_model.features.15.conv.3.running_var\", \"image_model.features.16.conv.0.0.weight\", \"image_model.features.16.conv.0.1.weight\", \"image_model.features.16.conv.0.1.bias\", \"image_model.features.16.conv.0.1.running_mean\", \"image_model.features.16.conv.0.1.running_var\", \"image_model.features.16.conv.1.0.weight\", \"image_model.features.16.conv.1.1.weight\", \"image_model.features.16.conv.1.1.bias\", \"image_model.features.16.conv.1.1.running_mean\", \"image_model.features.16.conv.1.1.running_var\", \"image_model.features.16.conv.2.weight\", \"image_model.features.16.conv.3.weight\", \"image_model.features.16.conv.3.bias\", \"image_model.features.16.conv.3.running_mean\", \"image_model.features.16.conv.3.running_var\", \"image_model.features.17.conv.0.0.weight\", \"image_model.features.17.conv.0.1.weight\", \"image_model.features.17.conv.0.1.bias\", \"image_model.features.17.conv.0.1.running_mean\", \"image_model.features.17.conv.0.1.running_var\", \"image_model.features.17.conv.1.0.weight\", \"image_model.features.17.conv.1.1.weight\", \"image_model.features.17.conv.1.1.bias\", \"image_model.features.17.conv.1.1.running_mean\", \"image_model.features.17.conv.1.1.running_var\", \"image_model.features.17.conv.2.weight\", \"image_model.features.17.conv.3.weight\", \"image_model.features.17.conv.3.bias\", \"image_model.features.17.conv.3.running_mean\", \"image_model.features.17.conv.3.running_var\", \"image_model.features.18.0.weight\", \"image_model.features.18.1.weight\", \"image_model.features.18.1.bias\", \"image_model.features.18.1.running_mean\", \"image_model.features.18.1.running_var\", \"fusion_head.5.weight\", \"fusion_head.5.bias\". \n\tUnexpected key(s) in state_dict: \"image_model.conv1.weight\", \"image_model.bn1.weight\", \"image_model.bn1.bias\", \"image_model.bn1.running_mean\", \"image_model.bn1.running_var\", \"image_model.bn1.num_batches_tracked\", \"image_model.layer1.0.conv1.weight\", \"image_model.layer1.0.bn1.weight\", \"image_model.layer1.0.bn1.bias\", \"image_model.layer1.0.bn1.running_mean\", \"image_model.layer1.0.bn1.running_var\", \"image_model.layer1.0.bn1.num_batches_tracked\", \"image_model.layer1.0.conv2.weight\", \"image_model.layer1.0.bn2.weight\", \"image_model.layer1.0.bn2.bias\", \"image_model.layer1.0.bn2.running_mean\", \"image_model.layer1.0.bn2.running_var\", \"image_model.layer1.0.bn2.num_batches_tracked\", \"image_model.layer1.0.conv3.weight\", \"image_model.layer1.0.bn3.weight\", \"image_model.layer1.0.bn3.bias\", \"image_model.layer1.0.bn3.running_mean\", \"image_model.layer1.0.bn3.running_var\", \"image_model.layer1.0.bn3.num_batches_tracked\", \"image_model.layer1.0.downsample.0.weight\", \"image_model.layer1.0.downsample.1.weight\", \"image_model.layer1.0.downsample.1.bias\", \"image_model.layer1.0.downsample.1.running_mean\", \"image_model.layer1.0.downsample.1.running_var\", \"image_model.layer1.0.downsample.1.num_batches_tracked\", \"image_model.layer1.1.conv1.weight\", \"image_model.layer1.1.bn1.weight\", \"image_model.layer1.1.bn1.bias\", \"image_model.layer1.1.bn1.running_mean\", \"image_model.layer1.1.bn1.running_var\", \"image_model.layer1.1.bn1.num_batches_tracked\", \"image_model.layer1.1.conv2.weight\", \"image_model.layer1.1.bn2.weight\", \"image_model.layer1.1.bn2.bias\", \"image_model.layer1.1.bn2.running_mean\", \"image_model.layer1.1.bn2.running_var\", \"image_model.layer1.1.bn2.num_batches_tracked\", \"image_model.layer1.1.conv3.weight\", \"image_model.layer1.1.bn3.weight\", \"image_model.layer1.1.bn3.bias\", \"image_model.layer1.1.bn3.running_mean\", \"image_model.layer1.1.bn3.running_var\", \"image_model.layer1.1.bn3.num_batches_tracked\", \"image_model.layer1.2.conv1.weight\", \"image_model.layer1.2.bn1.weight\", \"image_model.layer1.2.bn1.bias\", \"image_model.layer1.2.bn1.running_mean\", \"image_model.layer1.2.bn1.running_var\", \"image_model.layer1.2.bn1.num_batches_tracked\", \"image_model.layer1.2.conv2.weight\", \"image_model.layer1.2.bn2.weight\", \"image_model.layer1.2.bn2.bias\", \"image_model.layer1.2.bn2.running_mean\", \"image_model.layer1.2.bn2.running_var\", \"image_model.layer1.2.bn2.num_batches_tracked\", \"image_model.layer1.2.conv3.weight\", \"image_model.layer1.2.bn3.weight\", \"image_model.layer1.2.bn3.bias\", \"image_model.layer1.2.bn3.running_mean\", \"image_model.layer1.2.bn3.running_var\", \"image_model.layer1.2.bn3.num_batches_tracked\", \"image_model.layer2.0.conv1.weight\", \"image_model.layer2.0.bn1.weight\", \"image_model.layer2.0.bn1.bias\", \"image_model.layer2.0.bn1.running_mean\", \"image_model.layer2.0.bn1.running_var\", \"image_model.layer2.0.bn1.num_batches_tracked\", \"image_model.layer2.0.conv2.weight\", \"image_model.layer2.0.bn2.weight\", \"image_model.layer2.0.bn2.bias\", \"image_model.layer2.0.bn2.running_mean\", \"image_model.layer2.0.bn2.running_var\", \"image_model.layer2.0.bn2.num_batches_tracked\", \"image_model.layer2.0.conv3.weight\", \"image_model.layer2.0.bn3.weight\", \"image_model.layer2.0.bn3.bias\", \"image_model.layer2.0.bn3.running_mean\", \"image_model.layer2.0.bn3.running_var\", \"image_model.layer2.0.bn3.num_batches_tracked\", \"image_model.layer2.0.downsample.0.weight\", \"image_model.layer2.0.downsample.1.weight\", \"image_model.layer2.0.downsample.1.bias\", \"image_model.layer2.0.downsample.1.running_mean\", \"image_model.layer2.0.downsample.1.running_var\", \"image_model.layer2.0.downsample.1.num_batches_tracked\", \"image_model.layer2.1.conv1.weight\", \"image_model.layer2.1.bn1.weight\", \"image_model.layer2.1.bn1.bias\", \"image_model.layer2.1.bn1.running_mean\", \"image_model.layer2.1.bn1.running_var\", \"image_model.layer2.1.bn1.num_batches_tracked\", \"image_model.layer2.1.conv2.weight\", \"image_model.layer2.1.bn2.weight\", \"image_model.layer2.1.bn2.bias\", \"image_model.layer2.1.bn2.running_mean\", \"image_model.layer2.1.bn2.running_var\", \"image_model.layer2.1.bn2.num_batches_tracked\", \"image_model.layer2.1.conv3.weight\", \"image_model.layer2.1.bn3.weight\", \"image_model.layer2.1.bn3.bias\", \"image_model.layer2.1.bn3.running_mean\", \"image_model.layer2.1.bn3.running_var\", \"image_model.layer2.1.bn3.num_batches_tracked\", \"image_model.layer2.2.conv1.weight\", \"image_model.layer2.2.bn1.weight\", \"image_model.layer2.2.bn1.bias\", \"image_model.layer2.2.bn1.running_mean\", \"image_model.layer2.2.bn1.running_var\", \"image_model.layer2.2.bn1.num_batches_tracked\", \"image_model.layer2.2.conv2.weight\", \"image_model.layer2.2.bn2.weight\", \"image_model.layer2.2.bn2.bias\", \"image_model.layer2.2.bn2.running_mean\", \"image_model.layer2.2.bn2.running_var\", \"image_model.layer2.2.bn2.num_batches_tracked\", \"image_model.layer2.2.conv3.weight\", \"image_model.layer2.2.bn3.weight\", \"image_model.layer2.2.bn3.bias\", \"image_model.layer2.2.bn3.running_mean\", \"image_model.layer2.2.bn3.running_var\", \"image_model.layer2.2.bn3.num_batches_tracked\", \"image_model.layer2.3.conv1.weight\", \"image_model.layer2.3.bn1.weight\", \"image_model.layer2.3.bn1.bias\", \"image_model.layer2.3.bn1.running_mean\", \"image_model.layer2.3.bn1.running_var\", \"image_model.layer2.3.bn1.num_batches_tracked\", \"image_model.layer2.3.conv2.weight\", \"image_model.layer2.3.bn2.weight\", \"image_model.layer2.3.bn2.bias\", \"image_model.layer2.3.bn2.running_mean\", \"image_model.layer2.3.bn2.running_var\", \"image_model.layer2.3.bn2.num_batches_tracked\", \"image_model.layer2.3.conv3.weight\", \"image_model.layer2.3.bn3.weight\", \"image_model.layer2.3.bn3.bias\", \"image_model.layer2.3.bn3.running_mean\", \"image_model.layer2.3.bn3.running_var\", \"image_model.layer2.3.bn3.num_batches_tracked\", \"image_model.layer3.0.conv1.weight\", \"image_model.layer3.0.bn1.weight\", \"image_model.layer3.0.bn1.bias\", \"image_model.layer3.0.bn1.running_mean\", \"image_model.layer3.0.bn1.running_var\", \"image_model.layer3.0.bn1.num_batches_tracked\", \"image_model.layer3.0.conv2.weight\", \"image_model.layer3.0.bn2.weight\", \"image_model.layer3.0.bn2.bias\", \"image_model.layer3.0.bn2.running_mean\", \"image_model.layer3.0.bn2.running_var\", \"image_model.layer3.0.bn2.num_batches_tracked\", \"image_model.layer3.0.conv3.weight\", \"image_model.layer3.0.bn3.weight\", \"image_model.layer3.0.bn3.bias\", \"image_model.layer3.0.bn3.running_mean\", \"image_model.layer3.0.bn3.running_var\", \"image_model.layer3.0.bn3.num_batches_tracked\", \"image_model.layer3.0.downsample.0.weight\", \"image_model.layer3.0.downsample.1.weight\", \"image_model.layer3.0.downsample.1.bias\", \"image_model.layer3.0.downsample.1.running_mean\", \"image_model.layer3.0.downsample.1.running_var\", \"image_model.layer3.0.downsample.1.num_batches_tracked\", \"image_model.layer3.1.conv1.weight\", \"image_model.layer3.1.bn1.weight\", \"image_model.layer3.1.bn1.bias\", \"image_model.layer3.1.bn1.running_mean\", \"image_model.layer3.1.bn1.running_var\", \"image_model.layer3.1.bn1.num_batches_tracked\", \"image_model.layer3.1.conv2.weight\", \"image_model.layer3.1.bn2.weight\", \"image_model.layer3.1.bn2.bias\", \"image_model.layer3.1.bn2.running_mean\", \"image_model.layer3.1.bn2.running_var\", \"image_model.layer3.1.bn2.num_batches_tracked\", \"image_model.layer3.1.conv3.weight\", \"image_model.layer3.1.bn3.weight\", \"image_model.layer3.1.bn3.bias\", \"image_model.layer3.1.bn3.running_mean\", \"image_model.layer3.1.bn3.running_var\", \"image_model.layer3.1.bn3.num_batches_tracked\", \"image_model.layer3.2.conv1.weight\", \"image_model.layer3.2.bn1.weight\", \"image_model.layer3.2.bn1.bias\", \"image_model.layer3.2.bn1.running_mean\", \"image_model.layer3.2.bn1.running_var\", \"image_model.layer3.2.bn1.num_batches_tracked\", \"image_model.layer3.2.conv2.weight\", \"image_model.layer3.2.bn2.weight\", \"image_model.layer3.2.bn2.bias\", \"image_model.layer3.2.bn2.running_mean\", \"image_model.layer3.2.bn2.running_var\", \"image_model.layer3.2.bn2.num_batches_tracked\", \"image_model.layer3.2.conv3.weight\", \"image_model.layer3.2.bn3.weight\", \"image_model.layer3.2.bn3.bias\", \"image_model.layer3.2.bn3.running_mean\", \"image_model.layer3.2.bn3.running_var\", \"image_model.layer3.2.bn3.num_batches_tracked\", \"image_model.layer3.3.conv1.weight\", \"image_model.layer3.3.bn1.weight\", \"image_model.layer3.3.bn1.bias\", \"image_model.layer3.3.bn1.running_mean\", \"image_model.layer3.3.bn1.running_var\", \"image_model.layer3.3.bn1.num_batches_tracked\", \"image_model.layer3.3.conv2.weight\", \"image_model.layer3.3.bn2.weight\", \"image_model.layer3.3.bn2.bias\", \"image_model.layer3.3.bn2.running_mean\", \"image_model.layer3.3.bn2.running_var\", \"image_model.layer3.3.bn2.num_batches_tracked\", \"image_model.layer3.3.conv3.weight\", \"image_model.layer3.3.bn3.weight\", \"image_model.layer3.3.bn3.bias\", \"image_model.layer3.3.bn3.running_mean\", \"image_model.layer3.3.bn3.running_var\", \"image_model.layer3.3.bn3.num_batches_tracked\", \"image_model.layer3.4.conv1.weight\", \"image_model.layer3.4.bn1.weight\", \"image_model.layer3.4.bn1.bias\", \"image_model.layer3.4.bn1.running_mean\", \"image_model.layer3.4.bn1.running_var\", \"image_model.layer3.4.bn1.num_batches_tracked\", \"image_model.layer3.4.conv2.weight\", \"image_model.layer3.4.bn2.weight\", \"image_model.layer3.4.bn2.bias\", \"image_model.layer3.4.bn2.running_mean\", \"image_model.layer3.4.bn2.running_var\", \"image_model.layer3.4.bn2.num_batches_tracked\", \"image_model.layer3.4.conv3.weight\", \"image_model.layer3.4.bn3.weight\", \"image_model.layer3.4.bn3.bias\", \"image_model.layer3.4.bn3.running_mean\", \"image_model.layer3.4.bn3.running_var\", \"image_model.layer3.4.bn3.num_batches_tracked\", \"image_model.layer3.5.conv1.weight\", \"image_model.layer3.5.bn1.weight\", \"image_model.layer3.5.bn1.bias\", \"image_model.layer3.5.bn1.running_mean\", \"image_model.layer3.5.bn1.running_var\", \"image_model.layer3.5.bn1.num_batches_tracked\", \"image_model.layer3.5.conv2.weight\", \"image_model.layer3.5.bn2.weight\", \"image_model.layer3.5.bn2.bias\", \"image_model.layer3.5.bn2.running_mean\", \"image_model.layer3.5.bn2.running_var\", \"image_model.layer3.5.bn2.num_batches_tracked\", \"image_model.layer3.5.conv3.weight\", \"image_model.layer3.5.bn3.weight\", \"image_model.layer3.5.bn3.bias\", \"image_model.layer3.5.bn3.running_mean\", \"image_model.layer3.5.bn3.running_var\", \"image_model.layer3.5.bn3.num_batches_tracked\", \"image_model.layer4.0.conv1.weight\", \"image_model.layer4.0.bn1.weight\", \"image_model.layer4.0.bn1.bias\", \"image_model.layer4.0.bn1.running_mean\", \"image_model.layer4.0.bn1.running_var\", \"image_model.layer4.0.bn1.num_batches_tracked\", \"image_model.layer4.0.conv2.weight\", \"image_model.layer4.0.bn2.weight\", \"image_model.layer4.0.bn2.bias\", \"image_model.layer4.0.bn2.running_mean\", \"image_model.layer4.0.bn2.running_var\", \"image_model.layer4.0.bn2.num_batches_tracked\", \"image_model.layer4.0.conv3.weight\", \"image_model.layer4.0.bn3.weight\", \"image_model.layer4.0.bn3.bias\", \"image_model.layer4.0.bn3.running_mean\", \"image_model.layer4.0.bn3.running_var\", \"image_model.layer4.0.bn3.num_batches_tracked\", \"image_model.layer4.0.downsample.0.weight\", \"image_model.layer4.0.downsample.1.weight\", \"image_model.layer4.0.downsample.1.bias\", \"image_model.layer4.0.downsample.1.running_mean\", \"image_model.layer4.0.downsample.1.running_var\", \"image_model.layer4.0.downsample.1.num_batches_tracked\", \"image_model.layer4.1.conv1.weight\", \"image_model.layer4.1.bn1.weight\", \"image_model.layer4.1.bn1.bias\", \"image_model.layer4.1.bn1.running_mean\", \"image_model.layer4.1.bn1.running_var\", \"image_model.layer4.1.bn1.num_batches_tracked\", \"image_model.layer4.1.conv2.weight\", \"image_model.layer4.1.bn2.weight\", \"image_model.layer4.1.bn2.bias\", \"image_model.layer4.1.bn2.running_mean\", \"image_model.layer4.1.bn2.running_var\", \"image_model.layer4.1.bn2.num_batches_tracked\", \"image_model.layer4.1.conv3.weight\", \"image_model.layer4.1.bn3.weight\", \"image_model.layer4.1.bn3.bias\", \"image_model.layer4.1.bn3.running_mean\", \"image_model.layer4.1.bn3.running_var\", \"image_model.layer4.1.bn3.num_batches_tracked\", \"image_model.layer4.2.conv1.weight\", \"image_model.layer4.2.bn1.weight\", \"image_model.layer4.2.bn1.bias\", \"image_model.layer4.2.bn1.running_mean\", \"image_model.layer4.2.bn1.running_var\", \"image_model.layer4.2.bn1.num_batches_tracked\", \"image_model.layer4.2.conv2.weight\", \"image_model.layer4.2.bn2.weight\", \"image_model.layer4.2.bn2.bias\", \"image_model.layer4.2.bn2.running_mean\", \"image_model.layer4.2.bn2.running_var\", \"image_model.layer4.2.bn2.num_batches_tracked\", \"image_model.layer4.2.conv3.weight\", \"image_model.layer4.2.bn3.weight\", \"image_model.layer4.2.bn3.bias\", \"image_model.layer4.2.bn3.running_mean\", \"image_model.layer4.2.bn3.running_var\", \"image_model.layer4.2.bn3.num_batches_tracked\", \"image_projection.3.weight\", \"image_projection.3.bias\", \"text_projection.3.weight\", \"text_projection.3.bias\", \"fusion_head.6.weight\", \"fusion_head.6.bias\", \"fusion_head.9.weight\", \"fusion_head.9.bias\". \n\tsize mismatch for image_projection.0.weight: copying a param with shape torch.Size([512, 2048]) from checkpoint, the shape in current model is torch.Size([64, 1280]).\n\tsize mismatch for image_projection.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for text_projection.0.weight: copying a param with shape torch.Size([512, 768]) from checkpoint, the shape in current model is torch.Size([64, 768]).\n\tsize mismatch for text_projection.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fusion_head.0.weight: copying a param with shape torch.Size([256, 257]) from checkpoint, the shape in current model is torch.Size([64, 129]).\n\tsize mismatch for fusion_head.0.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([64]).\n\tsize mismatch for fusion_head.3.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([32, 64]).\n\tsize mismatch for fusion_head.3.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([32])."
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "print(\"Loading best model...\")\n",
    "checkpoint = torch.load(config.MODEL_SAVE_PATH, weights_only=False)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Best model loaded from epoch {checkpoint['epoch'] + 1}\")\n",
    "print(f\"Best validation SMAPE: {checkpoint['val_smape']:.4f}%\")\n",
    "print(f\"Best validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d15a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions for test set...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823c94018ed24f0e8ba1bc3984cb0054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating predictions:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating predictions for test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823c94018ed24f0e8ba1bc3984cb0054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating predictions:   0%|          | 0/2344 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Generate predictions\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerating predictions for test set...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m sample_ids, predictions = \u001b[43mgenerate_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGenerated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(predictions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m predictions\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mgenerate_predictions\u001b[39m\u001b[34m(model, dataloader, device)\u001b[39m\n\u001b[32m     20\u001b[39m         outputs = model(image, input_ids, attention_mask, ipq)\n\u001b[32m     22\u001b[39m         \u001b[38;5;66;03m# Store predictions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         predictions.extend(\u001b[43moutputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.numpy())\n\u001b[32m     24\u001b[39m         sample_ids.extend(batch[\u001b[33m'\u001b[39m\u001b[33msample_id\u001b[39m\u001b[33m'\u001b[39m].numpy())\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sample_ids, predictions\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def generate_predictions(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Generate predictions for test data.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    sample_ids = []\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc='Generating predictions')\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in progress_bar:\n",
    "            # Move data to device\n",
    "            image = batch['image'].to(device)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            ipq = batch['ipq'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(image, input_ids, attention_mask, ipq)\n",
    "            \n",
    "            # Store predictions\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            sample_ids.extend(batch['sample_id'].numpy())\n",
    "    \n",
    "    return sample_ids, predictions\n",
    "\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nGenerating predictions for test set...\")\n",
    "sample_ids, predictions = generate_predictions(model, test_loader, config.DEVICE)\n",
    "\n",
    "print(f\"Generated {len(predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1285a57a",
   "metadata": {},
   "source": [
    "## Step 15: Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5db5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': sample_ids,\n",
    "    'price': predictions\n",
    "})\n",
    "\n",
    "# Sort by sample_id\n",
    "submission_df = submission_df.sort_values('sample_id').reset_index(drop=True)\n",
    "\n",
    "# Ensure all prices are positive\n",
    "submission_df['price'] = submission_df['price'].apply(lambda x: max(x, 0.01))\n",
    "\n",
    "# Display sample\n",
    "print(\"Submission file preview:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "print(f\"\\nSubmission statistics:\")\n",
    "print(submission_df['price'].describe())\n",
    "\n",
    "# Save to CSV\n",
    "submission_df.to_csv(config.SUBMISSION_FILE, index=False)\n",
    "print(f\"\\n✓ Submission file saved as '{config.SUBMISSION_FILE}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec53f69c",
   "metadata": {},
   "source": [
    "## Step 16: Validation and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9cca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify submission file format\n",
    "print(\"Verifying submission file...\")\n",
    "submission_check = pd.read_csv(config.SUBMISSION_FILE)\n",
    "\n",
    "print(f\"✓ Number of rows: {len(submission_check)}\")\n",
    "print(f\"✓ Columns: {submission_check.columns.tolist()}\")\n",
    "print(f\"✓ All prices positive: {(submission_check['price'] > 0).all()}\")\n",
    "print(f\"✓ No missing values: {submission_check.isnull().sum().sum() == 0}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: Multimodal (ResNet50 + DistilBERT + IPQ)\")\n",
    "print(f\"Best Validation SMAPE: {best_val_smape:.4f}%\")\n",
    "print(f\"Training samples: {len(train_data)}\")\n",
    "print(f\"Validation samples: {len(val_data)}\")\n",
    "print(f\"Test samples: {len(test_df)}\")\n",
    "print(f\"Epochs trained: {config.EPOCHS}\")\n",
    "print(f\"Submission file: {config.SUBMISSION_FILE}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✓ All done! Ready for submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6ecb6",
   "metadata": {},
   "source": [
    "## Optional: Analyze Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6324e9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze validation set predictions\n",
    "model.eval()\n",
    "val_preds = []\n",
    "val_targets = []\n",
    "val_sample_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc='Analyzing validation predictions'):\n",
    "        image = batch['image'].to(config.DEVICE)\n",
    "        input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "        ipq = batch['ipq'].to(config.DEVICE)\n",
    "        \n",
    "        outputs = model(image, input_ids, attention_mask, ipq)\n",
    "        \n",
    "        val_preds.extend(outputs.cpu().numpy())\n",
    "        val_targets.extend(batch['price'].cpu().numpy())\n",
    "        val_sample_ids.extend(batch['sample_id'].numpy())\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'sample_id': val_sample_ids,\n",
    "    'actual_price': val_targets,\n",
    "    'predicted_price': val_preds\n",
    "})\n",
    "\n",
    "comparison_df['error'] = np.abs(comparison_df['actual_price'] - comparison_df['predicted_price'])\n",
    "comparison_df['error_pct'] = (comparison_df['error'] / comparison_df['actual_price']) * 100\n",
    "\n",
    "print(\"\\nValidation Predictions Analysis:\")\n",
    "print(\"\\nTop 10 best predictions (lowest error):\")\n",
    "print(comparison_df.nsmallest(10, 'error_pct')[['sample_id', 'actual_price', 'predicted_price', 'error_pct']])\n",
    "\n",
    "print(\"\\nTop 10 worst predictions (highest error):\")\n",
    "print(comparison_df.nlargest(10, 'error_pct')[['sample_id', 'actual_price', 'predicted_price', 'error_pct']])\n",
    "\n",
    "# Plot predictions vs actual\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(comparison_df['actual_price'], comparison_df['predicted_price'], alpha=0.5)\n",
    "plt.plot([comparison_df['actual_price'].min(), comparison_df['actual_price'].max()], \n",
    "         [comparison_df['actual_price'].min(), comparison_df['actual_price'].max()], \n",
    "         'r--', lw=2, label='Perfect Prediction')\n",
    "plt.xlabel('Actual Price')\n",
    "plt.ylabel('Predicted Price')\n",
    "plt.title('Predictions vs Actual Prices (Validation Set)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('predictions_vs_actual.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPrediction analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
